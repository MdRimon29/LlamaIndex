{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ccd7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.prompts.prompts import SimpleInputPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07068de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "Doc ID: 61814818-0e78-475c-b5d1-dbe126400646\n",
      "Text: Published as a conference paper at ICLR 2023 REAC T: S\n",
      "YNERGIZING REASONING AND ACTING IN LANGUAGE MODELS Shunyu Yao∗*,1,\n",
      "Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1,\n",
      "Yuan Cao2 1Department of Computer Science, Princeton University\n",
      "2Google Research, Brain team 1{shunyuy,karthikn}@princeton.edu\n",
      "2{jeffreyzhao,dianyu,dunan,...\n"
     ]
    }
   ],
   "source": [
    "documents= SimpleDirectoryReader(\"data\").load_data()\n",
    "print(len(documents))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "582ad4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\" \n",
    "You are a knowledgeable and smart Q&A assistant.\n",
    "\n",
    "Your goals:\n",
    "- Use the provided context or retrieved documents to give accurate and relevant answers.\n",
    "- If the context doesn’t contain the exact answer, say you don’t have enough information instead of guessing.\n",
    "- Always summarize clearly and concisely.\n",
    "- When possible, provide structured and easy-to-read responses (use bullet points or short paragraphs).\n",
    "- Do not include unrelated or speculative content.\n",
    "- If the user’s question is ambiguous, politely ask for clarification.\n",
    "- Maintain a professional yet approachable tone.\n",
    "\n",
    "Example style:\n",
    "User: \"What is LlamaIndex?\"\n",
    "Assistant: \"LlamaIndex is a data framework that helps you connect external data (like files, databases, or APIs) to large language models. It simplifies loading, indexing, and querying data.\"\n",
    "\n",
    "You always follow these principles for every response.\n",
    "\"\"\"\n",
    "\n",
    "# default format supportable by llama2\n",
    "query_wrapper_prompt = SimpleInputPrompt(f\"<|USER|>{system_prompt}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e070460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from typing import List, Optional\n",
    "HF_TOKEN: Optional[str] = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34b175",
   "metadata": {},
   "source": [
    "### **Now we set the settings for entire system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef2125f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 14:22:58,657 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"deepseek-ai/DeepSeek-R1-0528\",\n",
    "    token=HF_TOKEN,\n",
    "    provider=\"together\",  # this will use the best provider available\n",
    ")\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.context_window = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1626dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 64/64 [00:00<00:00, 766.53it/s]\n",
      "Generating embeddings: 100%|██████████| 163/163 [00:00<00:00, 193.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# a vector store index only needs an embed model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, embed_model=Settings.embed_model, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99f4c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_toolkit import prompt\n",
    "\n",
    "\n",
    "query_engine=index.as_query_engine(\n",
    "    prompt=query_wrapper_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0bbb2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Hmm, the user is asking about YOLO based on a research paper context. Let me analyze the provided text carefully.\n",
      "\n",
      "The context describes YOLO as a real-time object detection system that processes streaming video with under 25ms latency. It mentions three key advantages: superior speed/accuracy ratio compared to other real-time systems, global image reasoning that reduces background errors, and strong generalization to new domains like artwork.\n",
      "\n",
      "The technical section explains YOLO's unified approach - using a single neural network to predict all bounding boxes simultaneously across an S×S grid. Each grid cell predicts bounding boxes with confidence scores based on object presence and localization accuracy.\n",
      "\n",
      "I should highlight these core aspects: real-time capability, unified detection framework, grid-based prediction system, and its advantages over methods like R-CNN. The answer should synthesize these points without referencing the context directly.\n",
      "\n",
      "The response needs to be concise yet comprehensive, covering both the practical performance characteristics and the technical approach while maintaining natural flow. I'll avoid mentioning latency numbers since the user just asked \"what is\" rather than specific metrics.\n",
      "</think>\n",
      "YOLO (You Only Look Once) is a real-time object detection system that processes entire images in a single evaluation. It operates by dividing images into a grid where each\n"
     ]
    }
   ],
   "source": [
    "result = query_engine.query(\"What is YOLO?\")\n",
    "print(result.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
