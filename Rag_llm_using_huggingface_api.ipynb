{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ccd7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.prompts.prompts import SimpleInputPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07068de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "Doc ID: 49cf0157-c0b7-4e67-9dfa-f96b61fb83b3\n",
      "Text: Published as a conference paper at ICLR 2023 REAC T: S\n",
      "YNERGIZING REASONING AND ACTING IN LANGUAGE MODELS Shunyu Yao∗*,1,\n",
      "Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1,\n",
      "Yuan Cao2 1Department of Computer Science, Princeton University\n",
      "2Google Research, Brain team 1{shunyuy,karthikn}@princeton.edu\n",
      "2{jeffreyzhao,dianyu,dunan,...\n"
     ]
    }
   ],
   "source": [
    "documents= SimpleDirectoryReader(\"data\").load_data()\n",
    "print(len(documents))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "582ad4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\" \n",
    "You are a knowledgeable and smart Q&A assistant.\n",
    "\n",
    "Your goals:\n",
    "- Use the provided context or retrieved documents to give accurate and relevant answers.\n",
    "- If the context doesn’t contain the exact answer, say you don’t have enough information instead of guessing.\n",
    "- Always summarize clearly and concisely.\n",
    "- When possible, provide structured and easy-to-read responses (use bullet points or short paragraphs).\n",
    "- Do not include unrelated or speculative content.\n",
    "- If the user’s question is ambiguous, politely ask for clarification.\n",
    "- Maintain a professional yet approachable tone.\n",
    "\n",
    "Example style:\n",
    "User: \"What is LlamaIndex?\"\n",
    "Assistant: \"LlamaIndex is a data framework that helps you connect external data (like files, databases, or APIs) to large language models. It simplifies loading, indexing, and querying data.\"\n",
    "\n",
    "You always follow these principles for every response.\n",
    "\"\"\"\n",
    "\n",
    "# default format supportable by llama2\n",
    "query_wrapper_prompt = SimpleInputPrompt(f\"<|USER|>{system_prompt}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e070460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from typing import List, Optional\n",
    "HF_TOKEN: Optional[str] = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34b175",
   "metadata": {},
   "source": [
    "### **Now we set the settings for entire system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2125f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/betopia/LlamaIndex/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-04 14:37:16,076 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"moonshotai/Kimi-K2-Instruct-0905\",\n",
    "    token=HF_TOKEN,\n",
    "    provider=\"together\",  # this will use the best provider available\n",
    ")\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.context_window = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1626dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 64/64 [00:00<00:00, 258.27it/s]\n",
      "Generating embeddings: 100%|██████████| 163/163 [00:00<00:00, 172.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# a vector store index only needs an embed model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, embed_model=Settings.embed_model, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99f4c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from banks import Prompt\n",
    "from prompt_toolkit import prompt\n",
    "\n",
    "\n",
    "query_engine=index.as_query_engine(\n",
    "    Prompt=query_wrapper_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0bbb2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO is a unified, single-neural-network approach to object detection that processes entire images in real time. It divides the image into a grid, has each grid cell predict multiple bounding boxes together with confidence scores, and simultaneously forecasts all boxes across every object class. This global, end-to-end design keeps latency under 25 ms, yields more than double the mean average precision of other real-time detectors, and makes fewer background errors than methods like Fast R-CNN, while also learning representations that generalize well to new domains.\n"
     ]
    }
   ],
   "source": [
    "result = query_engine.query(\"What is YOLO?\")\n",
    "print(result.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
