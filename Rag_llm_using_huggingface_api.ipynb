{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ccd7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.prompts.prompts import SimpleInputPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07068de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "Doc ID: b55057b6-05ba-4421-a8d9-33bc080d23ba\n",
      "Text: Published as a conference paper at ICLR 2023 REAC T: S\n",
      "YNERGIZING REASONING AND ACTING IN LANGUAGE MODELS Shunyu Yao∗*,1,\n",
      "Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1,\n",
      "Yuan Cao2 1Department of Computer Science, Princeton University\n",
      "2Google Research, Brain team 1{shunyuy,karthikn}@princeton.edu\n",
      "2{jeffreyzhao,dianyu,dunan,...\n"
     ]
    }
   ],
   "source": [
    "documents= SimpleDirectoryReader(\"data\").load_data()\n",
    "print(len(documents))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "582ad4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\" \n",
    "You are a knowledgeable and smart Q&A assistant.\n",
    "\n",
    "Your goals:\n",
    "- Use the provided context or retrieved documents to give accurate and relevant answers.\n",
    "- If the context doesn’t contain the exact answer, say you don’t have enough information instead of guessing.\n",
    "- Always summarize clearly and concisely.\n",
    "- When possible, provide structured and easy-to-read responses (use bullet points or short paragraphs).\n",
    "- Do not include unrelated or speculative content.\n",
    "- If the user’s question is ambiguous, politely ask for clarification.\n",
    "- Maintain a professional yet approachable tone.\n",
    "\n",
    "Example style:\n",
    "User: \"What is LlamaIndex?\"\n",
    "Assistant: \"LlamaIndex is a data framework that helps you connect external data (like files, databases, or APIs) to large language models. It simplifies loading, indexing, and querying data.\"\n",
    "\n",
    "You always follow these principles for every response.\n",
    "\"\"\"\n",
    "\n",
    "# default format supportable by llama2\n",
    "query_wrapper_prompt = SimpleInputPrompt(f\"<|USER|>{system_prompt}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e070460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from typing import List, Optional\n",
    "HF_TOKEN: Optional[str] = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e98f635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/betopia/LlamaIndex/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "remotely_run = HuggingFaceInferenceAPI(\n",
    "    model_name=\"deepseek-ai/DeepSeek-R1-0528\",\n",
    "    token=HF_TOKEN,\n",
    "    provider=\"together\",  # this will use the best provider available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "832524b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 12:36:17,964 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model=HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34b175",
   "metadata": {},
   "source": [
    "### **Now we set the settings for entire system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2125f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 12:36:21,463 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"deepseek-ai/DeepSeek-R1-0528\",\n",
    "    token=HF_TOKEN,\n",
    "    provider=\"together\",  # this will use the best provider available\n",
    ")\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 1000\n",
    "Settings.context_window = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1626dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 64/64 [00:00<00:00, 408.47it/s]\n",
      "Generating embeddings: 100%|██████████| 163/163 [00:00<00:00, 171.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# a vector store index only needs an embed model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, embed_model=embed_model, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f4c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_toolkit import prompt\n",
    "\n",
    "\n",
    "query_engine=index.as_query_engine(prompt=query_wrapper_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0bbb2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Hmm, the user is asking about YOLO based on a research paper context. Let me analyze the provided text carefully.\n",
      "\n",
      "The context describes YOLO as a real-time object detection system that processes streaming video with under 25ms latency. It mentions three key advantages: superior speed/accuracy ratio compared to other real-time systems, global image reasoning that reduces background errors, and strong generalization to new domains like artwork.\n",
      "\n",
      "The technical details reveal it's a unified neural network that divides images into grids, with each grid cell predicting bounding boxes and confidence scores. The text specifically contrasts YOLO's approach with sliding window and region proposal methods like R-CNN, noting its end-to-end trainable architecture.\n",
      "\n",
      "I should highlight its core innovation - unifying detection components into a single network - while mentioning its tradeoffs: slightly lower accuracy than state-of-the-art systems, particularly with small object localization. The open-source nature and pretrained model availability seem worth noting too.\n",
      "\n",
      "The response must avoid referencing the context directly while synthesizing these key points: real-time capability, unified architecture, grid-based detection, and performance characteristics. The academic paper tone suggests the user likely wants technical precision rather than simplified explanations.\n",
      "</think>\n",
      "YOLO (You Only Look Once) is a real-time\n"
     ]
    }
   ],
   "source": [
    "result = query_engine.query(\"What is YOLO?\")\n",
    "print(result.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
