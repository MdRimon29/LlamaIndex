{"docstore/metadata": {"8fe7f791-0df3-4c83-a0b1-727c09f05980": {"doc_hash": "5f577ddccbe92bbcb023ab0f6a863306b3d10dd8af7489f725f252bbdc5dbb74"}, "b6b83cba-db9d-4f51-9a57-98553a8f585b": {"doc_hash": "5c361553038c6174373aa6228acfafa63d348c8ce409af84683ea8097f42fd4d"}, "7a699566-66b7-4775-b7f7-325184d696aa": {"doc_hash": "55a55dfd2ce381188400d69655cd66063ac99f42ce1de93a0e98213b2cf922f5"}, "2e780022-c55a-4597-9dfd-33e57de3178d": {"doc_hash": "7991ad20c32e208c4592306517fcc8f0df92a06fee7d047963d13706960e4e61"}, "542d7e53-3d40-49e8-b27f-3ce12cc7fe27": {"doc_hash": "5537956126f88d729d6567e9778accfc135a4ec289e5e6ebf62b75117ebc1da8"}, "6151682a-a138-4fd1-9963-86e2b323bc47": {"doc_hash": "7d42dfb4aab91baa80037ce082dd0520adf308f34b7c1c2da64dc3f9a18e2413"}, "0e2000de-b1b7-49db-8551-f13a88bd5b76": {"doc_hash": "9044e6e2d2a13c0d0d9229028985f89378683f8825cfd7b2ffbe9683fad90f18"}, "f022c336-2b9d-4255-9611-95200c3ccda7": {"doc_hash": "8112109ac4d397b761bb122a14cfa807fb723566c12caab4271bab0625d8e8bd"}, "09a58be2-948e-4afd-9266-81ab087339de": {"doc_hash": "a7fe22317253b8c64086f9640c2ff8875c01be9dd03f2b66015d6cde561f5683"}, "cc66b82d-aca3-4dc3-8a08-7e38a0169aa7": {"doc_hash": "20bf013b5bdbee6fbc848e4babb7ef5d25c49841bfbbef7e7070d20ad614b5db"}, "483f9e6c-4e8d-4a8b-9a6d-b91fa78fd616": {"doc_hash": "bc843ab34cd411e47038a9800caab17f3ea9f1bed2832a35867dc7a323456a31"}, "471671c9-c130-4201-97a2-6c830550b8db": {"doc_hash": "2d4236f323c1e85cde978fc93b2cb72dd27edf889fa9fc2d638587f2a9874d15"}, "e76590cc-06e7-4aea-8441-e2dd363f22d9": {"doc_hash": "e36a0b151f93e177a10c680c9e75287da7d6a26a04d6e06bf0d5badbb67cd97b"}, "a4cb70d6-c274-4d05-8f7c-3562a7b5a3c2": {"doc_hash": "d959ceac940fca18ffa5ce718f6a723bdf58da5331a4ba935c363de0db105cdf"}, "5e12086a-da1a-45f7-a4a2-61acfe5763db": {"doc_hash": "45539aa54f34b0bda4ac9202227b7645060841a611897a1be95e0f4e1eaae69b"}, "c5d884e3-745e-43d0-a430-d6f985ee2b15": {"doc_hash": "3547e0b2e1992fe1c1b00ecc7962a099e458dba796fafb0c70c9be5d536c7b10"}, "9e817b24-c86c-41c1-bc96-15c2bd748942": {"doc_hash": "5a5d19101cbf0c3fb6dd981fd455b14d8ab0f902204fd02c834c61b2e73de7d7"}, "b47afeb6-caa7-410b-ab54-6b7edc2a5571": {"doc_hash": "232ee602112c03e8b4fcd727de86849423659efe9405a89f03f2f6a347a44cc6"}, "6540d239-e9b8-4be6-9477-7c22b6f40b05": {"doc_hash": "2093335f16a6b5b0c31c56d9f3e67753e73799d787a9e0a5a5f7f3550cf32fec"}, "2e9928d3-6a8a-4719-bdcf-e1f85560b13e": {"doc_hash": "fc04ffb97ab35f3ef70346e15ddfc63cc5a3ae6b973ac3b26c7e46de9a04b818"}, "7fd83d10-afc0-4ab9-8da9-a2babf1b04fa": {"doc_hash": "86418691bb4a64404112e853a070aa0dda8753d31beedf110b3a4038f3599f1b"}, "df4c8a79-9c9e-414e-83f1-2c0133ca69d7": {"doc_hash": "87d66d35ca66ae8e54c553f71b66d4ec5afedcac8fac6dd6fa19351d6735b69e"}, "75de8181-2894-433e-97ab-01c1f0071eca": {"doc_hash": "ab7e949c11b583951aba18447ecf6abd5b6463778b2598e8e0d065fb78518740"}, "8925ba19-3aed-42ac-9f53-68b59387da55": {"doc_hash": "3df9de45b35f19ff611670a4bd1838d5a634b7094c73911026ff8a0b2bedbb92"}, "e72937d4-f3d9-4e98-b516-d8e7d3952d0b": {"doc_hash": "4a41528c78a27cddc81a5ccb4b901a5b768ad407e95fd78fcddddd000a4bc090"}, "e0cffa38-b905-4946-bb97-d96a8f789334": {"doc_hash": "39dca19b14e1c8b7d5b5c33732b06061ebe54b0203dd6aceb246d9bd9381b731"}, "b8c82527-1f8c-4b4f-a4cf-fbb04c346a5c": {"doc_hash": "6b5854671b37204df0e96283cbae3bd775b19760aa55e3d82b106917014a6f9d"}, "d51fe16e-98e8-4162-be95-60fa61ff96db": {"doc_hash": "261457212014ccbb02d291c3abf0a76e1ad12b7994665094d56c5827ed90f69e"}, "3268defb-8a93-41fd-84a8-5fb4b8633000": {"doc_hash": "3fc64cb09a99979f4547ed3dd42bb3425b96b81c7f7ed2290f160b022c670231"}, "34634f5b-ab87-4b7b-ac38-2b858e403710": {"doc_hash": "d7cf4aec17ad12bb5d612f5bd2245892178352e886a96d72e11811ecf19e0c59"}, "d55b8927-c92e-45c5-a5c8-cf27bf74860b": {"doc_hash": "4339d5d13420b8f6e55038938402f615e6e3045e200fa511ccac39dfd8aecd17"}, "0177ad22-60e4-4176-95bd-1f4d92675c7a": {"doc_hash": "27a82698d8860f22b0a465dafdc8c9effee324549d74782561c2c6b54709922b"}, "5dc6c946-8754-4f1c-a10e-1b24399902a0": {"doc_hash": "913c5a0b4f3d01a961bf6bcbdcedc760120d31e83d64477b4fb6a43c631ba64e"}, "16fd2e34-8661-4563-83ba-97061daab5f6": {"doc_hash": "bcd0778702703429a78110ed2331a970e917e208a598007e24a8961b45f395f9"}, "91bbd494-66ea-419c-a8c5-0e2e99927248": {"doc_hash": "8e393ca48c4603a15e8889ebd8e0df81de3a39f873953c7e62512df23b6780e5"}, "95ed5c99-8f17-452b-bb14-0a0a89cd33af": {"doc_hash": "c5a55e78add30aa9cb75664e7d146bc4da61d3c48b86ab93f1f87adb49d45f09"}, "ebd047c6-66b7-4795-aed0-6b02b1413e60": {"doc_hash": "f4c6ac5b5709bc324a0425b13a5ed6e673b4fec6db2eb9ef6424cf3b0bc916d0"}, "ad3ac395-2e77-48ec-b51f-efba9b6658e7": {"doc_hash": "cf230a35be6511800806b3e9b34520a68dd3101fedcc3f082545ec085f89423a"}, "af2f2297-262b-4c2a-a05e-dd5d1f356408": {"doc_hash": "e44cd7c0297e6e54fcbe814fb58c2896d891e989c284755daee6b951d954b0fc"}, "5198786b-0ad4-4039-a302-42457a51128a": {"doc_hash": "8e7e14abb4aca958853ed66063a835fd22c4e57fe9348afa3da288a508d0f09e"}, "f50d6d98-bda7-475d-8ac4-b72bbb409b90": {"doc_hash": "e5d67a181429e69789b9af4871d85fa02afd52f1d947f17a8ae0f241d629f139"}, "2f76df3b-fa06-411e-a400-291d0701813b": {"doc_hash": "8c07a5205f6df87675fa5fc2476c19013f486b8a9926194d78764ff17a72325d"}, "c02e1ec3-1955-4c54-b611-61d8fdb8e6bc": {"doc_hash": "ad02b814b82706c2a358d5291e861eaec0d293a7d9b4e848cbe53b8d95983935"}, "0b7f06ce-bb26-4fa5-9531-9fade38e867c": {"doc_hash": "878edbf03b60e50d425c81576e8a047e969cf65959d7838665fa20de97cb11af"}, "ab2ee5d2-a4f9-45f7-ba9f-ef20d4af81ca": {"doc_hash": "58a9539081699f32cc4272ed0651999120ac0b7e721b31656f29fe7766ae65a9"}, "7c1d8cb9-5d2c-453c-aef1-63ae2b74be30": {"doc_hash": "7226a5ceb668851f525dab4fef4dd311a6511679c468beae998c916ffdd28ddc"}, "e54825e2-f79d-4230-9479-782e0e6d891b": {"doc_hash": "72b06f0c111dfe2006bbd10484066ddc6e4e376c6363f6d8647fcbf2cd788d1e"}, "a30ac3c8-8693-4b03-bacc-0c9fd5839ef1": {"doc_hash": "ca0fd9caf00302c82031697ac4a2771eced1668d2c9943c3adae7f0dd6ec0834"}, "0d86b379-a6e5-4a09-b263-48cc07e41376": {"doc_hash": "c9650401b492d58d2aca7c79574538e705ba9f46a5330b894ec59f06b022951a"}, "d9cd65c3-9bf8-4711-81fd-42e5a3549eb0": {"doc_hash": "7f94571c244971009db5ccf097b21df4157dc87a2acc11770e4e081a3799be5d"}, "1e884046-6e61-4d08-a9ac-dfd68dc5aa65": {"doc_hash": "40f697dcb8a0f4a19742820845e0d1032995158425fbf98a17b859f3a6c68496"}, "1d34b3cb-958e-44cf-9a8e-dbdb09012c87": {"doc_hash": "eafc39a0b64acfb57c26974222b06491859f643efb822a45506338a6f3f41873"}, "35b475bd-e3f8-4fd9-bc50-96c6617246ef": {"doc_hash": "6a6f570003895789ac9f03bcd1705a129227513aef56744037c0144ddc0fe3a4"}, "3e384ff7-5141-423a-bc92-69856bab59e5": {"doc_hash": "a793d2ca4da74c7f54aa510cadffb19aa34a5f251521201b04184d678f40d374"}, "5d4a0209-60cc-472a-82c6-62d405bb3988": {"doc_hash": "07dd6e8181b64c66e749e057febd615598b4ca9e91c0f48f8045279d4091c24d"}, "1a4e0c2a-841d-4c8e-8900-45c38addbff8": {"doc_hash": "e9b2577ef9d62b142c6b84fdf08d0854861b439cfcdfcafe280c33ea0f38d790"}, "8c7c4363-4a3c-40ca-8522-72ade53f0968": {"doc_hash": "a65f74bf68e4b224d1297ec29dce04c4272f87515fcc25ad5d0430b8272b5b9c"}, "707b5854-4e98-4c49-bb14-5c58767834b9": {"doc_hash": "737f3598b5566cd72646a258e683c4f78cf1ac6f603fa7cd9fa2daccdba180d8"}, "9afffa19-c059-480c-af5a-8e971b57381e": {"doc_hash": "38cf6a8251ecc318f33b21d7e961592a5e78145becbb4704911f9eb885067c25"}, "7ea657a7-0df6-4d32-9c27-b3384842c752": {"doc_hash": "75371ea3ef4bf6f5e3ccb07fe6551d8228889670f055fa8828d08d9e2aa45a8f"}, "d6aeadcd-0426-4253-9f8f-6e09017ccf8b": {"doc_hash": "4869c4df4c781f8a013c21d9ba7ace6864a40f61d1059c10cadb69c012519b1c"}, "de4fd050-914c-477a-87fe-16bfebd34550": {"doc_hash": "8511f9f0bb2df3f27184ef94ecfcea5e26ab40c2287baa97be212ca32ec1b483"}, "7926f71f-4c4b-4e34-9079-672324d252a9": {"doc_hash": "4075ba7cad6b762893da36ff35a5a08def3ebabeb91ab4d47a3045b3b08933db"}, "0aecbee5-a75d-4421-858a-c7ee37042a3b": {"doc_hash": "790fc2cb8ce0c57dcaf8af11f21cd0cb0650231af00bb5cef42bfdb6f15f8bbf"}, "73683619-d9c7-41ca-acec-a202e5bce026": {"doc_hash": "365df878a6b6914c58d1c60716e98e5b7513c988e71a4680fe2d8d1bf1463248", "ref_doc_id": "8fe7f791-0df3-4c83-a0b1-727c09f05980"}, "2fcf50f3-9efc-45a6-bc93-d0a7686ae7a7": {"doc_hash": "e9bd49ada869938aeac374722d49a0ba04d0509d170b4a64afe7b9075b743f19", "ref_doc_id": "8fe7f791-0df3-4c83-a0b1-727c09f05980"}, "77bc0db0-4d6f-46ef-9d9e-e43c6bc66571": {"doc_hash": "3442a12ba5f2d888626b580048d45ef6638680488596f46625b131f716bcb0d7", "ref_doc_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b"}, "15c9da0b-798d-427f-96ac-e9cd5f8563ab": {"doc_hash": "67817cae4653ef24480a75e1dd3de637892f24e7602fea465d0e01da2f61d28f", "ref_doc_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b"}, "bd8ec96d-9a79-4e29-b0d3-9b1776ab5ae2": {"doc_hash": "351b8b901d54c1121aa81cf41a1c105a2da25daefc97adced55f8872ed67eff4", "ref_doc_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b"}, "6a1e9e6a-8977-4dc2-869b-27bd69d8434e": {"doc_hash": "65bdecd363de1a3906a8758c5c8124717f37689be6d1418703cab92dbcc60c3a", "ref_doc_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b"}, "5219db5c-46c9-4aa0-a935-56ad72f09a63": {"doc_hash": "7c540f430ae6c1fd25dd3499e5a2205a21934765254e88528f24d83fa3500026", "ref_doc_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b"}, "a55eb263-fe23-4a7d-8720-8044cccc30f3": {"doc_hash": "06d14c003a0bb375d35737d0da6565608cd9a204d5d7b7ec422dc0f5e19610df", "ref_doc_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b"}, "1720c4b1-2a33-4440-aae8-f872d7e4c222": {"doc_hash": "93729f0149ba1a46bd947dfc70f2e080c0c51e765e04714642ebdac0c4e2980b", "ref_doc_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b"}, "ff07bd8b-9610-4a23-8e21-e6c05db444ce": {"doc_hash": "f364ca315c15835aa623c880a3097da6ea1da83fc925791b4d5fb54ba6a1590f", "ref_doc_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b"}, "7f2a9c01-092f-4e22-bce6-720b6a33e8b1": {"doc_hash": "a7e03fc311fe6373b15a0067646304611d3b4af6ad212f6c74fdf56ef8763f66", "ref_doc_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b"}, "a0045dbd-4d80-4535-9428-b97c677e9728": {"doc_hash": "07d9607db48bdc7724c6f5e44e283bf8ca682fcd3091c863080ea42b33f34dc5", "ref_doc_id": "7a699566-66b7-4775-b7f7-325184d696aa"}, "cafcd2d9-05d6-4819-9f2b-8629495848e8": {"doc_hash": "56e7c4c99e0f1f1c8970a176e5b248c2b2f334843e35ccd9732b5c0dfd00cd1a", "ref_doc_id": "7a699566-66b7-4775-b7f7-325184d696aa"}, "91c514b5-e421-4bf1-bd0e-644abe3641b2": {"doc_hash": "4628dd7779451a8dc2b088d18e23aa75e774ebe8fe242a193f923019b0c96fb4", "ref_doc_id": "7a699566-66b7-4775-b7f7-325184d696aa"}, "8de2c143-587f-4705-b9ae-746b924ba119": {"doc_hash": "840e527b98252b3fcb9f324ff0f278046af44d6e5e0c842b4d0ec1f148114c45", "ref_doc_id": "2e780022-c55a-4597-9dfd-33e57de3178d"}, "2309b812-4a50-4ea5-a228-0904d1b2e1d5": {"doc_hash": "ccc89067bb8beb264bba595b74abacdcea2f9c314ec1658403b00ca6a6868fab", "ref_doc_id": "2e780022-c55a-4597-9dfd-33e57de3178d"}, "b9ecf477-266b-443a-a100-ab52983c1252": {"doc_hash": "b7d107fbc217ef31b4cb724c343c8d15e7d4ceaf68f585ab3127ec6870a53f1b", "ref_doc_id": "2e780022-c55a-4597-9dfd-33e57de3178d"}, "342a0277-e10b-4a3a-9263-3b70fa4a7aca": {"doc_hash": "223b4bfa9c94567c8e5f804e34084fc1ee620b39445721198b20022ca3715d6f", "ref_doc_id": "542d7e53-3d40-49e8-b27f-3ce12cc7fe27"}, "ffb50b71-522d-41b1-a83c-c2ae77f58958": {"doc_hash": "e5111d19a2ca89f6234dc84aff862112fcb16e3d60be250272cde6620b0c317e", "ref_doc_id": "542d7e53-3d40-49e8-b27f-3ce12cc7fe27"}, "c94f1182-6f0f-47b1-b1ef-2c2845a7e271": {"doc_hash": "94dcdab592ca8b76a298e7623a6c123bd2ea4ba66e62c1392d8d6458747cdaa3", "ref_doc_id": "542d7e53-3d40-49e8-b27f-3ce12cc7fe27"}, "a48cba17-a9e9-430e-9630-1bb97bd493b5": {"doc_hash": "61db41c1014734dbc809af7d19c84973a42f075369bef022d5bed77fc60c9974", "ref_doc_id": "6151682a-a138-4fd1-9963-86e2b323bc47"}, "a084e2a9-0c44-4f8d-885f-a8511c724ca4": {"doc_hash": "4305c8c8cc900f08aef28d1833616eaf0a3a7ed08362a0bcf60c35f6c4f53b29", "ref_doc_id": "6151682a-a138-4fd1-9963-86e2b323bc47"}, "1fd69a51-1fba-448b-8ac5-f89408c6b676": {"doc_hash": "77bab1e58f45a0423706d53f9454a55ab95ec925d6f9aa3b2a738846838856a4", "ref_doc_id": "6151682a-a138-4fd1-9963-86e2b323bc47"}, "9c77e0b2-d061-4d17-bf79-8e5d2deb29f8": {"doc_hash": "16f6c6708b577c18be1e63f9771a19cf2c7b846a2c142796ddbb6b505f6a8262", "ref_doc_id": "0e2000de-b1b7-49db-8551-f13a88bd5b76"}, "d02302cc-760b-4c0d-9dd3-51850df92501": {"doc_hash": "56b420c8e982b731f89bca6d8584c8b17abd5a3a0a67ca5f3b5d75dc4f62edee", "ref_doc_id": "0e2000de-b1b7-49db-8551-f13a88bd5b76"}, "85a46433-678d-41e0-9b84-7f1b40f22ad1": {"doc_hash": "52aa1408ee22b46c7b458939d14b362815d1da75b6dd895fe731ef520b4a7448", "ref_doc_id": "0e2000de-b1b7-49db-8551-f13a88bd5b76"}, "ed9a2722-215e-4e55-a6b8-dbffc81c6cc3": {"doc_hash": "cfda74a94dea0649ade93d934e82897fe3478654503c1dac20ba934620f3f88c", "ref_doc_id": "f022c336-2b9d-4255-9611-95200c3ccda7"}, "92849269-72d7-42b8-9178-70b3e34be72d": {"doc_hash": "fba764d99f41f9ed87af2cd12d8335a2ef7c0ab03e88bf74cc5296829e1af51c", "ref_doc_id": "f022c336-2b9d-4255-9611-95200c3ccda7"}, "819e2501-e5f0-48db-9692-060a8a7568be": {"doc_hash": "e137b38ed3bf448f8f7ad237fd7121b30545dae05f4c0ae1f4b12a97b4ff0d27", "ref_doc_id": "f022c336-2b9d-4255-9611-95200c3ccda7"}, "30427af7-31a2-4d3e-9821-75fce86ba9f7": {"doc_hash": "1ebf0ef47a393d47b486848d9f7dd60ee68b7ba48b05aaf2fc0c462401030b89", "ref_doc_id": "09a58be2-948e-4afd-9266-81ab087339de"}, "7922a6c8-3912-4df7-b23a-2ea38d3f3aea": {"doc_hash": "63e183f934406b48caca26b66d1cd5ff80dc49b6cf6a4197ee653d6e6e90508c", "ref_doc_id": "09a58be2-948e-4afd-9266-81ab087339de"}, "0c8d9dea-1adc-4679-8bfa-ad5e4f04f63d": {"doc_hash": "bd956c28efe6c8905e876634aead2541df71817ef883636c5963d17532aea3ce", "ref_doc_id": "09a58be2-948e-4afd-9266-81ab087339de"}, "b963f9e4-eceb-4f04-918c-38c21df6d0b1": {"doc_hash": "96876852af3d8591ed18b5f762ae7f0207ce133052fe122f83a0c63729637cb2", "ref_doc_id": "cc66b82d-aca3-4dc3-8a08-7e38a0169aa7"}, "52473bb4-9388-498e-b70f-53545960402f": {"doc_hash": "8542e5c96b0fe3395df0e4ccdd1b43c3d3cabe3d478c646edbc83dbcf6cf5984", "ref_doc_id": "cc66b82d-aca3-4dc3-8a08-7e38a0169aa7"}, "c64fe3c9-a89d-4d19-99da-dc52157e6963": {"doc_hash": "13790bcca6095a5baff8a9135ea34653b03582f1f41abe6e9ec204e3096d1bed", "ref_doc_id": "cc66b82d-aca3-4dc3-8a08-7e38a0169aa7"}, "94a68eee-16b6-4f33-ae36-3b457afb3bb5": {"doc_hash": "527c079c1a8c368d39761d96e971dc036e1b4d27a8135eac0bfa5d2089898062", "ref_doc_id": "483f9e6c-4e8d-4a8b-9a6d-b91fa78fd616"}, "12f767a0-a741-414e-967e-e9c483b15877": {"doc_hash": "7d50ad939385b0acccb7a16db4bb6d54afb694a18ff33e1527af09c0a6e9e574", "ref_doc_id": "483f9e6c-4e8d-4a8b-9a6d-b91fa78fd616"}, "c11bd8af-a63e-4251-aaa7-d48b91a1d92d": {"doc_hash": "6fc43dd786611ea878e1c83c99f766dd9110da11ba09a5e58fb5cef6b8701ba9", "ref_doc_id": "483f9e6c-4e8d-4a8b-9a6d-b91fa78fd616"}, "cb5b3aea-572b-4a0b-be39-ce380715c141": {"doc_hash": "b2b00fd76c8f21ccfa007895f59c81270011e87abbb33d7c8e843362b1f83784", "ref_doc_id": "471671c9-c130-4201-97a2-6c830550b8db"}, "4dc1c486-bc29-41e6-89ca-05fbcb387af1": {"doc_hash": "246e16dd6b75b979bf91a72e6586fab9486bd013ce980ed1a7174870346693c3", "ref_doc_id": "471671c9-c130-4201-97a2-6c830550b8db"}, "08f36c52-7469-4903-ad2b-e19180dd5385": {"doc_hash": "f6d24dc919a8aeed526d3f03923386ed6366e320039de77180a3c8324867bf75", "ref_doc_id": "471671c9-c130-4201-97a2-6c830550b8db"}, "f5a1dede-4fc9-4fa5-953d-6d13a1221268": {"doc_hash": "752866ad47c3ee933f3120c38e15123ab1e95a9bbc3e80ad44a321a9b06398b9", "ref_doc_id": "e76590cc-06e7-4aea-8441-e2dd363f22d9"}, "45d61ee0-cedd-4764-bb2d-59643d40227d": {"doc_hash": "ecdb29567bf6281f87d806125f6dc48a1af77b94cc0cf6de186162561f25dd03", "ref_doc_id": "a4cb70d6-c274-4d05-8f7c-3562a7b5a3c2"}, "b524774d-03e3-4331-afc4-b05f840260ff": {"doc_hash": "cb09bd97442fdbe5e64e83b20c1d1dd4d8a2d79c867841c5aa70ce812354bfc8", "ref_doc_id": "a4cb70d6-c274-4d05-8f7c-3562a7b5a3c2"}, "1cc14555-df9d-463b-8d77-b9626057d74b": {"doc_hash": "ddfd19d61770807a6775392e7a4074fcecb7b9e0835c2a3b2a30c8ef05732c02", "ref_doc_id": "a4cb70d6-c274-4d05-8f7c-3562a7b5a3c2"}, "64f25ea0-264c-477c-aef2-3185c548ffb1": {"doc_hash": "da0382084c386a3cbc5f934d7706ba15c7b13de8cdbb7f1e85f113e5767d406e", "ref_doc_id": "a4cb70d6-c274-4d05-8f7c-3562a7b5a3c2"}, "d0e0545a-0ac5-4755-934d-65dfd5170fd5": {"doc_hash": "9da5c09788f1f04759e3e5b972744a0a028041b22de8a1b7a660b885d3a8e635", "ref_doc_id": "a4cb70d6-c274-4d05-8f7c-3562a7b5a3c2"}, "f1b7ead6-334e-4bab-8b28-92da4c16d369": {"doc_hash": "7e3dd2e4636eff88b2c3b071e2f54c96edbfcd37c3ccc7cb5a091a7f54a1b3e0", "ref_doc_id": "5e12086a-da1a-45f7-a4a2-61acfe5763db"}, "1ce39cc1-62ba-493e-9956-13eaf90e8797": {"doc_hash": "26a90d724f6d1d241b5a2e574334dd3f8653c16977b0e7d1695807291cfe8398", "ref_doc_id": "5e12086a-da1a-45f7-a4a2-61acfe5763db"}, "e24752aa-02a9-4e8c-919b-8ff8784d527e": {"doc_hash": "c00fc0bc922ccf483c904f9f554e2f72e400bae54e1e5f057569c92d9ffa72ad", "ref_doc_id": "5e12086a-da1a-45f7-a4a2-61acfe5763db"}, "fbb1a6d5-e54c-44ee-8c63-a39c78ea30f1": {"doc_hash": "f9aebec557841641a0a47ccc74dadac5597d07ba58eaed71f90dcc68973028e9", "ref_doc_id": "5e12086a-da1a-45f7-a4a2-61acfe5763db"}, "2ebdecee-617c-465b-a07a-deb11920bd47": {"doc_hash": "60b2638c57e2b2fa754e89fed95a662ed5a1230928fa92076a14ab707c3634e2", "ref_doc_id": "5e12086a-da1a-45f7-a4a2-61acfe5763db"}, "125f3631-432f-43ad-b65a-49444f75201b": {"doc_hash": "b2ed1fa006bdb3d5443c63132dcae2cf00a9d273f48bec907309bab8a7b04991", "ref_doc_id": "5e12086a-da1a-45f7-a4a2-61acfe5763db"}, "6076b0eb-bb62-4e5f-bc01-16e2233317e9": {"doc_hash": "fb1143783148ce8a96944778e311f07a021d36ca67178ec22a8aa2a85dd101b1", "ref_doc_id": "c5d884e3-745e-43d0-a430-d6f985ee2b15"}, "f32e4167-2373-4926-b914-fe6f93b3fbed": {"doc_hash": "3697a85fd083f02b7b31012389e6002ac55c5181604936e73155560855dfcdf6", "ref_doc_id": "c5d884e3-745e-43d0-a430-d6f985ee2b15"}, "b779898c-20aa-4835-beed-ab538e96b831": {"doc_hash": "1a0699c63ee1627b3e282efeceda6916557eaf367059ac06c3ce9b8ba252cc06", "ref_doc_id": "9e817b24-c86c-41c1-bc96-15c2bd748942"}, "cc507c24-d73e-4f0e-a68c-ffe149806176": {"doc_hash": "18c149660f5f6eeccc24754efeae6f623645a36307330a0d26d202f53d0fb8c1", "ref_doc_id": "9e817b24-c86c-41c1-bc96-15c2bd748942"}, "65e6d4fa-b0e7-4caf-b68a-040933c97eeb": {"doc_hash": "f412933e5d79b719aa225ae2c3ad76d2404f6e418af39c00677210f826126c21", "ref_doc_id": "9e817b24-c86c-41c1-bc96-15c2bd748942"}, "30146c2c-f4cd-46f7-9b67-ae6f6b22eee2": {"doc_hash": "d2eab1926f5e418c496f25729a025341d6d31479efa005f16a0c050654da8742", "ref_doc_id": "b47afeb6-caa7-410b-ab54-6b7edc2a5571"}, "8a0e7340-8945-4915-b980-3e6bf39a930b": {"doc_hash": "43cafa6f898fb34698821bdd89da419c4d66de151fbd1c0262f82bdd6133dacd", "ref_doc_id": "b47afeb6-caa7-410b-ab54-6b7edc2a5571"}, "892e5084-a8b3-47e2-bdb6-99918e767acc": {"doc_hash": "d994710f61b42876331e8028bd05bddd37da2145a60dfe1da5501b7097b8cd95", "ref_doc_id": "b47afeb6-caa7-410b-ab54-6b7edc2a5571"}, "a2467112-7946-45a5-aa5e-248b162cc546": {"doc_hash": "a39737023dc9818dc53becd0bc227e435fe21f353ab59887117365ae302cf2a4", "ref_doc_id": "6540d239-e9b8-4be6-9477-7c22b6f40b05"}, "ee6372f7-f846-4363-b88a-b87a2aaae5c0": {"doc_hash": "6943fd96ff27a2f5112b912a43df4e52b47c5ef8d5fb8382b08d911c24f329ae", "ref_doc_id": "2e9928d3-6a8a-4719-bdcf-e1f85560b13e"}, "a5e5c86e-07d4-4867-baf1-de1868428f42": {"doc_hash": "1ba2c0f463b9e5351afd4d1560d48dd627bd3e102645c76263be6ddbf0b5740f", "ref_doc_id": "2e9928d3-6a8a-4719-bdcf-e1f85560b13e"}, "de4269f2-1ff5-46f8-b0e8-c293802a3d5f": {"doc_hash": "9012b839c1964e06ed483ca4e15860a6ec6fab68e1f11a72b046a0ce7dc7c8f2", "ref_doc_id": "7fd83d10-afc0-4ab9-8da9-a2babf1b04fa"}, "a7e4a0db-c132-4cf1-a17b-1b49a202d6b0": {"doc_hash": "d1e022a6dbd57d1e31268751ee6a2b8bb72d1de2e4084ebfd250e90b13ab76e6", "ref_doc_id": "7fd83d10-afc0-4ab9-8da9-a2babf1b04fa"}, "70ca7e62-0f41-4481-b881-29dbf8fa668a": {"doc_hash": "9e4bbe54dcdc073b5b05ca3eaca9d5ebd5b87c177aef9692d20d4afff951c12e", "ref_doc_id": "df4c8a79-9c9e-414e-83f1-2c0133ca69d7"}, "1efaf9f5-0d96-438b-bb38-554f9275dab4": {"doc_hash": "4b72b1ff9caf435da57197038637ccf37c482fdbd7d7e7815705b338e61ce83b", "ref_doc_id": "df4c8a79-9c9e-414e-83f1-2c0133ca69d7"}, "46f985eb-eda5-46c7-8e45-444ca4bc37b6": {"doc_hash": "7c35da7225162ee64b1b09ae7c9d01ed9753ce52f0a7289bd1c732ad2031ef73", "ref_doc_id": "df4c8a79-9c9e-414e-83f1-2c0133ca69d7"}, "6e8210cf-2387-4d76-a7b5-988a1a6cbb6d": {"doc_hash": "22ec621b32b8faf348179d614f402ffe2e214733b6f55138a377e4c484b915a3", "ref_doc_id": "75de8181-2894-433e-97ab-01c1f0071eca"}, "c9efea2b-46de-49fe-9bd0-1815101b30be": {"doc_hash": "776d2dafbe7ffe780be73331372196bdbb680da259fdd05d418af07da61d8656", "ref_doc_id": "75de8181-2894-433e-97ab-01c1f0071eca"}, "289320ac-592b-474b-8e7f-6cec29db816d": {"doc_hash": "4cb1bdbc90a9c810f029b8a6833974178d66397e5949b77c4d4747e4d31d00c1", "ref_doc_id": "8925ba19-3aed-42ac-9f53-68b59387da55"}, "32fdd2c6-5d40-4a8c-8683-613efa20df90": {"doc_hash": "5d64e0bf861086a9956e56b4ad1ceb91afd787c95373fc0cf2749a1d055aebc5", "ref_doc_id": "8925ba19-3aed-42ac-9f53-68b59387da55"}, "4088ee3f-49e7-4cd8-8ba2-35c961116bb1": {"doc_hash": "01f0a78ed2023883251279e526714fe07500ce369cc319357e284831f1381c00", "ref_doc_id": "e72937d4-f3d9-4e98-b516-d8e7d3952d0b"}, "2e511fbc-5a81-4644-b301-d501d7a53f17": {"doc_hash": "c4de603ef5a13ccbddd48d54a504a9e66c8f280fc3e41e191f686fa315cf6ffe", "ref_doc_id": "e72937d4-f3d9-4e98-b516-d8e7d3952d0b"}, "641e96e8-b42f-40b1-9b65-77d69356bad8": {"doc_hash": "8d2f82d9caa42265aadc45169c6c46f35c8831fe3f2a4a49f3b3ee185b3c36e4", "ref_doc_id": "e0cffa38-b905-4946-bb97-d96a8f789334"}, "a3cad464-7862-4346-b54f-b50c14c485e9": {"doc_hash": "e7f670d4a8bb7b68307c67361c2d3ad46caf176f502178ed7870ada4436dabd7", "ref_doc_id": "e0cffa38-b905-4946-bb97-d96a8f789334"}, "49d78103-c38b-4c64-a8a1-0c4e7495f6b8": {"doc_hash": "8f966a34407dbe483a35ee63833d029c44271b626c592f67cc99819e2caf7b11", "ref_doc_id": "b8c82527-1f8c-4b4f-a4cf-fbb04c346a5c"}, "e26484bf-e9a7-4849-821b-a9c8ff7cadb2": {"doc_hash": "627f175c20075b548b5b8b0c06178309a5c8c9849f0ed9ea0893fdb79649bc14", "ref_doc_id": "d51fe16e-98e8-4162-be95-60fa61ff96db"}, "c01a7198-fa33-49f0-8f2a-5e4e68c1ae68": {"doc_hash": "0d4ace2a33873e42aa19ff5ff2a23e2aae97840880334c8e25e55e7bd2a43f96", "ref_doc_id": "d51fe16e-98e8-4162-be95-60fa61ff96db"}, "9026eceb-3364-40ac-bbca-3e7eb22ea7ad": {"doc_hash": "e556b0c8f3fdb39ffd800da4313a930c55e9af762e3d18a0aed0a06c1788cb9e", "ref_doc_id": "3268defb-8a93-41fd-84a8-5fb4b8633000"}, "7239e08d-d0a6-4ee3-ac36-bc79310b3029": {"doc_hash": "f22a1889889fceea92ea5a234fd8ac8c602c77e8c1ae60fb377cf870b71e7a34", "ref_doc_id": "3268defb-8a93-41fd-84a8-5fb4b8633000"}, "bc95d600-23b2-4404-810b-64d7740631ab": {"doc_hash": "a90cb3445e74ee12505f038089ce6c46ba0acdc791776750f1cf231c2ae53eb8", "ref_doc_id": "3268defb-8a93-41fd-84a8-5fb4b8633000"}, "f8c6f3b2-e9ad-4527-8a36-6bdb5bfac9b1": {"doc_hash": "a21cb85e09322e29e37629e8ab5b93c1ccf9a145f96e392a24fbcf0d03e4fba1", "ref_doc_id": "34634f5b-ab87-4b7b-ac38-2b858e403710"}, "651819ab-c8c4-450f-ab10-75abb1c9112d": {"doc_hash": "767acd9850da185200e10b15f1123b12b1a7ef45bc1b5899495f067b7589dee2", "ref_doc_id": "34634f5b-ab87-4b7b-ac38-2b858e403710"}, "cf22cb2c-eb68-4fe8-ad0c-6f38cb315979": {"doc_hash": "689abee2caaa0dcf9e1a762131e79f9b51ad56c9e3f6a763f64ed5f4b0fa9714", "ref_doc_id": "d55b8927-c92e-45c5-a5c8-cf27bf74860b"}, "384c3914-52cf-45e4-afbe-aaa39e9e27a8": {"doc_hash": "a1399c6e185f5a424dd7c2932af18dbdeb967109a5bed6e17cd0f9544fb0f26b", "ref_doc_id": "d55b8927-c92e-45c5-a5c8-cf27bf74860b"}, "9bf31843-d616-494f-b39f-7d0678d4ddd1": {"doc_hash": "c367b4ee9d182fd44f0065aca4ee251c040cb7c04aba57376dbe94f27ea583e6", "ref_doc_id": "0177ad22-60e4-4176-95bd-1f4d92675c7a"}, "8b991390-00f7-40c4-ba5b-fdc07433044e": {"doc_hash": "13a45485bbba0981af4f88cc381d9047172c593cf55628709b2dff57fddb8d47", "ref_doc_id": "0177ad22-60e4-4176-95bd-1f4d92675c7a"}, "30b419c3-c04b-47bc-8250-bceaf6b41f5f": {"doc_hash": "8a12f81a8909cce45d5f05a2ff54eb9223fb26a4e14b8ccdb0601f6f0b853b87", "ref_doc_id": "5dc6c946-8754-4f1c-a10e-1b24399902a0"}, "72c0fc9b-2702-45a5-bca5-7d23c2a001c7": {"doc_hash": "81d97f905cd87b4e8555b6ba7b0480146a37e204ea55cd4dca072e5318bb6d65", "ref_doc_id": "16fd2e34-8661-4563-83ba-97061daab5f6"}, "0b836885-ede9-49d0-9a7f-e6812763e8a6": {"doc_hash": "e2eafb3b50d1e81b965bc785cdae50226ea4c949949d4836af88404004892cf6", "ref_doc_id": "16fd2e34-8661-4563-83ba-97061daab5f6"}, "fe966015-65d2-4a9c-93a8-ce6a55c4014e": {"doc_hash": "5503dbfd78efdddc328f6a130d24f67786341397081a01508de85e3e1410bb01", "ref_doc_id": "91bbd494-66ea-419c-a8c5-0e2e99927248"}, "fb9098f4-7962-483e-a2e5-15d2734619fa": {"doc_hash": "1ac9c6d53694751b80875b34574eaf1827ed5810d3d5b2d4b0f32c13ee2ff792", "ref_doc_id": "91bbd494-66ea-419c-a8c5-0e2e99927248"}, "d5a0bc12-e0f9-42d5-b45e-bba001ffa39a": {"doc_hash": "9dea3d19f2727d43b18bb795d45985c1423a47eefbb61db5213e830a3c879b57", "ref_doc_id": "95ed5c99-8f17-452b-bb14-0a0a89cd33af"}, "3c8b5395-dc88-472a-8191-fba27a69cc96": {"doc_hash": "a5a724a6d2a9885f7337107474eef2c0286f144b388f350dfbb44f4262420fdc", "ref_doc_id": "ebd047c6-66b7-4795-aed0-6b02b1413e60"}, "d5eee747-b453-40e5-9801-b54f469a6d94": {"doc_hash": "77877669b181dd07f5f6c9a405af5387c8f66732b9d7443a6f88f5b7949da7ae", "ref_doc_id": "ebd047c6-66b7-4795-aed0-6b02b1413e60"}, "56d067d4-ae0d-4b0f-a9d5-4f60096b3b45": {"doc_hash": "b78dd415dd7e978e66e4e9f0537c8fe5be99f957043f3931c88204e18dd6e734", "ref_doc_id": "ad3ac395-2e77-48ec-b51f-efba9b6658e7"}, "4c7fa795-7e64-449e-89a1-fb95fa8bd3cd": {"doc_hash": "1a43e98e02d119837061cc93407f2f427d04c95e6adb89125c413c59ff6459da", "ref_doc_id": "ad3ac395-2e77-48ec-b51f-efba9b6658e7"}, "136f7dd4-eb94-4700-a805-d14e2826b8b3": {"doc_hash": "2d35ed330bb2c11179a5bd49f2cd6ed132ba7b739c330ee64827620762ff3e34", "ref_doc_id": "af2f2297-262b-4c2a-a05e-dd5d1f356408"}, "4c2d90cb-b27b-4870-82a9-d73c62260e1d": {"doc_hash": "7967641c0c883d7c1e8a6c1689a2fbebb08b2b9dc4f1ffebe6d3a6d43e56f63f", "ref_doc_id": "af2f2297-262b-4c2a-a05e-dd5d1f356408"}, "8a382839-bead-4c3a-92dc-720364648d83": {"doc_hash": "31f261a89d06a324e61e5a5323728a9ac25bd481fac1168422b2d77580307af7", "ref_doc_id": "5198786b-0ad4-4039-a302-42457a51128a"}, "7ff918b0-f6a4-420f-8ac3-d9e771decf91": {"doc_hash": "b155da75072a73933704b853361a6169b9e10812ba8008afd9a69dd02cc724d1", "ref_doc_id": "5198786b-0ad4-4039-a302-42457a51128a"}, "e9c8e0ee-8a64-466b-9130-7d10d9747c88": {"doc_hash": "c0c0cf74575a35c7eda0ef643071356f3b4d42d141b3fff3acfb603583e0e97d", "ref_doc_id": "f50d6d98-bda7-475d-8ac4-b72bbb409b90"}, "73a3a64e-46ef-4a18-81b6-bd8febbfee64": {"doc_hash": "942d6f58a8223556b1d25c718c03395f0f1cce0c0ef32e78afedaa9c3f67b11f", "ref_doc_id": "f50d6d98-bda7-475d-8ac4-b72bbb409b90"}, "5c13cf12-d797-40e9-9e12-5cb01332da8b": {"doc_hash": "be2d8066a21d6ac42ddf03d44427047ccb2ceb244b641c652dc0e9614c06cc4f", "ref_doc_id": "f50d6d98-bda7-475d-8ac4-b72bbb409b90"}, "7ec93aa3-c377-497f-a7a6-7794e78892fe": {"doc_hash": "5d6d88b203dbe2da778d78f424526f97d700c658d7d43e4003d44484ca9cdfe9", "ref_doc_id": "2f76df3b-fa06-411e-a400-291d0701813b"}, "c5bb93ee-1434-4942-8594-a80297295025": {"doc_hash": "3bbeb68fe3a75850e4921a6b69b8684869a92b3d93d3a47934fc0f0647d71e03", "ref_doc_id": "2f76df3b-fa06-411e-a400-291d0701813b"}, "3932b202-d880-4662-a53e-01526a108b31": {"doc_hash": "bf94adc2dde05bd6be6647682da94ee3cbc8c54621bbc700fab857337c2c6cf4", "ref_doc_id": "c02e1ec3-1955-4c54-b611-61d8fdb8e6bc"}, "d61012a7-f300-4b23-80c6-1520c01e440d": {"doc_hash": "3ff79b61991e025f6f1420eaeb9d151e9e49ad73497d69d4c73b7f2232525ea5", "ref_doc_id": "c02e1ec3-1955-4c54-b611-61d8fdb8e6bc"}, "1a99e637-1d66-4f76-be63-0d5f0076db53": {"doc_hash": "6b8bf6004c1ee19629ff98e74b162cdd39f815ff4ecc12e90b05a7b32db45f7d", "ref_doc_id": "0b7f06ce-bb26-4fa5-9531-9fade38e867c"}, "469a47cc-bfe4-4664-9e96-47515fb59d92": {"doc_hash": "7f05ec52fa5a8a751d0ce48ed527fb1b1c14a299bc581c01115a0243b14768e3", "ref_doc_id": "0b7f06ce-bb26-4fa5-9531-9fade38e867c"}, "fe9505f8-f97a-4a55-80f9-218eb747c05f": {"doc_hash": "e51270a01463b4f0812fbd6d33a5ca8925386ddf2f818ae7178ea7b8366821e3", "ref_doc_id": "0b7f06ce-bb26-4fa5-9531-9fade38e867c"}, "ca6c63c5-a7ba-417b-990f-4e8ca4cab9a4": {"doc_hash": "b94fda0613b903905a680da40917cc114c7ad08cfb033cdcb713368e04f10f1f", "ref_doc_id": "ab2ee5d2-a4f9-45f7-ba9f-ef20d4af81ca"}, "4d3efd38-624c-418a-a875-3edc6ece41c3": {"doc_hash": "e30048566e36c12c1f0a4944569f1134f50dacd0ecc61e7748027a91c4d330b5", "ref_doc_id": "ab2ee5d2-a4f9-45f7-ba9f-ef20d4af81ca"}, "3adb667e-d9a1-4a66-acd8-03e6888a536c": {"doc_hash": "c79fd2b70efe6561385b321c871f799eac56fab5dea7c753e4fdbd335de1bcdf", "ref_doc_id": "ab2ee5d2-a4f9-45f7-ba9f-ef20d4af81ca"}, "eebe6332-9e51-40a2-9c4b-5e8becf069bf": {"doc_hash": "2e7dfc964035bc9e07a107ae022d75d3b1dec326e0c178a34a0f83c4b7577030", "ref_doc_id": "7c1d8cb9-5d2c-453c-aef1-63ae2b74be30"}, "904d540f-6917-4864-b1cc-2d08e99f7b7a": {"doc_hash": "e893ce9942f7a713d163ec739d7ab6c7c5fc1ca4f36eefefac46b100274de2b0", "ref_doc_id": "e54825e2-f79d-4230-9479-782e0e6d891b"}, "7442f82b-cb88-412b-8aff-6261d773d4b1": {"doc_hash": "2990eca92806f01c05018fe855e6dcfc63555468613e3176c75e3d12f50fa2a8", "ref_doc_id": "a30ac3c8-8693-4b03-bacc-0c9fd5839ef1"}, "ec3d26ba-59eb-44db-9645-e2a3d6e429b5": {"doc_hash": "7c4c43aefc84410ff42254f7325d1b0f7b2f409cdf8b54c0a79c8c0a62c49b35", "ref_doc_id": "0d86b379-a6e5-4a09-b263-48cc07e41376"}, "a0a2ff6a-610b-4262-94cf-f2b683fe305a": {"doc_hash": "db5e82c65b1f6da1ac613cab5f56189b2f52248437e7a32a85d9247bcfbdf222", "ref_doc_id": "0d86b379-a6e5-4a09-b263-48cc07e41376"}, "59f99bc7-2b82-4a92-8818-a4ec813e631e": {"doc_hash": "a2dc84acc2ab181030922d3492da7419238570f2152ceccfe1a8d6476897371c", "ref_doc_id": "d9cd65c3-9bf8-4711-81fd-42e5a3549eb0"}, "ecda83d1-b0d6-472c-aa67-22b4b6216cba": {"doc_hash": "d705fd36c61dc578747c96f4b46e7d3a87f01525713fc9ff0e6cbdd6d5be947e", "ref_doc_id": "d9cd65c3-9bf8-4711-81fd-42e5a3549eb0"}, "917304a0-405e-422a-8adb-b5fc2ca91a4d": {"doc_hash": "0312ec8af41faa4b36bb91cf1323c70815bfc2dca7b4e5c65fc95c27199e42e5", "ref_doc_id": "1e884046-6e61-4d08-a9ac-dfd68dc5aa65"}, "ed4c610b-5d77-4c05-baec-75bd54659204": {"doc_hash": "2ef5330e8fc79ce647c272268d57e2d3ec9e7a778828fcb8820b9f8588887fd2", "ref_doc_id": "1e884046-6e61-4d08-a9ac-dfd68dc5aa65"}, "797fae54-f6e0-4047-9572-63fe0a9e6d01": {"doc_hash": "5fd21bae56fa7d3e2861f79d16d507220d5dc9dc7d46a72e11e790a803c066bb", "ref_doc_id": "1d34b3cb-958e-44cf-9a8e-dbdb09012c87"}, "b9198871-fcac-409e-8dda-a566b1d157ac": {"doc_hash": "a6f83c2bf0c610a13e588021ae6a9bcfb4e13f61f372272663903a1e486a6d54", "ref_doc_id": "1d34b3cb-958e-44cf-9a8e-dbdb09012c87"}, "5b76221d-f6e0-4b4b-9d12-5fdce7620aa1": {"doc_hash": "08f98188f6215bf7c7562de3d4a5f4911c95e303d8b9f19162a97e084b6f43cb", "ref_doc_id": "35b475bd-e3f8-4fd9-bc50-96c6617246ef"}, "529b2a6e-43e1-42c8-9be8-df971716cbc3": {"doc_hash": "827293358757e65a0d389fc31ce0fc502428c03d29a62393eefa202f00cc6a77", "ref_doc_id": "35b475bd-e3f8-4fd9-bc50-96c6617246ef"}, "af4fd4c8-c17c-45d9-9b9d-73a30271553a": {"doc_hash": "1f7f2e6da12e588c40f26174a3d59601a1e08e507dac3452efd767e831eff536", "ref_doc_id": "35b475bd-e3f8-4fd9-bc50-96c6617246ef"}, "66a98e4a-95d5-46e4-af7f-f234f5fcac99": {"doc_hash": "74bd7dbcee11a74195327547fc6fc295b8ecd6a9678cca9b4459e4fd516359ab", "ref_doc_id": "3e384ff7-5141-423a-bc92-69856bab59e5"}, "cf687506-631b-4419-9f9c-ef3f37a164c8": {"doc_hash": "ef3331b9214fe18880e747617e17d4cffe5c09d50d598b2dd4ff4ccc40c86b4d", "ref_doc_id": "3e384ff7-5141-423a-bc92-69856bab59e5"}, "586221dd-d100-4885-9d2a-5ca9b79b131b": {"doc_hash": "bee77c4028cc1bec5273a7c33909b88ea537f089258f00a0f695e630e0a88030", "ref_doc_id": "5d4a0209-60cc-472a-82c6-62d405bb3988"}, "8d14f7d6-1c9e-4a57-ad87-6f38afdbb2ed": {"doc_hash": "587991b271f8f96c1a19e9553aacb9a79a7f28b9f6b3f0a08ae04aa6f46ff200", "ref_doc_id": "5d4a0209-60cc-472a-82c6-62d405bb3988"}, "bd63ddcd-3581-43d2-b28c-ab98caf2ce09": {"doc_hash": "73dd7d4028aed2e0faacdba09a6754bd9c533171f3f8b7fc2c4f0d66afb8d71a", "ref_doc_id": "5d4a0209-60cc-472a-82c6-62d405bb3988"}, "e1d4b10e-2d7c-43cf-82ba-939e6582616c": {"doc_hash": "dc5b11467618df8429fb7434b9f88ae0cbfd214dd7f905a8d384f6eeff033236", "ref_doc_id": "1a4e0c2a-841d-4c8e-8900-45c38addbff8"}, "d8800e22-7291-4f96-88f5-351af77e1ba5": {"doc_hash": "b8a59abde89fae5afd4643ca045ef6cd116814365ee2008035eae1a262118ca3", "ref_doc_id": "1a4e0c2a-841d-4c8e-8900-45c38addbff8"}, "774e0607-d24b-44b1-ac79-517ff73dde79": {"doc_hash": "72bd86685ebb51bfdd2d6934d15b2ddafb72d361efc21f319d9c2207aa261c4f", "ref_doc_id": "1a4e0c2a-841d-4c8e-8900-45c38addbff8"}, "e70a1c3e-0601-4cd1-992d-eeded998e664": {"doc_hash": "228d301939daeab5982ff578e210994ecc97455e3c4382b842e50445631f37d0", "ref_doc_id": "8c7c4363-4a3c-40ca-8522-72ade53f0968"}, "87defc6e-a0a2-4b5c-9966-74a1f8154c97": {"doc_hash": "2b2ed26493c0a27991549ffbccfa55396a0901fe94edb057370ee4f7bd31c56c", "ref_doc_id": "8c7c4363-4a3c-40ca-8522-72ade53f0968"}, "11443c17-7442-4c57-baea-779e91715b1b": {"doc_hash": "6099a572a766a03a1a2bd4dcc07fc8eea3e2c2292fb869ee157ea30d360c8d60", "ref_doc_id": "8c7c4363-4a3c-40ca-8522-72ade53f0968"}, "1d8bb0ad-4d0e-45f3-a78f-fe083ad7958d": {"doc_hash": "ab0925322db907902e494bc4003f007b3aca60201a0fc18550757c6302a90de8", "ref_doc_id": "707b5854-4e98-4c49-bb14-5c58767834b9"}, "2a285257-a731-4a55-adcb-7a900738db4e": {"doc_hash": "910180b7088b2a15cd2ff35931c55903f60b2909394f9e096fa223a0981953cd", "ref_doc_id": "707b5854-4e98-4c49-bb14-5c58767834b9"}, "dcf26f26-1430-4911-993e-4c88f18121e6": {"doc_hash": "48c4a1c3fd758ac302e8145ec9473f388194f9d8bb431e16868902f7c6ce80a0", "ref_doc_id": "707b5854-4e98-4c49-bb14-5c58767834b9"}, "979b24b7-0e7c-4aad-93a3-2b05f03816c5": {"doc_hash": "f4f90c3bf1f1d7058953fd15f4da2ae9869b68c279a68fcf7df79d23dccaff21", "ref_doc_id": "9afffa19-c059-480c-af5a-8e971b57381e"}, "fbe1bce6-84ea-4eb7-bc63-dfed44c50517": {"doc_hash": "c227fe426042bccb3f2c15200e4b47af20b10fa53b16bf917da1dbf8fa199c7e", "ref_doc_id": "9afffa19-c059-480c-af5a-8e971b57381e"}, "1f78ad04-fc98-443e-ac0c-6a42369faef8": {"doc_hash": "9fa6c77284a3a9507dd66a6b81ffcef2d1e50297663547c4ad5115111d687c45", "ref_doc_id": "9afffa19-c059-480c-af5a-8e971b57381e"}, "c217c2e2-8542-453f-829a-017a3c8a55e0": {"doc_hash": "6638aa7b71bdc795c502fc0824c9a4b844aed3c70906a88035e1eaa29c01d062", "ref_doc_id": "7ea657a7-0df6-4d32-9c27-b3384842c752"}, "6508351a-2c61-4275-b22d-446bec30cedf": {"doc_hash": "12ea5377e53fbf3c76ab0600f515cecd3b3d5f7f347d7f7fd712c2ec1ed7bd5f", "ref_doc_id": "7ea657a7-0df6-4d32-9c27-b3384842c752"}, "6d593f51-ba53-41a9-8c4f-df2be1d4cc4a": {"doc_hash": "c2ef514aef8a8a8b6cbc319f0c26aab52f2f2cb5760ed4b1b8870d4ac2535715", "ref_doc_id": "7ea657a7-0df6-4d32-9c27-b3384842c752"}, "ddb7948f-45ef-4220-afe6-3c117218fbd9": {"doc_hash": "f2179c59b07dc7b33782188df18f1dd1273fe8f096ad478c48ada20f0bae844d", "ref_doc_id": "d6aeadcd-0426-4253-9f8f-6e09017ccf8b"}, "e2b57a5e-2b4a-4231-89a2-e922de189bd0": {"doc_hash": "aae602652206090edd4d4efd6d0aaeffc74a18cc3a98fea163be2b1bfe5bd7c6", "ref_doc_id": "d6aeadcd-0426-4253-9f8f-6e09017ccf8b"}, "a68eeb05-c4bc-4fe1-9b55-b0ced343280d": {"doc_hash": "f247a43bcae254c1d1a327b1e4be5abb4b0b86bceedf78ddbb7e063925819ad0", "ref_doc_id": "d6aeadcd-0426-4253-9f8f-6e09017ccf8b"}, "24d1a104-133c-4206-9866-a07a7e1c8e01": {"doc_hash": "0665d5aaea513da74be3efe7bd21406a0e472468d9065d883c02ef7dea9e9726", "ref_doc_id": "d6aeadcd-0426-4253-9f8f-6e09017ccf8b"}, "77c0d9b4-2834-4c24-bee6-fb56399af23c": {"doc_hash": "731fa5cd0fea8b2f487e87626145733a3a02eb13393d3109b0a65df36d77e63f", "ref_doc_id": "d6aeadcd-0426-4253-9f8f-6e09017ccf8b"}, "8a4e76af-c1b4-42d9-a5f7-8224e0843116": {"doc_hash": "da34d10fc4aae6ae199b61d7c6f0f287d9b62dc419777bc46d5c8cb7017977ff", "ref_doc_id": "d6aeadcd-0426-4253-9f8f-6e09017ccf8b"}, "1c718caf-4dd3-42f1-a46d-574012f9c5d7": {"doc_hash": "6a4e1e15629f41b11ef9e8395a14610d73d450e069030873d8e05a95e7d080e7", "ref_doc_id": "de4fd050-914c-477a-87fe-16bfebd34550"}, "f1cbd5f2-d8b6-4156-9e7a-7f833ea8212d": {"doc_hash": "0e17ce0587dc520ecde1b17a24aa1b628f65c4c35da00e3b1cdc2d5c94579819", "ref_doc_id": "7926f71f-4c4b-4e34-9079-672324d252a9"}, "3f679a57-91f0-4365-a125-142d0b65f90b": {"doc_hash": "c2e98e6fa111f639b790a11b4f58b64924aef4793dc4a5729be6de7c26692e4a", "ref_doc_id": "7926f71f-4c4b-4e34-9079-672324d252a9"}, "e823a811-c96b-405e-b264-e46fda9204f8": {"doc_hash": "9bdb4c386ee7dffa48f38adc6a2d91fa640823db6f72a1f60012cbba20deade4", "ref_doc_id": "7926f71f-4c4b-4e34-9079-672324d252a9"}, "989ff429-2ab2-485c-805c-9ca537c7cc63": {"doc_hash": "0de9a8f8219f78ae68ebed7916c1783fc9b061b9474959b2ea5e0a319f33774c", "ref_doc_id": "7926f71f-4c4b-4e34-9079-672324d252a9"}, "400832fc-628f-4c7c-ab89-6540ff9aa9ff": {"doc_hash": "ce8318810bf067bb6a686462a042073d2443c39bfa24a7c2b14cf86e86f46c71", "ref_doc_id": "7926f71f-4c4b-4e34-9079-672324d252a9"}, "85c2b027-7de7-40bf-a62a-ad643a2f7b0f": {"doc_hash": "4eecb1989f41cff1d3a035b5a6244a3fc7dcfaf00ba9d0421270e9360fc7ad6e", "ref_doc_id": "0aecbee5-a75d-4421-858a-c7ee37042a3b"}}, "docstore/ref_doc_info": {"8fe7f791-0df3-4c83-a0b1-727c09f05980": {"node_ids": ["73683619-d9c7-41ca-acec-a202e5bce026", "2fcf50f3-9efc-45a6-bc93-d0a7686ae7a7"], "metadata": {"page_label": "1", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "b6b83cba-db9d-4f51-9a57-98553a8f585b": {"node_ids": ["77bc0db0-4d6f-46ef-9d9e-e43c6bc66571", "15c9da0b-798d-427f-96ac-e9cd5f8563ab", "bd8ec96d-9a79-4e29-b0d3-9b1776ab5ae2", "6a1e9e6a-8977-4dc2-869b-27bd69d8434e", "5219db5c-46c9-4aa0-a935-56ad72f09a63", "a55eb263-fe23-4a7d-8720-8044cccc30f3", "1720c4b1-2a33-4440-aae8-f872d7e4c222", "ff07bd8b-9610-4a23-8e21-e6c05db444ce", "7f2a9c01-092f-4e22-bce6-720b6a33e8b1"], "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "7a699566-66b7-4775-b7f7-325184d696aa": {"node_ids": ["a0045dbd-4d80-4535-9428-b97c677e9728", "cafcd2d9-05d6-4819-9f2b-8629495848e8", "91c514b5-e421-4bf1-bd0e-644abe3641b2"], "metadata": {"page_label": "3", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "2e780022-c55a-4597-9dfd-33e57de3178d": {"node_ids": ["8de2c143-587f-4705-b9ae-746b924ba119", "2309b812-4a50-4ea5-a228-0904d1b2e1d5", "b9ecf477-266b-443a-a100-ab52983c1252"], "metadata": {"page_label": "4", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "542d7e53-3d40-49e8-b27f-3ce12cc7fe27": {"node_ids": ["342a0277-e10b-4a3a-9263-3b70fa4a7aca", "ffb50b71-522d-41b1-a83c-c2ae77f58958", "c94f1182-6f0f-47b1-b1ef-2c2845a7e271"], "metadata": {"page_label": "5", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "6151682a-a138-4fd1-9963-86e2b323bc47": {"node_ids": ["a48cba17-a9e9-430e-9630-1bb97bd493b5", "a084e2a9-0c44-4f8d-885f-a8511c724ca4", "1fd69a51-1fba-448b-8ac5-f89408c6b676"], "metadata": {"page_label": "6", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "0e2000de-b1b7-49db-8551-f13a88bd5b76": {"node_ids": ["9c77e0b2-d061-4d17-bf79-8e5d2deb29f8", "d02302cc-760b-4c0d-9dd3-51850df92501", "85a46433-678d-41e0-9b84-7f1b40f22ad1"], "metadata": {"page_label": "7", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "f022c336-2b9d-4255-9611-95200c3ccda7": {"node_ids": ["ed9a2722-215e-4e55-a6b8-dbffc81c6cc3", "92849269-72d7-42b8-9178-70b3e34be72d", "819e2501-e5f0-48db-9692-060a8a7568be"], "metadata": {"page_label": "8", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "09a58be2-948e-4afd-9266-81ab087339de": {"node_ids": ["30427af7-31a2-4d3e-9821-75fce86ba9f7", "7922a6c8-3912-4df7-b23a-2ea38d3f3aea", "0c8d9dea-1adc-4679-8bfa-ad5e4f04f63d"], "metadata": {"page_label": "9", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "cc66b82d-aca3-4dc3-8a08-7e38a0169aa7": {"node_ids": ["b963f9e4-eceb-4f04-918c-38c21df6d0b1", "52473bb4-9388-498e-b70f-53545960402f", "c64fe3c9-a89d-4d19-99da-dc52157e6963"], "metadata": {"page_label": "10", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "483f9e6c-4e8d-4a8b-9a6d-b91fa78fd616": {"node_ids": ["94a68eee-16b6-4f33-ae36-3b457afb3bb5", "12f767a0-a741-414e-967e-e9c483b15877", "c11bd8af-a63e-4251-aaa7-d48b91a1d92d"], "metadata": {"page_label": "11", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "471671c9-c130-4201-97a2-6c830550b8db": {"node_ids": ["cb5b3aea-572b-4a0b-be39-ce380715c141", "4dc1c486-bc29-41e6-89ca-05fbcb387af1", "08f36c52-7469-4903-ad2b-e19180dd5385"], "metadata": {"page_label": "12", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "e76590cc-06e7-4aea-8441-e2dd363f22d9": {"node_ids": ["f5a1dede-4fc9-4fa5-953d-6d13a1221268"], "metadata": {"page_label": "13", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "a4cb70d6-c274-4d05-8f7c-3562a7b5a3c2": {"node_ids": ["45d61ee0-cedd-4764-bb2d-59643d40227d", "b524774d-03e3-4331-afc4-b05f840260ff", "1cc14555-df9d-463b-8d77-b9626057d74b", "64f25ea0-264c-477c-aef2-3185c548ffb1", "d0e0545a-0ac5-4755-934d-65dfd5170fd5"], "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "5e12086a-da1a-45f7-a4a2-61acfe5763db": {"node_ids": ["f1b7ead6-334e-4bab-8b28-92da4c16d369", "1ce39cc1-62ba-493e-9956-13eaf90e8797", "e24752aa-02a9-4e8c-919b-8ff8784d527e", "fbb1a6d5-e54c-44ee-8c63-a39c78ea30f1", "2ebdecee-617c-465b-a07a-deb11920bd47", "125f3631-432f-43ad-b65a-49444f75201b"], "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "c5d884e3-745e-43d0-a430-d6f985ee2b15": {"node_ids": ["6076b0eb-bb62-4e5f-bc01-16e2233317e9", "f32e4167-2373-4926-b914-fe6f93b3fbed"], "metadata": {"page_label": "16", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "9e817b24-c86c-41c1-bc96-15c2bd748942": {"node_ids": ["b779898c-20aa-4835-beed-ab538e96b831", "cc507c24-d73e-4f0e-a68c-ffe149806176", "65e6d4fa-b0e7-4caf-b68a-040933c97eeb"], "metadata": {"page_label": "17", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "b47afeb6-caa7-410b-ab54-6b7edc2a5571": {"node_ids": ["30146c2c-f4cd-46f7-9b67-ae6f6b22eee2", "8a0e7340-8945-4915-b980-3e6bf39a930b", "892e5084-a8b3-47e2-bdb6-99918e767acc"], "metadata": {"page_label": "18", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "6540d239-e9b8-4be6-9477-7c22b6f40b05": {"node_ids": ["a2467112-7946-45a5-aa5e-248b162cc546"], "metadata": {"page_label": "19", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "2e9928d3-6a8a-4719-bdcf-e1f85560b13e": {"node_ids": ["ee6372f7-f846-4363-b88a-b87a2aaae5c0", "a5e5c86e-07d4-4867-baf1-de1868428f42"], "metadata": {"page_label": "20", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "7fd83d10-afc0-4ab9-8da9-a2babf1b04fa": {"node_ids": ["de4269f2-1ff5-46f8-b0e8-c293802a3d5f", "a7e4a0db-c132-4cf1-a17b-1b49a202d6b0"], "metadata": {"page_label": "21", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "df4c8a79-9c9e-414e-83f1-2c0133ca69d7": {"node_ids": ["70ca7e62-0f41-4481-b881-29dbf8fa668a", "1efaf9f5-0d96-438b-bb38-554f9275dab4", "46f985eb-eda5-46c7-8e45-444ca4bc37b6"], "metadata": {"page_label": "22", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "75de8181-2894-433e-97ab-01c1f0071eca": {"node_ids": ["6e8210cf-2387-4d76-a7b5-988a1a6cbb6d", "c9efea2b-46de-49fe-9bd0-1815101b30be"], "metadata": {"page_label": "23", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "8925ba19-3aed-42ac-9f53-68b59387da55": {"node_ids": ["289320ac-592b-474b-8e7f-6cec29db816d", "32fdd2c6-5d40-4a8c-8683-613efa20df90"], "metadata": {"page_label": "24", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "e72937d4-f3d9-4e98-b516-d8e7d3952d0b": {"node_ids": ["4088ee3f-49e7-4cd8-8ba2-35c961116bb1", "2e511fbc-5a81-4644-b301-d501d7a53f17"], "metadata": {"page_label": "25", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "e0cffa38-b905-4946-bb97-d96a8f789334": {"node_ids": ["641e96e8-b42f-40b1-9b65-77d69356bad8", "a3cad464-7862-4346-b54f-b50c14c485e9"], "metadata": {"page_label": "26", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "b8c82527-1f8c-4b4f-a4cf-fbb04c346a5c": {"node_ids": ["49d78103-c38b-4c64-a8a1-0c4e7495f6b8"], "metadata": {"page_label": "27", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "d51fe16e-98e8-4162-be95-60fa61ff96db": {"node_ids": ["e26484bf-e9a7-4849-821b-a9c8ff7cadb2", "c01a7198-fa33-49f0-8f2a-5e4e68c1ae68"], "metadata": {"page_label": "28", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "3268defb-8a93-41fd-84a8-5fb4b8633000": {"node_ids": ["9026eceb-3364-40ac-bbca-3e7eb22ea7ad", "7239e08d-d0a6-4ee3-ac36-bc79310b3029", "bc95d600-23b2-4404-810b-64d7740631ab"], "metadata": {"page_label": "29", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "34634f5b-ab87-4b7b-ac38-2b858e403710": {"node_ids": ["f8c6f3b2-e9ad-4527-8a36-6bdb5bfac9b1", "651819ab-c8c4-450f-ab10-75abb1c9112d"], "metadata": {"page_label": "30", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "d55b8927-c92e-45c5-a5c8-cf27bf74860b": {"node_ids": ["cf22cb2c-eb68-4fe8-ad0c-6f38cb315979", "384c3914-52cf-45e4-afbe-aaa39e9e27a8"], "metadata": {"page_label": "31", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "0177ad22-60e4-4176-95bd-1f4d92675c7a": {"node_ids": ["9bf31843-d616-494f-b39f-7d0678d4ddd1", "8b991390-00f7-40c4-ba5b-fdc07433044e"], "metadata": {"page_label": "32", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "5dc6c946-8754-4f1c-a10e-1b24399902a0": {"node_ids": ["30b419c3-c04b-47bc-8250-bceaf6b41f5f"], "metadata": {"page_label": "33", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "16fd2e34-8661-4563-83ba-97061daab5f6": {"node_ids": ["72c0fc9b-2702-45a5-bca5-7d23c2a001c7", "0b836885-ede9-49d0-9a7f-e6812763e8a6"], "metadata": {"page_label": "1", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "91bbd494-66ea-419c-a8c5-0e2e99927248": {"node_ids": ["fe966015-65d2-4a9c-93a8-ce6a55c4014e", "fb9098f4-7962-483e-a2e5-15d2734619fa"], "metadata": {"page_label": "2", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "95ed5c99-8f17-452b-bb14-0a0a89cd33af": {"node_ids": ["d5a0bc12-e0f9-42d5-b45e-bba001ffa39a"], "metadata": {"page_label": "3", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "ebd047c6-66b7-4795-aed0-6b02b1413e60": {"node_ids": ["3c8b5395-dc88-472a-8191-fba27a69cc96", "d5eee747-b453-40e5-9801-b54f469a6d94"], "metadata": {"page_label": "4", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "ad3ac395-2e77-48ec-b51f-efba9b6658e7": {"node_ids": ["56d067d4-ae0d-4b0f-a9d5-4f60096b3b45", "4c7fa795-7e64-449e-89a1-fb95fa8bd3cd"], "metadata": {"page_label": "5", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "af2f2297-262b-4c2a-a05e-dd5d1f356408": {"node_ids": ["136f7dd4-eb94-4700-a805-d14e2826b8b3", "4c2d90cb-b27b-4870-82a9-d73c62260e1d"], "metadata": {"page_label": "6", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "5198786b-0ad4-4039-a302-42457a51128a": {"node_ids": ["8a382839-bead-4c3a-92dc-720364648d83", "7ff918b0-f6a4-420f-8ac3-d9e771decf91"], "metadata": {"page_label": "7", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "f50d6d98-bda7-475d-8ac4-b72bbb409b90": {"node_ids": ["e9c8e0ee-8a64-466b-9130-7d10d9747c88", "73a3a64e-46ef-4a18-81b6-bd8febbfee64", "5c13cf12-d797-40e9-9e12-5cb01332da8b"], "metadata": {"page_label": "8", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "2f76df3b-fa06-411e-a400-291d0701813b": {"node_ids": ["7ec93aa3-c377-497f-a7a6-7794e78892fe", "c5bb93ee-1434-4942-8594-a80297295025"], "metadata": {"page_label": "9", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "c02e1ec3-1955-4c54-b611-61d8fdb8e6bc": {"node_ids": ["3932b202-d880-4662-a53e-01526a108b31", "d61012a7-f300-4b23-80c6-1520c01e440d"], "metadata": {"page_label": "10", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "0b7f06ce-bb26-4fa5-9531-9fade38e867c": {"node_ids": ["1a99e637-1d66-4f76-be63-0d5f0076db53", "469a47cc-bfe4-4664-9e96-47515fb59d92", "fe9505f8-f97a-4a55-80f9-218eb747c05f"], "metadata": {"page_label": "11", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "ab2ee5d2-a4f9-45f7-ba9f-ef20d4af81ca": {"node_ids": ["ca6c63c5-a7ba-417b-990f-4e8ca4cab9a4", "4d3efd38-624c-418a-a875-3edc6ece41c3", "3adb667e-d9a1-4a66-acd8-03e6888a536c"], "metadata": {"page_label": "12", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "7c1d8cb9-5d2c-453c-aef1-63ae2b74be30": {"node_ids": ["eebe6332-9e51-40a2-9c4b-5e8becf069bf"], "metadata": {"page_label": "13", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "e54825e2-f79d-4230-9479-782e0e6d891b": {"node_ids": ["904d540f-6917-4864-b1cc-2d08e99f7b7a"], "metadata": {"page_label": "14", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "a30ac3c8-8693-4b03-bacc-0c9fd5839ef1": {"node_ids": ["7442f82b-cb88-412b-8aff-6261d773d4b1"], "metadata": {"page_label": "15", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}}, "0d86b379-a6e5-4a09-b263-48cc07e41376": {"node_ids": ["ec3d26ba-59eb-44db-9645-e2a3d6e429b5", "a0a2ff6a-610b-4262-94cf-f2b683fe305a"], "metadata": {"page_label": "1", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "d9cd65c3-9bf8-4711-81fd-42e5a3549eb0": {"node_ids": ["59f99bc7-2b82-4a92-8818-a4ec813e631e", "ecda83d1-b0d6-472c-aa67-22b4b6216cba"], "metadata": {"page_label": "2", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "1e884046-6e61-4d08-a9ac-dfd68dc5aa65": {"node_ids": ["917304a0-405e-422a-8adb-b5fc2ca91a4d", "ed4c610b-5d77-4c05-baec-75bd54659204"], "metadata": {"page_label": "3", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "1d34b3cb-958e-44cf-9a8e-dbdb09012c87": {"node_ids": ["797fae54-f6e0-4047-9572-63fe0a9e6d01", "b9198871-fcac-409e-8dda-a566b1d157ac"], "metadata": {"page_label": "4", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "35b475bd-e3f8-4fd9-bc50-96c6617246ef": {"node_ids": ["5b76221d-f6e0-4b4b-9d12-5fdce7620aa1", "529b2a6e-43e1-42c8-9be8-df971716cbc3", "af4fd4c8-c17c-45d9-9b9d-73a30271553a"], "metadata": {"page_label": "5", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "3e384ff7-5141-423a-bc92-69856bab59e5": {"node_ids": ["66a98e4a-95d5-46e4-af7f-f234f5fcac99", "cf687506-631b-4419-9f9c-ef3f37a164c8"], "metadata": {"page_label": "6", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "5d4a0209-60cc-472a-82c6-62d405bb3988": {"node_ids": ["586221dd-d100-4885-9d2a-5ca9b79b131b", "8d14f7d6-1c9e-4a57-ad87-6f38afdbb2ed", "bd63ddcd-3581-43d2-b28c-ab98caf2ce09"], "metadata": {"page_label": "1", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "1a4e0c2a-841d-4c8e-8900-45c38addbff8": {"node_ids": ["e1d4b10e-2d7c-43cf-82ba-939e6582616c", "d8800e22-7291-4f96-88f5-351af77e1ba5", "774e0607-d24b-44b1-ac79-517ff73dde79"], "metadata": {"page_label": "2", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "8c7c4363-4a3c-40ca-8522-72ade53f0968": {"node_ids": ["e70a1c3e-0601-4cd1-992d-eeded998e664", "87defc6e-a0a2-4b5c-9966-74a1f8154c97", "11443c17-7442-4c57-baea-779e91715b1b"], "metadata": {"page_label": "3", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "707b5854-4e98-4c49-bb14-5c58767834b9": {"node_ids": ["1d8bb0ad-4d0e-45f3-a78f-fe083ad7958d", "2a285257-a731-4a55-adcb-7a900738db4e", "dcf26f26-1430-4911-993e-4c88f18121e6"], "metadata": {"page_label": "4", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "9afffa19-c059-480c-af5a-8e971b57381e": {"node_ids": ["979b24b7-0e7c-4aad-93a3-2b05f03816c5", "fbe1bce6-84ea-4eb7-bc63-dfed44c50517", "1f78ad04-fc98-443e-ac0c-6a42369faef8"], "metadata": {"page_label": "5", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "7ea657a7-0df6-4d32-9c27-b3384842c752": {"node_ids": ["c217c2e2-8542-453f-829a-017a3c8a55e0", "6508351a-2c61-4275-b22d-446bec30cedf", "6d593f51-ba53-41a9-8c4f-df2be1d4cc4a"], "metadata": {"page_label": "6", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "d6aeadcd-0426-4253-9f8f-6e09017ccf8b": {"node_ids": ["ddb7948f-45ef-4220-afe6-3c117218fbd9", "e2b57a5e-2b4a-4231-89a2-e922de189bd0", "a68eeb05-c4bc-4fe1-9b55-b0ced343280d", "24d1a104-133c-4206-9866-a07a7e1c8e01", "77c0d9b4-2834-4c24-bee6-fb56399af23c", "8a4e76af-c1b4-42d9-a5f7-8224e0843116"], "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "de4fd050-914c-477a-87fe-16bfebd34550": {"node_ids": ["1c718caf-4dd3-42f1-a46d-574012f9c5d7"], "metadata": {"page_label": "8", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "7926f71f-4c4b-4e34-9079-672324d252a9": {"node_ids": ["f1cbd5f2-d8b6-4156-9e7a-7f833ea8212d", "3f679a57-91f0-4365-a125-142d0b65f90b", "e823a811-c96b-405e-b264-e46fda9204f8", "989ff429-2ab2-485c-805c-9ca537c7cc63", "400832fc-628f-4c7c-ab89-6540ff9aa9ff"], "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}, "0aecbee5-a75d-4421-858a-c7ee37042a3b": {"node_ids": ["85c2b027-7de7-40bf-a62a-ad643a2f7b0f"], "metadata": {"page_label": "10", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}}}, "docstore/data": {"73683619-d9c7-41ca-acec-a202e5bce026": {"__data__": {"id_": "73683619-d9c7-41ca-acec-a202e5bce026", "embedding": null, "metadata": {"page_label": "1", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe7f791-0df3-4c83-a0b1-727c09f05980", "node_type": "4", "metadata": {"page_label": "1", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5f577ddccbe92bbcb023ab0f6a863306b3d10dd8af7489f725f252bbdc5dbb74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fcf50f3-9efc-45a6-bc93-d0a7686ae7a7", "node_type": "1", "metadata": {}, "hash": "fc2737fb090809ffaa24988b6244fd797a38112ca935132367f072b9512148a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nREAC T: S YNERGIZING REASONING AND ACTING IN\nLANGUAGE MODELS\nShunyu Yao\u2217*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\n1Department of Computer Science, Princeton University\n2Google Research, Brain team\n1{shunyuy,karthikn}@princeton.edu\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\nABSTRACT\nWhile large language models (LLMs) have demonstrated impressive performance\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\nplan generation) have primarily been studied as separate topics. In this paper, we\nexplore the use of LLMs to generate both reasoning traces and task-speci\ufb01c actions\nin an interleaved manner, allowing for greater synergy between the two: reasoning\ntraces help the model induce, track, and update action plans as well as handle\nexceptions, while actions allow it to interface with and gather additional information\nfrom external sources such as knowledge bases or environments. We apply our\napproach, named ReAct, to a diverse set of language and decision making tasks\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\nimproved human interpretability and trustworthiness. Concretely, on question\nanswering (HotpotQA) and fact veri\ufb01cation (Fever), ReAct overcomes prevalent\nissues of hallucination and error propagation in chain-of-thought reasoning by\ninteracting with a simple Wikipedia API, and generating human-like task-solving\ntrajectories that are more interpretable than baselines without reasoning traces.\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\nan absolute success rate of 34% and 10% respectively, while being prompted with\nonly one or two in-context examples.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1927, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2fcf50f3-9efc-45a6-bc93-d0a7686ae7a7": {"__data__": {"id_": "2fcf50f3-9efc-45a6-bc93-d0a7686ae7a7", "embedding": null, "metadata": {"page_label": "1", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe7f791-0df3-4c83-a0b1-727c09f05980", "node_type": "4", "metadata": {"page_label": "1", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5f577ddccbe92bbcb023ab0f6a863306b3d10dd8af7489f725f252bbdc5dbb74", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73683619-d9c7-41ca-acec-a202e5bce026", "node_type": "1", "metadata": {"page_label": "1", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "365df878a6b6914c58d1c60716e98e5b7513c988e71a4680fe2d8d1bf1463248", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 I NTRODUCTION\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\nsider the example of cooking up a dish in the kitchen. Between any two speci\ufb01c actions, we may\nreason in language in order to track progress (\u201cnow that everything is cut, I should heat up the pot of\nwater\u201d), to handle exceptions or adjust the plan according to the situation (\u201cI don\u2019t have salt, so let\nme use soy sauce and pepper instead\u201d), and to realize when external information is needed (\u201chow do\nI prepare dough? Let me search on the Internet\u201d). We may also act (open a cookbook to read the\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (\u201cWhat\ndish can I make right now?\u201d). This tight synergy between \u201cacting\u201d and \u201creasoning\u201d allows humans\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\nunseen circumstances or facing information uncertainties.\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\n\u2217Work during Google internship. Projet page with code: https://react-lm.github.io/.\n1\narXiv:2210.03629v3  [cs.CL]  10 Mar 2023", "mimetype": "text/plain", "start_char_idx": 1928, "end_char_idx": 3601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77bc0db0-4d6f-46ef-9d9e-e43c6bc66571": {"__data__": {"id_": "77bc0db0-4d6f-46ef-9d9e-e43c6bc66571", "embedding": null, "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5c361553038c6174373aa6228acfafa63d348c8ce409af84683ea8097f42fd4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15c9da0b-798d-427f-96ac-e9cd5f8563ab", "node_type": "1", "metadata": {}, "hash": "15b3f7602ba211bb0f0ae696642069f728a5ad8468bbf5de7eee89441968876d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\n$FW\u0003\u0014\u001d\u00037KLQN>)LUVW\u0003,\u0003QHHG\u0003WR\u0003ILQG\u0003D\u0003SHSSHU\u0003VKDNHU\u0011\u0011\u0011PRUH\u0003\nOLNHO\\\u0003WR\u0003DSSHDU\u0003LQ\u0003FDELQHWV\u0003\u000b\u0014\u0010\u0019\f\u000f\u0003FRXQWHUWRSV\u0003\u000b\u0014\u0010\u0016\f\u000f\u0003\u0011\u0011\u0011\u0003@\n$FW\u0003\u0015\u001d\u0003*R\u0003WR\u0003FDELQHW\u0003\u0014\u0003\n2EV\u0003\u0015\u001d\u00032Q\u0003WKH\u0003FDELQHW\u0003\u0014\u000f\u0003\\RX\u0003VHH\u0003D\u0003YDVH\u0003\u0015\u0011\u0003\n\u000b+HUH\u0003WKH\u0003DJHQW\u0003JR\u0003WR\u0003FDELQHW\u0003\u0014\u000f\u0003WKHQ\u0003FDELQHW\u0003\u0015\u000f\u0003\u0016\u000f\u0003WKHQ\u0003FRXQWHUWRS\u0003\u0014\u0003DQG\u0003\u0015\f\n$FW\u0003\u0019\u001d\u0003*R\u0003WR\u0003FRXQWHUWRS\u0003\u0016\u00032EV\u0003\u0019\u001d\u00032Q\u0003WKH\u0003FRXQWHUWRS\u0003\u0016\u000f\u0003\\RX\u0003VHH\u0003D\u0003DSSOH\u0003\u0014\u000f\u0003D\u0003EUHDG\u0003\u0014\u000f\u0003\n\u0011\u0011\u0011\u0011\u0011\u0011\u0003D\u0003SHSSHU\u0003VKDNHU\u0003\u0014\u000f\u0003DQG\u0003D\u0003YDVH\u0003\u0016\u0011\n$FW\u0003\u001a\u001d\u00037DNH\u0003SHSSHU\u0003VKDNHU\u0003\u0014\u0003IURP\u0003FRXQWHUWRS\u0003\u0016\u0003\n2EV\u0003\u001a\u001d\u0003<RX\u0003SLFN\u0003XS\u0003WKH\u0003SHSSHU\u0003VKDNHU\u0003\u0014\u0003IURP\u0003WKH\u0003FRXQWHUWRS\u0003\u0016\u0011\u0003\n$FW\u0003\u001b\u001d\u00037KLQN>1RZ\u0003,\u0003ILQG\u0003D\u0003SHSSHU\u0003VKDNHU\u0003\u0014\u0011\u00031H[W\u000f\u0003,\u0003QHHG\u0003WR\u0003\nSXW\u0003LW\u0003LQ\u0012RQ\u0003GUDZHU\u0003\u0014\u0011@\n$FW\u0003\u001c\u001d\u0003*R\u0003WR\u0003GUDZH", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 646, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15c9da0b-798d-427f-96ac-e9cd5f8563ab": {"__data__": {"id_": "15c9da0b-798d-427f-96ac-e9cd5f8563ab", "embedding": null, "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5c361553038c6174373aa6228acfafa63d348c8ce409af84683ea8097f42fd4d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77bc0db0-4d6f-46ef-9d9e-e43c6bc66571", "node_type": "1", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "3442a12ba5f2d888626b580048d45ef6638680488596f46625b131f716bcb0d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd8ec96d-9a79-4e29-b0d3-9b1776ab5ae2", "node_type": "1", "metadata": {}, "hash": "4adaa2682cc1522485c89cd182952560e7a459e5d1483c2fb0b28778f577a0c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "@\n$FW\u0003\u001c\u001d\u0003*R\u0003WR\u0003GUDZHU\u0003\u0014\u0003\n2EV\u0003\u001c\u001d\u0003'UDZHU\u0003\u0014\u0003LV\u0003FORVHG\u0011\u0003\n$FW\u0003\u0014\u0013\u001d\u00032SHQ\u0003GUDZHU\u0003\u0014\u0003\n2EV\u0003\u0014\u0013\u001d\u0003<RX\u0003RSHQ\u0003'UDZHU\u0003\u0014\u0003\u00aa\n$FW\u0003\u0014\u0014\u001d\u00033XW\u0003SHSSHU\u0003VKDNHU\u0003\u0014\u0003LQ\u0012RQ\u0003GUDZHU\u0003\u0014\u0003\n2EV\u0003\u0014\u0014\u001d\u0003<RX\u0003SXW\u0003SHSSHU\u0003VKDNHU\u0003\u0014\u0003LQ\u0012RQ\u0003WKH\u0003GUDZHU\u0003\u0014\u0011\n\u000b\u0014D\f\u00036WDQGDUG\n$QVZHU\u001d\u0003L3RG\n\u000b\u0014E\f\u0003&R7\u0003\u000b5HDVRQ\u00032QO\\\f\n7KRXJKW\u001d\u0003/HW\nV\u0003WKLQN\u0003VWHS\u0003\nE\\\u0003VWHS\u0011\u0003$SSOH\u00035HPRWH\u0003ZDV\u0003\nRULJLQDOO\\\u0003GHVLJQHG\u0003WR\u0003\nLQWHUDFW\u0003ZLWK\u0003$SSOH\u000379\u0011\u0003\n$SSOH\u000379\u0003FDQ\u0003EH\u0003FRQWUROOHG\u0003\nE\\\u0003L3KRQH\u000f\u0003L3DG\u000f\u0003DQG\u0003L3RG\u0003\n7RXFK\u0011\u00036R\u0003WKH\u0003DQVZHU\u0003LV\u0003\nL3KRQH\u000f\u0003L3DG\u000f\u0003DQG\u0003L3RG\u0003\n7RXFK\u0011\n$QVZHU\u001d\u0003L3KRQH\u000f\u0003L3DG\u000f\u0003L3RG", "mimetype": "text/plain", "start_char_idx": 626, "end_char_idx": 1116, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd8ec96d-9a79-4e29-b0d3-9b1776ab5ae2": {"__data__": {"id_": "bd8ec96d-9a79-4e29-b0d3-9b1776ab5ae2", "embedding": null, "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5c361553038c6174373aa6228acfafa63d348c8ce409af84683ea8097f42fd4d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15c9da0b-798d-427f-96ac-e9cd5f8563ab", "node_type": "1", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "67817cae4653ef24480a75e1dd3de637892f24e7602fea465d0e01da2f61d28f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a1e9e6a-8977-4dc2-869b-27bd69d8434e", "node_type": "1", "metadata": {}, "hash": "42df94b98a22f26667ca75e95a4c0887d2dadcb8d18a4b4f5506e45157b67f92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u0003L3KRQH\u000f\u0003L3DG\u000f\u0003L3RG\u0003\n7RXFK\n\u000b\u0014F\f\u0003$FW\u00102QO\\\u0003\n$FW\u0003\u0014\u001d\u00036HDUFK>$SSOH\u00035HPRWH@\u0003\n2EV\u0003\u0014\u001d\u00037KH\u0003$SSOH\u00035HPRWH\u0003LV\u0003D\u0003\nUHPRWH\u0003FRQWURO\u0003\u00aa\n$FW\u0003\u0015\u001d\u00036HDUFK>)URQW\u00035RZ@\u0003\n2EV\u0003\u0015\u001d\u0003&RXOG\u0003QRW\u0003ILQG\u0003>)URQW\u0003\n5RZ@\u0011\u00036LPLODU\u001d\u0003\u00aa\n$FW\u0003\u0016\u001d\u00036HDUFK>)URQW\u00035RZ\u0003\n\u000bVRIWZDUH\f@\u0003\n2EV\u0003\u0016\u001d\u0003)URQW\u00035RZ\u0003LV\u0003D\u0003\nGLVFRQWLQXHG\u0003PHGLD\u0003FHQWHU\u0003\nVRIWZDUH\u0003\u00aa\n$FW\u0003\u0017\u001d\u0003)LQLVK>\\HV@\n\u000b\u0014G\f\u00035H$FW\u0003\u000b5HDVRQ\u0003\u000e\u0003$FW\f\n7KRXJKW\u0003\u0014\u001d\u0003,", "mimetype": "text/plain", "start_char_idx": 1097, "end_char_idx": 1444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a1e9e6a-8977-4dc2-869b-27bd69d8434e": {"__data__": {"id_": "6a1e9e6a-8977-4dc2-869b-27bd69d8434e", "embedding": null, "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5c361553038c6174373aa6228acfafa63d348c8ce409af84683ea8097f42fd4d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd8ec96d-9a79-4e29-b0d3-9b1776ab5ae2", "node_type": "1", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "351b8b901d54c1121aa81cf41a1c105a2da25daefc97adced55f8872ed67eff4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5219db5c-46c9-4aa0-a935-56ad72f09a63", "node_type": "1", "metadata": {}, "hash": "d9724396a1e934e378bda9e5686624a66dd7ea143e07074e5726ee4aaa9fddd9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u0003\u000e\u0003$FW\f\n7KRXJKW\u0003\u0014\u001d\u0003,\u0003QHHG\u0003WR\u0003VHDUFK\u0003$SSOH\u00035HPRWH\u0003DQG\u0003ILQG\u0003WKH\u0003\nSURJUDP\u0003LW\u0003ZDV\u0003RULJLQDOO\\\u0003GHVLJQHG\u0003WR\u0003LQWHUDFW\u0003ZLWK\u0011\n$FW\u0003\u0014\u001d\u00036HDUFK>$SSOH\u00035HPRWH@\n2EV\u0003\u0014\u001d\u00037KH\u0003$SSOH\u00035HPRWH\u0003LV\u0003D\u0003UHPRWH\u0003FRQWURO\u0003LQWURGXFHG\u0003LQ\u0003\n2FWREHU\u0003\u0015\u0013\u0013\u0018\u0003E\\\u0003$SSOH\u0003\u00aa\u0003RULJLQDOO\\\u0003GHVLJQHG\u0003WR\u0003FRQWURO\u0003WKH\u0003\n)URQW\u00035RZ\u0003PHGLD\u0003FHQWHU\u0003SURJUDP\u0003\u00aa\n7KRXJKW\u0003\u0015\u001d\u0003$SSOH\u00035HPRWH\u0003ZDV\u0003RULJLQDOO\\\u0003GHVLJQHG\u0003WR\u0003FRQWURO\u0003\nWKH\u0003)URQW\u00035RZ\u0003PHGLD\u0003FHQWHU\u0003SURJUDP\u0011\u0003,", "mimetype": "text/plain", "start_char_idx": 1424, "end_char_idx": 1817, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5219db5c-46c9-4aa0-a935-56ad72f09a63": {"__data__": {"id_": "5219db5c-46c9-4aa0-a935-56ad72f09a63", "embedding": null, "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5c361553038c6174373aa6228acfafa63d348c8ce409af84683ea8097f42fd4d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a1e9e6a-8977-4dc2-869b-27bd69d8434e", "node_type": "1", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "65bdecd363de1a3906a8758c5c8124717f37689be6d1418703cab92dbcc60c3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a55eb263-fe23-4a7d-8720-8044cccc30f3", "node_type": "1", "metadata": {}, "hash": "3b46c1a1b31992ec3da8061160bc1162514729859c86ce5c98d9a3c65574a300", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u0003QHHG\u0003WR\u0003VHDUFK\u0003)URQW\u0003\n5RZ\u0003QH[W\u0003DQG\u0003ILQG\u0003ZKDW\u0003RWKHU\u0003GHYLFH\u0003FDQ\u0003FRQWURO\u0003LW\u0011\n$FW\u0003\u0015\u001d\u00036HDUFK>)URQW\u00035RZ@\n2EV\u0003\u0015\u001d\u0003&RXOG\u0003QRW\u0003ILQG\u0003>)URQW\u00035RZ@\u0011\u00036LPLODU\u001d\u0003>\n)URQW\u00035RZ\u0003\n6HDW\u0003WR\u0003(DUWK\n\u000f\u0003\n)URQW\u00035RZ\u00030RWRUVSRUWV\n\u000f\n)URQW\u00035RZ\u0003\n\u000bVRIWZDUH\f\n\u000f\u0003\u00aa@\n7KRXJKW\u0003\u0016\u001d\u0003)URQW\u00035RZ\u0003LV\u0003QRW\u0003IRXQG\u0011\u0003,", "mimetype": "text/plain", "start_char_idx": 1817, "end_char_idx": 2078, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a55eb263-fe23-4a7d-8720-8044cccc30f3": {"__data__": {"id_": "a55eb263-fe23-4a7d-8720-8044cccc30f3", "embedding": null, "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5c361553038c6174373aa6228acfafa63d348c8ce409af84683ea8097f42fd4d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5219db5c-46c9-4aa0-a935-56ad72f09a63", "node_type": "1", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "7c540f430ae6c1fd25dd3499e5a2205a21934765254e88528f24d83fa3500026", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1720c4b1-2a33-4440-aae8-f872d7e4c222", "node_type": "1", "metadata": {}, "hash": "4610b48d105bd309f78cfff8ee30955e4e3cd8412a22a334d9d0b56fbc57c83c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u0003QHHG\u0003WR\u0003VHDUFK\u0003)URQW\u0003\n5RZ\u0003\u000bVRIWZDUH\f\u0003\u0011\n$FW\u0003\u0016\u001d\u00036HDUFK>)URQW\u00035RZ\u0003\u000bVRIWZDUH\f@\n2EV\u0003\u0016\u001d\u0003)URQW\u00035RZ\u0003LV\u0003D\u0003GLVFRQWLQXHG\u0003PHGLD\u0003FHQWHU\u0003VRIWZDUH\u0003\u00aa\n7KRXJKW\u0003\u0017\u001d\u0003)URQW\u00035RZ\u0003\u000bVRIWZDUH\f\u0003LV\u0003FRQWUROOHG\u0003E\\\u0003DQ\u0003$SSOH\u0003\n5HPRWH\u0003RU\u0003WKH\u0003NH\\ERDUG\u0003IXQFWLRQ\u0003NH\\V\u0011\u00036R\u0003WKH\u0003DQVZHU\u0003LV\u0003\nNH\\ERDUG\u0003IXQFWLRQ\u0003NH\\V\u0011\n$FW\u0003\u0017\u001d\u0003)LQLVK>NH\\ERDUG\u0003IXQFWLRQ\u0003NH\\V@\n\u04bc\n4XHVWLRQ\u001d\u0003$VLGH\u0003IURP\u0003WKH\u0003$SSOH\u00035HPRWH\u000f\u0003ZKDW\u0003RWKHU\u0003GHYLFH\u0003\nFDQ\u0003FRQWURO\u0003WKH\u0003SURJUDP\u0003$SSOH\u00035HPRWH\u0003ZDV\u0003RULJLQDOO\\\u0003\nGHVLJQHG\u0003WR\u0003LQWHUDFW\u0003ZLWK\"\n\u000b\u0015D\f\u0003$FW\u00102QO\\\u0003$FW\u0003\u0014\u001d\u0003*R\u0003WR\u0003GUDZHU\u0003\u0014\u0003\n2EV\u0003\u0014\u001d\u00037KH\u0003GUDZHU\u0003\u0014\u0003LV\u0003FORVHG\u0011\u0003\n$FW\u0003\u0015\u001d\u00032SHQ\u0003GUDZHU\u0003\u0014\u0003\n2EV\u0003\u0015\u001d\u0003<RX\u0003RSHQ\u0003WKH\u0003GUDZHU\u0003\u0014\u0011\u00037KH\u0003GUDZHU\u0003\u0014\u0003LV\u0003RSHQ\u0011\u0003,", "mimetype": "text/plain", "start_char_idx": 2078, "end_char_idx": 2674, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1720c4b1-2a33-4440-aae8-f872d7e4c222": {"__data__": {"id_": "1720c4b1-2a33-4440-aae8-f872d7e4c222", "embedding": null, "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5c361553038c6174373aa6228acfafa63d348c8ce409af84683ea8097f42fd4d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a55eb263-fe23-4a7d-8720-8044cccc30f3", "node_type": "1", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "06d14c003a0bb375d35737d0da6565608cd9a204d5d7b7ec422dc0f5e19610df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff07bd8b-9610-4a23-8e21-e6c05db444ce", "node_type": "1", "metadata": {}, "hash": "d9142172aa9140e93daf2aa064e1f909332a6a4e8e14a5437335385ddfb40e54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Q\u0003LW\u000f\u0003\n\\RX\u0003VHH\u0003D\u0003GLVKVSRQJH\u0003\u0015\u000f\u0003DQG\u0003D\u0003VSRRQ\u0003\u0014\u0011\n\u0003\n$FW\u0003\u0016\u001d\u0003*R\u0003WR\u0003VLQNEDVLQ\u0003\u0014\u0003\n2EV\u0003\u0016\u001d\u00032Q\u0003WKH\u0003VLQNEDVLQ\u0003\u0014\u000f\u0003\\RX\u0003VHH\u0003D\u0003GLVKVSRQJH\u0003\u0016\u000f\u0003D\u0003\nVSDWXOD\u0003\u0014\u000f\u0003DQG\u0003D\u0003VSRRQ\u0003\u0015\u0011\u0003\n$FW\u0003\u0017\u001d\u00037DNH\u0003SHSSHUVKDNHU\u0003\u0014\u0003IURP\u0003VLQNEDVLQ\u0003\u0014\u0003\n2EV\u0003\u0017\u001d\u00031RWKLQJ\u0003KDSSHQV\u0011\u0003\n$FW\u0003\u0018\u001d\u00037DNH\u0003SHSSHUVKDNHU\u0003\u0014\u0003IURP\u0003VLQNEDVLQ\u0003\u0014\u0003\n2EV\u0003\u0018\u001d\u00031RWKLQJ\u0003KDSSHQV\u0011\u0003\n\u000b\u0015E\f\u00035H$FW\u0003\u000b5HDVRQ\u0003\u000e\u0003$FW\f\n\u04bc\n<RX\u0003DUH\u0003LQ\u0003WKH\u0003PLGGOH\u0003RI\u0003D\u0003URRP\u0011\u0003/RRNLQJ\u0003TXLFNO\\\u0003DURXQG\u0003\n\\RX\u000f\u0003\\RX\u0003VHH\u0003D\u0003FDELQHW\u0003\u0019\u000f\u0003D\u0003FDELQHW\u0003\u0014\u000f\u0003D\u0003FRIIHH\u0003PDFKLQH\u0003\u0014\u000f\u0003\nD\u0003FRXQWHUWRS\u0003\u0016\u000f\u0003\u0003D\u0003VWRYH\u0003EXUQHU\u0003\u0014\u000f\u0003DQG\u0003D\u0003WRDVWHU\u0003\u0014\u0011\u0003\n<RXU\u0003WDVN\u0003LV\u0003WR\u001d\u00033XW\u0003VRPH\u0003SHSSHU\u0003VKDNHU\u0003RQ\u0003D\u0003GUDZHU\u0011\n\u000b\u0015\f\u0003$OI:RUOG\n\u000b\u0014\f\u0003+RWVSRW\u00034$\nFigure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought ( CoT,\nReason Only), (c) Act-only,", "mimetype": "text/plain", "start_char_idx": 2674, "end_char_idx": 3366, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff07bd8b-9610-4a23-8e21-e6c05db444ce": {"__data__": {"id_": "ff07bd8b-9610-4a23-8e21-e6c05db444ce", "embedding": null, "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5c361553038c6174373aa6228acfafa63d348c8ce409af84683ea8097f42fd4d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1720c4b1-2a33-4440-aae8-f872d7e4c222", "node_type": "1", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "93729f0149ba1a46bd947dfc70f2e080c0c51e765e04714642ebdac0c4e2980b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f2a9c01-092f-4e22-bce6-720b6a33e8b1", "node_type": "1", "metadata": {}, "hash": "4a725c5c6879808cd04f6629f1d9e9a7ac238afc36fc613e41e71ae783af81a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)\nquestion. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar\net al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task\nsolving trajectories generated by the model (Act, Thought) and the environment (Obs).\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,\n2022). However, this \u201cchain-of-thought\u201d reasoning is a static black box, in that the model uses\nits own internal representations to generate thoughts and is not grounded in the external world,\nwhich limits its ability to reason reactively or update its knowledge. This can lead to issues like fact\nhallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,\nrecent work has explored the use of pre-trained language models for planning and acting in interactive\nenvironments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with\na focus on predicting actions via language priors. These approaches usually convert multi-modal\nobservations into text, use a language model to generate domain-speci\ufb01c actions or plans, and then\nuse a controller to choose or execute them. However, they do not employ language models to reason\nabstractly about high-level goals or maintain a working memory to support acting, barring Huang\net al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the\ncurrent state. Beyond such simple embodied tasks to interact with a few blocks, there have not been\nstudies on how reasoning and acting can be combined in a synergistic manner for general task solving,\nand if such a combination can bring systematic bene\ufb01ts compared to reasoning or acting alone.\nIn this work, we present ReAct, a general paradigm to combine reasoning and acting with language\nmodels for solving diverse language reasoning and decision making tasks (Figure 1).", "mimetype": "text/plain", "start_char_idx": 3339, "end_char_idx": 5371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f2a9c01-092f-4e22-bce6-720b6a33e8b1": {"__data__": {"id_": "7f2a9c01-092f-4e22-bce6-720b6a33e8b1", "embedding": null, "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6b83cba-db9d-4f51-9a57-98553a8f585b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5c361553038c6174373aa6228acfafa63d348c8ce409af84683ea8097f42fd4d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff07bd8b-9610-4a23-8e21-e6c05db444ce", "node_type": "1", "metadata": {"page_label": "2", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "f364ca315c15835aa623c880a3097da6ea1da83fc925791b4d5fb54ba6a1590f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ReAct\nprompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an\ninterleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and\nadjust high-level plans for acting (reason to act), while also interact with the external environments\n(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\n2", "mimetype": "text/plain", "start_char_idx": 5372, "end_char_idx": 5759, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0045dbd-4d80-4535-9428-b97c677e9728": {"__data__": {"id_": "a0045dbd-4d80-4535-9428-b97c677e9728", "embedding": null, "metadata": {"page_label": "3", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a699566-66b7-4775-b7f7-325184d696aa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "55a55dfd2ce381188400d69655cd66063ac99f42ce1de93a0e98213b2cf922f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cafcd2d9-05d6-4819-9f2b-8629495848e8", "node_type": "1", "metadata": {}, "hash": "1793c4bd336eb3de621f93765a89fb944312cdafeb536a6f976613b9903bd0f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nWe conduct empirical evaluations ofReAct and state-of-the-art baselines on four diverse benchmarks:\nquestion answering (HotPotQA, Yang et al., 2018), fact veri\ufb01cation (Fever, Thorne et al., 2018),\ntext-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\net al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\nwith, ReAct outperforms vanilla action generation models while being competitive with chain-of-\nthought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct\nand CoT that allows for the use of both internal knowledge and externally obtained information\nduring reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able\nto outperform imitation or reinforcement learning methods trained with 103 \u223c105 task instances,\nwith an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate\nthe importance of sparse, versatile reasoning in decision making by showing consistent advantages\nover controlled baselines with actions only. Besides general applicability and performance boost,\nthe combination of reasoning and acting also contributes to model interpretability, trustworthiness,\nand diagnosability across all domains, as humans can readily distinguish information from model\u2019s\ninternal knowledge versus external environments, as well as inspect reasoning traces to understand\nthe decision basis of model actions.\nTo summarize, our key contributions are the following: (1) we introduce ReAct, a novel prompt-\nbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we\nperform extensive experiments across diverse benchmarks to showcase the advantage ofReAct in a\nfew-shot learning setup over prior approaches that perform either reasoning or action generation in\nisolation; (3) we present systematic ablations and analysis to understand the importance of acting in\nreasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations ofReAct under the\nprompting setup (i.e.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2142, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cafcd2d9-05d6-4819-9f2b-8629495848e8": {"__data__": {"id_": "cafcd2d9-05d6-4819-9f2b-8629495848e8", "embedding": null, "metadata": {"page_label": "3", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a699566-66b7-4775-b7f7-325184d696aa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "55a55dfd2ce381188400d69655cd66063ac99f42ce1de93a0e98213b2cf922f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0045dbd-4d80-4535-9428-b97c677e9728", "node_type": "1", "metadata": {"page_label": "3", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "07d9607db48bdc7724c6f5e44e283bf8ca682fcd3091c863080ea42b33f34dc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91c514b5-e421-4bf1-bd0e-644abe3641b2", "node_type": "1", "metadata": {}, "hash": "ef9a46eb55ec806748d73d047e21bfaed05eaac1ce596b596bd2af29a9878bc1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "limited support of reasoning and acting behaviors), and perform initial \ufb01netuning\nexperiments showing the potential of ReAct to improve with additional training data. Scaling up\nReAct to train and operate on more tasks and combining it with complementary paradigms like\nreinforcement learning could further unlock the potential of large language models.\n2 REAC T: S YNERGIZING REASONING + AC TING\nConsider a general setup of an agent interacting with an environment for task solving. At time\nstep t, an agent receives an observation ot \u2208O from the environment and takes an action at \u2208A\nfollowing some policy \u03c0(at|ct), where ct = (o1,a1,\u00b7\u00b7\u00b7 ,ot\u22121,at\u22121,ot) is the context to the agent.\nLearning a policy is challenging when the mapping ct \u21a6\u2192at is highly implicit and requires extensive\ncomputation. For example, the agent shown in Figure 1(1c) is unable to generate the correct \ufb01nal\naction (Act 4) to \ufb01nish the QA task as it requires complex reasoning over the trajectory context\n(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the\ncontext that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\nThe idea of ReAct is simple: we augment the agent\u2019s action space to \u02c6A= A\u222aL, where Lis the\nspace of language. An action \u02c6at \u2208L in the language space, which we will refer to as a thought or a\nreasoning trace, does not affect the external environment, thus leading to no observation feedback.\nInstead, a thought \u02c6at aims to compose useful information by reasoning over the current context ct,\nand update the context ct+1 = (ct,\u02c6at) to support future reasoning or acting. As shown in Figure 1,\nthere could be various types of useful thoughts, e.g.", "mimetype": "text/plain", "start_char_idx": 2143, "end_char_idx": 3868, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "91c514b5-e421-4bf1-bd0e-644abe3641b2": {"__data__": {"id_": "91c514b5-e421-4bf1-bd0e-644abe3641b2", "embedding": null, "metadata": {"page_label": "3", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a699566-66b7-4775-b7f7-325184d696aa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "55a55dfd2ce381188400d69655cd66063ac99f42ce1de93a0e98213b2cf922f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cafcd2d9-05d6-4819-9f2b-8629495848e8", "node_type": "1", "metadata": {"page_label": "3", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "56e7c4c99e0f1f1c8970a176e5b248c2b2f334843e35ccd9732b5c0dfd00cd1a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As shown in Figure 1,\nthere could be various types of useful thoughts, e.g. decomposing task goals and create action plans\n(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),\nextracting important parts from observations (1d, Thought2, 4), track progress and transit action plans\n(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.\nHowever, as the language space Lis unlimited, learning in this augmented action space is dif\ufb01cult\nand requires strong language priors. In this paper, we mainly focus on the setup where a frozen\nlarge language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context\nexamples to generate both domain-speci\ufb01c actions and free-form language thoughts for task solving\n(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and\nenvironment observations to solve a task instance (see Appendix C). For the tasks where reasoning is\nof primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the\ntask-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision\nmaking tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to\n1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B.\n3", "mimetype": "text/plain", "start_char_idx": 3793, "end_char_idx": 5188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8de2c143-587f-4705-b9ae-746b924ba119": {"__data__": {"id_": "8de2c143-587f-4705-b9ae-746b924ba119", "embedding": null, "metadata": {"page_label": "4", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e780022-c55a-4597-9dfd-33e57de3178d", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "7991ad20c32e208c4592306517fcc8f0df92a06fee7d047963d13706960e4e61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2309b812-4a50-4ea5-a228-0904d1b2e1d5", "node_type": "1", "metadata": {}, "hash": "438c9d694b8254c40fb4bcdb7d51946fb0cb34aa2a6417d3c9a414e0b6224e77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nappear sparsely in the most relevant positions of a trajectory, so we let the language model decide the\nasynchronous occurrence of thoughts and actions for itself.\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct\nenjoys several unique features: A) Intuitive and easy to design : Designing ReAct prompts is\nstraightforward as human annotators just type down their thoughts in language on top of their actions\ntaken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail\nprompt design for each task in Sections 3 and 4. B) General and \ufb02exible: Due to the \ufb02exible thought\nspace and thought-action occurrence format, ReAct works for diverse tasks with distinct action\nspaces and reasoning needs, including but not limited to QA, fact veri\ufb01cation, text game, and web\nnavigation. C) Performant and robust: ReAct shows strong generalization to new task instances\nwhile learning solely from one to six in-context examples, consistently outperforming baselines with\nonly reasoning or acting across different domains. We also show in Section 3 additional bene\ufb01ts\nwhen \ufb01netuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\nD) Human aligned and controllable: ReAct promises an interpretable sequential decision making\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\nFigure 5 in Section 4.\n3 K NOWLEDGE -INTENSIVE REASONING TASKS\nWe begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact\nveri\ufb01cation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to\nretrieve information to support reasoning, while also use reasoning to target what to retrieve next,\ndemonstrating a synergy of reasoning and acting.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1974, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2309b812-4a50-4ea5-a228-0904d1b2e1d5": {"__data__": {"id_": "2309b812-4a50-4ea5-a228-0904d1b2e1d5", "embedding": null, "metadata": {"page_label": "4", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e780022-c55a-4597-9dfd-33e57de3178d", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "7991ad20c32e208c4592306517fcc8f0df92a06fee7d047963d13706960e4e61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8de2c143-587f-4705-b9ae-746b924ba119", "node_type": "1", "metadata": {"page_label": "4", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "840e527b98252b3fcb9f324ff0f278046af44d6e5e0c842b4d0ec1f148114c45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9ecf477-266b-443a-a100-ab52983c1252", "node_type": "1", "metadata": {}, "hash": "81c079b4a1d7ba8d3b5c4ddd85c4815191410942a3420feac5a8c920777c378f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.1 S ETUP\nDomains We consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-\nPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning\nover two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veri\ufb01cation\nbenchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\non if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only\nsetup for both tasks, where models only receive the question/claim as input without access to support\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\nan external environment to support reasoning.\nAction Space We design a simple Wikipedia web API with three types of actions to support\ninteractive information retrieval: (1) search[entity], which returns the \ufb01rst 5 sentences from\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\nWikipedia search engine, (2) lookup[string], which would return the next sentence in the page\ncontaining string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which\nwould \ufb01nish the current task with answer. We note that this action space mostly can only retrieve a\nsmall part of a passage based on exact passage name, which is signi\ufb01cantly weaker than state-of-the-\nart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\nand force models to retrieve via explicit reasoning in language.\n3.2 M ETHODS\nReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2 from the training\nset and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\nthought), where free-form thoughts are used for various purposes.", "mimetype": "text/plain", "start_char_idx": 1975, "end_char_idx": 3907, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9ecf477-266b-443a-a100-ab52983c1252": {"__data__": {"id_": "b9ecf477-266b-443a-a100-ab52983c1252", "embedding": null, "metadata": {"page_label": "4", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e780022-c55a-4597-9dfd-33e57de3178d", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "7991ad20c32e208c4592306517fcc8f0df92a06fee7d047963d13706960e4e61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2309b812-4a50-4ea5-a228-0904d1b2e1d5", "node_type": "1", "metadata": {"page_label": "4", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "ccc89067bb8beb264bba595b74abacdcea2f9c314ec1658403b00ca6a6868fab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "dense\nthought), where free-form thoughts are used for various purposes. Speci\ufb01cally, we use a combination\nof thoughts that decompose questions (\u201cI need to search x, \ufb01nd y, then \ufb01nd z\u201d), extract information\nfrom Wikipedia observations (\u201cx was started in 1844\u201d, \u201cThe paragraph does not tell x\u201d), perform\ncommonsense (\u201cx is not y, so z must instead be...\u201d) or arithmetic reasoning (\u201c1844 < 1989\u201d), guide\n2We \ufb01nd more examples do not improve performance.\n4", "mimetype": "text/plain", "start_char_idx": 3836, "end_char_idx": 4288, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "342a0277-e10b-4a3a-9263-3b70fa4a7aca": {"__data__": {"id_": "342a0277-e10b-4a3a-9263-3b70fa4a7aca", "embedding": null, "metadata": {"page_label": "5", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "542d7e53-3d40-49e8-b27f-3ce12cc7fe27", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5537956126f88d729d6567e9778accfc135a4ec289e5e6ebf62b75117ebc1da8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffb50b71-522d-41b1-a83c-c2ae77f58958", "node_type": "1", "metadata": {}, "hash": "b842155b1e37c6404a40f191ecb726a489e02640ab01f21c22d2c742b08c9522", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nPrompt Methoda HotpotQA Fever\n(EM) (Acc)\nStandard 28.7 57.1\nCoT(Wei et al., 2022) 29.4 56.3\nCoT-SC(Wang et al., 2022a) 33.4 60.4\nAct 25.7 58.9\nReAct 27.4 60.9\nCoT-SC\u2192ReAct 34.2 64.6\nReAct\u2192CoT-SC 35.1 62.0\nSupervised SoTAb 67.5 89.5\nTable 1: PaLM-540B prompting results on\nHotpotQA and Fever.\naHotpotQA EM is 27.1, 28.9, 33.8 for Standard, CoT,\nCoT-SC in Wang et al. (2022b).\nb(Zhu et al., 2021; Lewis et al., 2020)\n0 5 10 15 20\n#CoT-SC trials\n26\n28\n30\n32\n34HotpotQA EM\n0 5 10 15 20\n#CoT-SC trials\n47.5\n50.0\n52.5\n55.0\n57.5\n60.0\n62.5\n65.0Fever Acc\nMethod\nCoT-SC -> ReAct\nReAct -> CoT-SC\nCoT-SC\nReAct\nCoT\nFigure 2: PaLM-540B prompting results with respect to\nnumber of CoT-SC samples used.\nsearch reformulation (\u201cmaybe I can search/look up x instead\u201d), and synthesize the \ufb01nal answer (\u201c...so\nthe answer is x\u201d). See Appendix C for more details.\nBaselines We systematically ablateReAct trajectories to build prompts for multiple baselines (with\nformats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts,\nactions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,\n2022), which removes actions and observations and serve as a reasoning-only baseline.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ffb50b71-522d-41b1-a83c-c2ae77f58958": {"__data__": {"id_": "ffb50b71-522d-41b1-a83c-c2ae77f58958", "embedding": null, "metadata": {"page_label": "5", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "542d7e53-3d40-49e8-b27f-3ce12cc7fe27", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5537956126f88d729d6567e9778accfc135a4ec289e5e6ebf62b75117ebc1da8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "342a0277-e10b-4a3a-9263-3b70fa4a7aca", "node_type": "1", "metadata": {"page_label": "5", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "223b4bfa9c94567c8e5f804e34084fc1ee620b39445721198b20022ca3715d6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c94f1182-6f0f-47b1-b1ef-2c2845a7e271", "node_type": "1", "metadata": {}, "hash": "9cc7baa5360d2756c29b6820daebd7975506bdf1aff61d272767ef9a71ac120d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also\nbuild a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories\nwith decoding temperature 0.7 during inference and adopting the majority answer, which is found to\nconsistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts\nin ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the\nInternet to answer questions, though it operates on a different task and action space, and uses imitation\nand reinforcement learning instead of prompting.\nCombining Internal and External Knowledge As will be detail in Section 3.3, we observe that\nthe problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\nis more accurate in formulating reasoning structure but can easily suffer from hallucinated facts\nor thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide\nwhen to switch to the other method based on the following heuristics: A) ReAct \u2192CoT-SC: when\nReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for\nHotpotQA and FEVER respectively as we \ufb01nd more steps will not improve ReAct performance3.\nB) CoT-SC \u2192ReAct: when the majority answer among nCoT-SC samples occurs less than n/2\ntimes (i.e. internal knowledge might not support the task con\ufb01dently), back off to ReAct.\nFinetuning Due to the challenge of manually annotating reasoning traces and actions at scale,\nwe consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories\nwith correct answers generated by ReAct (also for other baselines) to \ufb01netune smaller language\nmodels (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on\ninput questions/claims. More details are in Appendix B.1.", "mimetype": "text/plain", "start_char_idx": 1258, "end_char_idx": 3074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c94f1182-6f0f-47b1-b1ef-2c2845a7e271": {"__data__": {"id_": "c94f1182-6f0f-47b1-b1ef-2c2845a7e271", "embedding": null, "metadata": {"page_label": "5", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "542d7e53-3d40-49e8-b27f-3ce12cc7fe27", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5537956126f88d729d6567e9778accfc135a4ec289e5e6ebf62b75117ebc1da8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffb50b71-522d-41b1-a83c-c2ae77f58958", "node_type": "1", "metadata": {"page_label": "5", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "e5111d19a2ca89f6234dc84aff862112fcb16e3d60be250272cde6620b0c317e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More details are in Appendix B.1.\n3.3 R ESULTS AND OBSERVATIONS\nReAct outperforms Act consistently Table 1 shows HotpotQA and Fever results using PaLM-\n540B as the base model with different prompting methods. We note that ReAct is better than Act\non both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the\n\ufb01nal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also con\ufb01rm the bene\ufb01t of reasoning\ntraces for more informed acting.\n3Of all trajectories with correct \ufb01nal answers, those with 7 steps on HotpotQA and 5 steps on FEVER only\ntake up 0.84% and 1.33% respectively.\n5", "mimetype": "text/plain", "start_char_idx": 3041, "end_char_idx": 3666, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a48cba17-a9e9-430e-9630-1bb97bd493b5": {"__data__": {"id_": "a48cba17-a9e9-430e-9630-1bb97bd493b5", "embedding": null, "metadata": {"page_label": "6", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6151682a-a138-4fd1-9963-86e2b323bc47", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "7d42dfb4aab91baa80037ce082dd0520adf308f34b7c1c2da64dc3f9a18e2413", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a084e2a9-0c44-4f8d-885f-a8511c724ca4", "node_type": "1", "metadata": {}, "hash": "6f405f79b8162e49027f5a82c7fb56a014eae000ee6965159f34fa3ae8e0697d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nType De\ufb01nition ReAct CoT\nSuccess True positive Correct reasoning trace and facts 94% 86%\nFalse positive Hallucinated reasoning trace or facts 6% 14%\nFailure\nReasoning error Wrong reasoning trace (including failing to recover from repetitive steps) 47% 16%\nSearch result error Search return empty or does not contain useful information 23% -\nHallucination Hallucinated reasoning trace or facts 0% 56%\nLabel ambiguity Right prediction but did not match the label precisely 29% 28%\nTable 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their\npercentages in randomly selected examples studied by human.\nReAct vs. CoT On the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly\nlags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only\ndiffer by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge\nis vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we\nrandomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct\nand CoT respectively (thus 200 examples in total), and manually labeled their success and failure\nmodes in Table 2. Some key observations are as follows:\nA) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than\nReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the\nproblem solving trajectory of ReActis more grounded, fact-driven, and trustworthy, thanks to the\naccess of an external knowledge base.\nB) While interleaving reasoning, action and observation steps improves ReAct\u2019s grounded-\nness and trustworthiness, such a structural constraint also reduces its \ufb02exibility in formulating\nreasoning steps, leading to more reasoning error rate than CoT.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1880, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a084e2a9-0c44-4f8d-885f-a8511c724ca4": {"__data__": {"id_": "a084e2a9-0c44-4f8d-885f-a8511c724ca4", "embedding": null, "metadata": {"page_label": "6", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6151682a-a138-4fd1-9963-86e2b323bc47", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "7d42dfb4aab91baa80037ce082dd0520adf308f34b7c1c2da64dc3f9a18e2413", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a48cba17-a9e9-430e-9630-1bb97bd493b5", "node_type": "1", "metadata": {"page_label": "6", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "61db41c1014734dbc809af7d19c84973a42f075369bef022d5bed77fc60c9974", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fd69a51-1fba-448b-8ac5-f89408c6b676", "node_type": "1", "metadata": {}, "hash": "9a26f7e4d50a564572e25c11926d1b0644f2f2ea36851e0f6c51a269f67b1dee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we note that there is one frequent\nerror pattern speci\ufb01c to ReAct, in which the model repetitively generates the previous thoughts and\nactions, and we categorize it as part of \u201creasoning error\u201d as the model fails to reason about what the\nproper next action to take and jump out of the loop4.\nC) For ReAct, successfully retrieving informative knowledge via search is critical. Non-\ninformative search, which counts for 23% of the error cases, derails the model reasoning and gives\nit a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between\nfactuality and \ufb02exibility, which motivates our proposed strategies of combining two methods.\nWe provide examples for each success and failure modes in Appendix E.1. We also \ufb01nd some\nHotpotQA questions may contain outdated answer labels, see Figure 4 for example.\nReAct + CoT-SC perform best for prompting LLMs Also shown in Table 1, the best prompting\nmethod on HotpotQA and Fever are ReAct \u2192CoT-SC and CoT-SC \u2192ReAct respectively.\nFurthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC\nsamples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both\nsigni\ufb01cantly and consistently outperform CoT-SC across different number of samples, reaching\nCoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of\nproperly combining model internal knowledge and external knowledge for reasoning tasks.\nReAct performs best for \ufb01ne-tuning Figure 3 shows the scaling effect of prompting/\ufb01netuning\nfour methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, promptingReAct\nperforms worst among four methods due to the dif\ufb01culty to learn both reasoning and acting from\nin-context examples.", "mimetype": "text/plain", "start_char_idx": 1881, "end_char_idx": 3648, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1fd69a51-1fba-448b-8ac5-f89408c6b676": {"__data__": {"id_": "1fd69a51-1fba-448b-8ac5-f89408c6b676", "embedding": null, "metadata": {"page_label": "6", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6151682a-a138-4fd1-9963-86e2b323bc47", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "7d42dfb4aab91baa80037ce082dd0520adf308f34b7c1c2da64dc3f9a18e2413", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a084e2a9-0c44-4f8d-885f-a8511c724ca4", "node_type": "1", "metadata": {"page_label": "6", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4305c8c8cc900f08aef28d1833616eaf0a3a7ed08362a0bcf60c35f6c4f53b29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, when \ufb01netuned with just 3,000 examples, ReAct becomes the best\nmethod among the four, with PaLM-8B \ufb01netuned ReAct outperforming all PaLM-62B prompting\nmethods, and PaLM-62B \ufb01netuned ReAct outperforming all 540B prompting methods. In contrast,\n\ufb01netuning Standard or CoT is signi\ufb01cantly worse than \ufb01netuning ReAct or Act for both PaLM-\n8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge\nfacts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a\nmore generalizable skill for knowledge reasoning. As all prompting methods are still signi\ufb01cantly\nfar from domain-speci\ufb01c state-of-the-art approaches (Table 1), we believe \ufb01netuning with more\nhuman-written data might be a better way to unleash the power of ReAct.\n4We suspect that this could be due to the sub-optimal greedy decoding procedure, and future work using\nbetter decoding (e.g. beam search) might help address this issue.\n6", "mimetype": "text/plain", "start_char_idx": 3649, "end_char_idx": 4624, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c77e0b2-d061-4d17-bf79-8e5d2deb29f8": {"__data__": {"id_": "9c77e0b2-d061-4d17-bf79-8e5d2deb29f8", "embedding": null, "metadata": {"page_label": "7", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e2000de-b1b7-49db-8551-f13a88bd5b76", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "9044e6e2d2a13c0d0d9229028985f89378683f8825cfd7b2ffbe9683fad90f18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d02302cc-760b-4c0d-9dd3-51850df92501", "node_type": "1", "metadata": {}, "hash": "482ef0307aa4647098dd51e2fc878e6c224c091d5ae4915298f4ba1ca1861b14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\n8b 62b 540b\nsize\n0\n5\n10\n15\n20\n25\n30HotpotQA EM\nlearning = prompt\n8b 62b 540b\nsize\nlearning = finetune\nMethod\nStandard\nCoT\nAct\nReAct\nFigure 3: Scaling results for prompting and \ufb01netuning on HotPotQA withReAct (ours) and baselines.\n4 D ECISION MAKING TASKS\nWe also test ReAct on two language-based interactive decision-making tasks, ALFWorld and\nWebShop, both of which feature complex environments that require agents to act over long horizons\nwith sparse rewards, warranting the need for reasoning to act and explore effectively.\nALFWorld ALFWorld (Shridhar et al., 2020b) (Figure 1(2)) is a synthetic text-based game designed\nto align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of\ntasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by\nnavigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take\npaper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy\nmore than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore\nsystematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into\nALFWorld is the need to determine likely locations for common household items (e.g. desklamps will\nlikely be on desks, shelfs, or dressers), making this environment a good \ufb01t for LLMs to exploit their\npretrained commonsense knowledge.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d02302cc-760b-4c0d-9dd3-51850df92501": {"__data__": {"id_": "d02302cc-760b-4c0d-9dd3-51850df92501", "embedding": null, "metadata": {"page_label": "7", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e2000de-b1b7-49db-8551-f13a88bd5b76", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "9044e6e2d2a13c0d0d9229028985f89378683f8825cfd7b2ffbe9683fad90f18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c77e0b2-d061-4d17-bf79-8e5d2deb29f8", "node_type": "1", "metadata": {"page_label": "7", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "16f6c6708b577c18be1e63f9771a19cf2c7b846a2c142796ddbb6b505f6a8262", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85a46433-678d-41e0-9b84-7f1b40f22ad1", "node_type": "1", "metadata": {}, "hash": "0ddd9ff4aed32b42ad7fc8f51c544541ce019404ff6aacadecab423da1acd922", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To prompt ReAct, we randomly annotate three trajectories\nfrom the training set for each task type, where each trajectory includes sparse thoughts that (1)\ndecompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via\ncommonsense where to \ufb01nd an object and what to do with it. We show prompts used for ALFWorld\nin Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games\nin a task-speci\ufb01c setup. For robustness, we construct 6 prompts for each task type through each\npermutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using\nthe same trajectories, but without thoughts \u2014 since task instances are randomly chosen from the\ntraining set, it favors neitherReAct nor Act and provides a fair and controlled comparison to test the\nimportance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation\nlearning agent trained on 105 expert trajectories for each task type5.\nWebShop Can ReAct also interact with noisy real-world language environments for practical\napplications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping\nwebsite environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld,\nWebshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions,\nand options crawled from Amazon), and requires an agent to purchase a product based on a user\ninstruction (e.g. \u201cI am looking for a nightstand with drawers. It should have a nickel \ufb01nish, and\npriced lower than $140\u201d) through web interactions (e.g. search \u201cnightstand drawers\u201d, choose buttons\nsuch as \u201ccolor: modern-nickel-white\u201d or \u201cback to search\u201d). This task is evaluated by average score\n(percentage of desired attributes covered by the chosen product averaged across all episodes) and\nsuccess rate (percentage of episodes where the chosen product satis\ufb01es all requirements) on 500 test\ninstructions.", "mimetype": "text/plain", "start_char_idx": 1505, "end_char_idx": 3499, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85a46433-678d-41e0-9b84-7f1b40f22ad1": {"__data__": {"id_": "85a46433-678d-41e0-9b84-7f1b40f22ad1", "embedding": null, "metadata": {"page_label": "7", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e2000de-b1b7-49db-8551-f13a88bd5b76", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "9044e6e2d2a13c0d0d9229028985f89378683f8825cfd7b2ffbe9683fad90f18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d02302cc-760b-4c0d-9dd3-51850df92501", "node_type": "1", "metadata": {"page_label": "7", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "56b420c8e982b731f89bca6d8584c8b17abd5a3a0a67ca5f3b5d75dc4f62edee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We formulate Act prompts with actions to search, choose product, choose options,\nand buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy,\nand what products options are relevant to the instruction. See Table 6 for an example prompt, and\nTable 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method\n5Micheli & Fleuret (2021) \ufb01netuned a GPT-2 model on 3553 task instances and achieved a much improved\nperformance than BUTLER, but it is trained on all task types, thus not included as a baseline.\n7", "mimetype": "text/plain", "start_char_idx": 3500, "end_char_idx": 4066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed9a2722-215e-4e55-a6b8-dbffc81c6cc3": {"__data__": {"id_": "ed9a2722-215e-4e55-a6b8-dbffc81c6cc3", "embedding": null, "metadata": {"page_label": "8", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f022c336-2b9d-4255-9611-95200c3ccda7", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "8112109ac4d397b761bb122a14cfa807fb723566c12caab4271bab0625d8e8bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92849269-72d7-42b8-9178-70b3e34be72d", "node_type": "1", "metadata": {}, "hash": "e3a467f18a6346eccab9ba058b0358b928ecea8c2b0e4100e44208b3580e0986", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nMethod Pick Clean Heat Cool Look Pick 2 All\nAct(best of 6) 88 42 74 67 72 41 45\nReAct(avg) 65 39 83 76 55 24 57\nReAct(best of 6) 92 58 96 86 78 41 71\nReAct-IM(avg) 55 59 60 55 23 24 48\nReAct-IM(best of 6) 62 68 87 57 39 33 53\nBUTLERg (best of 8) 33 26 70 76 17 12 22\nBUTLER(best of 8) 46 39 74 100 22 24 37\nTable 3: AlfWorld task-speci\ufb01c success rates (%). BUTLER and\nBUTLERg results are from Table 4 of Shridhar et al. (2020b). All\nmethods use greedy decoding, except that BUTLER uses beam search.\nMethod Score SR\nAct 62.3 30.1\nReAct 66.6 40.0\nIL 59.9 29.1\nIL+RL 62.4 28.7\nHuman 82.1 59.6Expert\nTable 4: Score and suc-\ncess rate (SR) on Web-\nshop. IL/IL+RL taken\nfrom Yao et al. (2022).\ntrained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)\nmethod additionally trained with 10,587 training instructions.\nResults ReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On\nALFWorld, the bestReAct trial achieves an average success rate of 71%, signi\ufb01cantly outperforming\nthe best Act (45%) and BUTLER (37%) trials. In fact, even the worse ReAct trial (48%) beats\nthe best trial of both methods.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1200, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92849269-72d7-42b8-9178-70b3e34be72d": {"__data__": {"id_": "92849269-72d7-42b8-9178-70b3e34be72d", "embedding": null, "metadata": {"page_label": "8", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f022c336-2b9d-4255-9611-95200c3ccda7", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "8112109ac4d397b761bb122a14cfa807fb723566c12caab4271bab0625d8e8bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed9a2722-215e-4e55-a6b8-dbffc81c6cc3", "node_type": "1", "metadata": {"page_label": "8", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "cfda74a94dea0649ade93d934e82897fe3478654503c1dac20ba934620f3f88c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "819e2501-e5f0-48db-9692-060a8a7568be", "node_type": "1", "metadata": {}, "hash": "d89e9e807e3738e836f083016a6d69609e5d17a890050a0e3f6245402972e16d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, the advantage of ReAct over Act is consistent across\nsix controlled trials, with relative performance gain ranging from 33% to 90% and averaging 62%.\nQualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals\ninto smaller subgoals, or loses track of the current state of the environment. Example trajectories\ncomparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2.\nOn Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With\nadditional sparse reasoning, ReAct achieves signi\ufb01cantly better performance, with an absolute 10%\nimprovement over the previous best success rate. By checking examples, we \ufb01nd that ReAct is more\nlikely to identify instruction-relevant products and options by reasoning to bridge the gap between\nnoisy observations and actions (e.g. \u201cFor \u2018space-saving ottoman bench for living room\u2019, the item\nhas options \u201839x18x18inch\u2019 and \u2018blue\u2019 and seems good to buy.\u201d). However, existing methods are\nstill far from the performance of expert humans (Table 4), who perform signi\ufb01cantly more product\nexplorations and query re-formulations that are still challenging for prompting-based methods.\nOn the value of internal reasoning vs. external feedback To our knowledge, ReAct is the \ufb01rst\ndemonstration of combined reasoning and action using an LLM applied to an interactive environment\nwithin a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang\net al. (2022b), in which actions from an embodied agent are motivated by an eponymous \u201cinner\nmonologue\u201d. However, IM\u2019s \u201cinner monologue\u201d is limited to observations of the environment\nstate and what needs to be completed by the agent for the goal to be satis\ufb01ed. In contrast, the\nreasoning traces in ReAct for decision making is \ufb02exible and sparse, allowing diverse reasoning\ntypes (see Section 2) to be induced for different tasks.\nTo demonstrate the differences between ReAct and IM, and to highlight the importance of internal\nreasoning vs.", "mimetype": "text/plain", "start_char_idx": 1201, "end_char_idx": 3230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "819e2501-e5f0-48db-9692-060a8a7568be": {"__data__": {"id_": "819e2501-e5f0-48db-9692-060a8a7568be", "embedding": null, "metadata": {"page_label": "8", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f022c336-2b9d-4255-9611-95200c3ccda7", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "8112109ac4d397b761bb122a14cfa807fb723566c12caab4271bab0625d8e8bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92849269-72d7-42b8-9178-70b3e34be72d", "node_type": "1", "metadata": {"page_label": "8", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "fba764d99f41f9ed87af2cd12d8335a2ef7c0ab03e88bf74cc5296829e1af51c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "simple reactions to external feedback, we ran an ablation experiment using a thought\npattern composed of IM-like dense external feedback. As can be seen in Table 3,ReAct substantially\noutperforms IM-style prompting ( ReAct-IM) (71 vs. 53 overall success rate), with consistent\nadvantages on \ufb01ve out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakes\nin identifying when subgoals were \ufb01nished, or what the next subgoal should be, due to a lack of high-\nlevel goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where\nan item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.\nBoth shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in\nAppendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example\ntrajectory in Appendix D.2.3.\n8", "mimetype": "text/plain", "start_char_idx": 3231, "end_char_idx": 4113, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30427af7-31a2-4d3e-9821-75fce86ba9f7": {"__data__": {"id_": "30427af7-31a2-4d3e-9821-75fce86ba9f7", "embedding": null, "metadata": {"page_label": "9", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "09a58be2-948e-4afd-9266-81ab087339de", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "a7fe22317253b8c64086f9640c2ff8875c01be9dd03f2b66015d6cde561f5683", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7922a6c8-3912-4df7-b23a-2ea38d3f3aea", "node_type": "1", "metadata": {}, "hash": "91e60fe9fc153990dda839bea22e5bf12bb6c58afc89b347bf0617cf8188e7a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\n5 R ELATED WORK\nLanguage model for reasoning Perhaps the most well-known work of using LLMs for reasoning\nis Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their\nown \u201cthinking procedure\u201d for problem solving. Several follow-up works have since been performed,\nincluding least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot-\nCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,\n(Madaan & Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and\nobserved that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other\nwork has also been extended to more sophisticated reasoning architecture beyond simple prompting.\nFor example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps\nof \u201cselection\u201d and \u201cinference\u201d. STaR (Zelikman et al., 2022) bootstraps the reasoning process by\n\ufb01netuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell\n& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated\nLM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which \ufb01netunes a LM on\nintermediate computation steps, also demonstrate improvement on multi-step computation problems.\nIn contrast to these methods, ReAct performs more than just isolated, \ufb01xed reasoning, and integrates\nmodel actions and their corresponding observations into a coherent stream of inputs for the model to\nreason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\nLanguage model for decision making The strong capability of LLMs has enabled them to perform\ntasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a\npolicy model for decision making, especially in interactive environments.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7922a6c8-3912-4df7-b23a-2ea38d3f3aea": {"__data__": {"id_": "7922a6c8-3912-4df7-b23a-2ea38d3f3aea", "embedding": null, "metadata": {"page_label": "9", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "09a58be2-948e-4afd-9266-81ab087339de", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "a7fe22317253b8c64086f9640c2ff8875c01be9dd03f2b66015d6cde561f5683", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30427af7-31a2-4d3e-9821-75fce86ba9f7", "node_type": "1", "metadata": {"page_label": "9", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "1ebf0ef47a393d47b486848d9f7dd60ee68b7ba48b05aaf2fc0c462401030b89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c8d9dea-1adc-4679-8bfa-ad5e4f04f63d", "node_type": "1", "metadata": {}, "hash": "4ba1bc2a6188325820c6283e1eb9a85360765788c533747e8d80048c6d9cde22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "WebGPT (Nakano et al.,\n2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to\ncomplicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not\nexplicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for\nreinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)\nand Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl\net al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly\nconsider the reasoning procedure either, and also relies on expensive datasets and human feedback\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\ndecision making process only requires language description of the reasoning procedure.6\nLLMS have also been increasingly employed in interactive and embodied environments for planning\nand decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)\nand Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision\nmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which\nis then reranked by an affordance model grounded on the visual environments for \ufb01nal prediction.\nInner Monologue made further improvements by adding the eponymous \u201cinner monologue\", which is\nimplemented as injected feedback from the environment. To our knowledge, Inner Monologue is the\n\ufb01rst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue\nthat Inner Monologue does not truly comprise of inner thoughts \u2014 this is elaborated in Section 4. We\nalso note that leveraging language as semantically-rich inputs in the process of interactive decision\nmaking has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti\net al., 2021; Huang et al., 2022a; Li et al., 2022).", "mimetype": "text/plain", "start_char_idx": 1984, "end_char_idx": 4005, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c8d9dea-1adc-4679-8bfa-ad5e4f04f63d": {"__data__": {"id_": "0c8d9dea-1adc-4679-8bfa-ad5e4f04f63d", "embedding": null, "metadata": {"page_label": "9", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "09a58be2-948e-4afd-9266-81ab087339de", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "a7fe22317253b8c64086f9640c2ff8875c01be9dd03f2b66015d6cde561f5683", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7922a6c8-3912-4df7-b23a-2ea38d3f3aea", "node_type": "1", "metadata": {"page_label": "9", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "63e183f934406b48caca26b66d1cd5ff80dc49b6cf6a4197ee653d6e6e90508c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is becoming more evident that with the help of\nLLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and\ndecision making. What is more, progress in LLMs has also inspired the development of versatile and\ngeneralist agents like Reed et al. (2022).\n6 C ONCLUSION\nWe have proposedReAct \u2013 a simple yet effective method for synergizing reasoning and acting in\nlarge language models. Through a diverse set of experiments on multi-hop question-answering, fact\nchecking, and interactive decision-making tasks, we show that ReAct leads to superior performance\nwith interpretable decision traces. Despite the simplicity of our method, complex tasks with large\naction spaces require more demonstrations to learn well, which unfortunately can easily go beyond\nthe input length limit of in-context learning. We explore the \ufb01ne-tuning approach on HotpotQA\n6Human feedback can also be incorporated in a complementary manner but we leave it for future work.\n9", "mimetype": "text/plain", "start_char_idx": 4006, "end_char_idx": 4991, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b963f9e4-eceb-4f04-918c-38c21df6d0b1": {"__data__": {"id_": "b963f9e4-eceb-4f04-918c-38c21df6d0b1", "embedding": null, "metadata": {"page_label": "10", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc66b82d-aca3-4dc3-8a08-7e38a0169aa7", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "20bf013b5bdbee6fbc848e4babb7ef5d25c49841bfbbef7e7070d20ad614b5db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52473bb4-9388-498e-b70f-53545960402f", "node_type": "1", "metadata": {}, "hash": "94f4c245377b77789172ef722a09d944c822fb7885cf5b1266193da1631fcf0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nwith initial promising results, but learning from more high-quality human annotations will be the\ndesiderata to further improve the performance. Scaling up ReAct with multi-task training and\ncombining it with complementary paradigms like reinforcement learning could result in stronger\nagents that further unlock the potential of LLMs for more applications.\nACKNOWLEDGMENTS\nWe thank the support and feedback of many people from Google Brain team and Princeton NLP\nGroup. This work was supported in part by the National Science Foundation under Grant No.\n2107048. Any opinions, \ufb01ndings, and conclusions or recommendations expressed in this material are\nthose of the author(s) and do not necessarily re\ufb02ect the views of the National Science Foundation.\nREPRODUCIBILITY STATEMENT\nOur main experiments are done on PaLM (Chowdhery et al., 2022), which is not an openly accessible\nmodel yet. To increase reproducibility, we have included all used prompts in Appendix C, additional\nexperiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct\nprompting code at https://anonymous.4open.science/r/ReAct-2268/.\nETHICS STATEMENT\nReAct prompts large language models to generate more human interpretable, diagnosable, and\ncontrollable task-solving trajectories than previous methods. However, hooking up a large language\nmodel with an action space to interact with external environments (e.g. the web, physical environ-\nments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful\nactions in an environment. Our experiments minimize such risks by limiting the interactions to\nspeci\ufb01c websites (Wikipedia or WebShop) that are free of private information, without any dangerous\nactions in the action space design (i.e. models cannot really buy products on WebShop the research\nbenchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing\nmore extensive experiments in the future.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52473bb4-9388-498e-b70f-53545960402f": {"__data__": {"id_": "52473bb4-9388-498e-b70f-53545960402f", "embedding": null, "metadata": {"page_label": "10", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc66b82d-aca3-4dc3-8a08-7e38a0169aa7", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "20bf013b5bdbee6fbc848e4babb7ef5d25c49841bfbbef7e7070d20ad614b5db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b963f9e4-eceb-4f04-918c-38c21df6d0b1", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "96876852af3d8591ed18b5f762ae7f0207ce133052fe122f83a0c63729637cb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c64fe3c9-a89d-4d19-99da-dc52157e6963", "node_type": "1", "metadata": {}, "hash": "2183de76dfb4c21e7fa2a966e18b4c36a9df347769cb36c85eb49b159a1a423f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We believe researchers should be aware of such risks before designing\nmore extensive experiments in the future.\nREFERENCES\nJosh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita\nChhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim\nHarley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathew-\nson, So\u02c7na Mokr\u00e1, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,\nDuncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence,\n2020. URL https://arxiv.org/abs/2012.05672.\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally\nJesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka\nRao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and\nAndy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL\nhttps://arxiv.org/abs/2204.01691.\nBen Alderson-Day and Charles Fernyhough. Inner speech: development, cognitive functions,\nphenomenology, and neurobiology. Psychological bulletin, 141(5):931, 2015.\nAlan Baddeley.", "mimetype": "text/plain", "start_char_idx": 1907, "end_char_idx": 3484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c64fe3c9-a89d-4d19-99da-dc52157e6963": {"__data__": {"id_": "c64fe3c9-a89d-4d19-99da-dc52157e6963", "embedding": null, "metadata": {"page_label": "10", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc66b82d-aca3-4dc3-8a08-7e38a0169aa7", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "20bf013b5bdbee6fbc848e4babb7ef5d25c49841bfbbef7e7070d20ad614b5db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52473bb4-9388-498e-b70f-53545960402f", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "8542e5c96b0fe3395df0e4ccdd1b43c3d3cabe3d478c646edbc83dbcf6cf5984", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Alan Baddeley. Working memory. Science, 255(5044):556\u2013559, 1992.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n10", "mimetype": "text/plain", "start_char_idx": 3470, "end_char_idx": 3808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94a68eee-16b6-4f33-ae36-3b457afb3bb5": {"__data__": {"id_": "94a68eee-16b6-4f33-ae36-3b457afb3bb5", "embedding": null, "metadata": {"page_label": "11", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "483f9e6c-4e8d-4a8b-9a6d-b91fa78fd616", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "bc843ab34cd411e47038a9800caab17f3ea9f1bed2832a35867dc7a323456a31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12f767a0-a741-414e-967e-e9c483b15877", "node_type": "1", "metadata": {}, "hash": "36b5db009c450692f6c285775ea11ff2927bd0d8859951684e14fc2649d1cc2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nAntonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL\nhttps://arxiv.org/abs/2208.14271.\nAntonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large\nlanguage models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/\n2205.09712.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 3558\u20133567, Florence, Italy, July 2019. Association for Com-\nputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/\nP19-1346.\nCharles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social\ninteraction and the development of social understanding and executive functions, pp. 56\u201379, 2010.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12f767a0-a741-414e-967e-e9c483b15877": {"__data__": {"id_": "12f767a0-a741-414e-967e-e9c483b15877", "embedding": null, "metadata": {"page_label": "11", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "483f9e6c-4e8d-4a8b-9a6d-b91fa78fd616", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "bc843ab34cd411e47038a9800caab17f3ea9f1bed2832a35867dc7a323456a31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94a68eee-16b6-4f33-ae36-3b457afb3bb5", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "527c079c1a8c368d39761d96e971dc036e1b4d27a8135eac0bfa5d2089898062", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c11bd8af-a63e-4251-aaa7-d48b91a1d92d", "node_type": "1", "metadata": {}, "hash": "8873006b619f84c9d776927096301070917d46bc5ac95268248adf3ff9c62443", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "56\u201379, 2010.\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari-\nbeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,\nJonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth\nDathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green,\nSo\u02c7na Mokr\u00e1, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,\nWilliam Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and\nGeoffrey Irving. Improving alignment of dialogue agents via targeted human judgements,\n2022. URL https://storage.googleapis.com/deepmind-media/DeepMind.\ncom/Authors-Notes/sparrow/sparrow-final.pdf.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple\nlanguage model for task-oriented dialogue. Advances in Neural Information Processing Systems,\n33:20179\u201320191, 2020.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207,\n2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022b.\nSiddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed\nlatent actions. In CoRL, pp. 1379\u20131390, 2021.", "mimetype": "text/plain", "start_char_idx": 1169, "end_char_idx": 2699, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c11bd8af-a63e-4251-aaa7-d48b91a1d92d": {"__data__": {"id_": "c11bd8af-a63e-4251-aaa7-d48b91a1d92d", "embedding": null, "metadata": {"page_label": "11", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "483f9e6c-4e8d-4a8b-9a6d-b91fa78fd616", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "bc843ab34cd411e47038a9800caab17f3ea9f1bed2832a35867dc7a323456a31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12f767a0-a741-414e-967e-e9c483b15877", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "7d50ad939385b0acccb7a16db4bb6d54afb694a18ff33e1527af09c0a6e9e574", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In CoRL, pp. 1379\u20131390, 2021. URL https://proceedings.mlr.press/\nv164/karamcheti22a.html.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-\naugmented language models through few-shot prompting for open-domain question answering.\narXiv preprint arXiv:2203.05115, 2022.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented genera-\ntion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:\n9459\u20139474, 2020.\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An\nHuang, Ekin Aky\u00fcrek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,\nand Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https:\n//arxiv.org/abs/2202.01771.\n11", "mimetype": "text/plain", "start_char_idx": 2670, "end_char_idx": 3748, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb5b3aea-572b-4a0b-be39-ce380715c141": {"__data__": {"id_": "cb5b3aea-572b-4a0b-be39-ce380715c141", "embedding": null, "metadata": {"page_label": "12", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "471671c9-c130-4201-97a2-6c830550b8db", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "2d4236f323c1e85cde978fc93b2cb72dd27edf889fa9fc2d638587f2a9874d15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dc1c486-bc29-41e6-89ca-05fbcb387af1", "node_type": "1", "metadata": {}, "hash": "9bc328a61063b9225b353145d70eed6214d11aa1c7d9daccbf1c20f125e7c6cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nAleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. Neuropsy-\nchologia, 3(4):387\u2013392, 1965.\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes\ntwo to tango, 2022. URL https://arxiv.org/abs/2209.07686.\nVincent Micheli and Fran\u00e7ois Fleuret. Language models are few-shot butlers. arXiv preprint\narXiv:2104.07972, 2021.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\nBrowser-assisted question-answering with human feedback, 2021. URL https://arxiv.\norg/abs/2112.09332.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and\nAugustus Odena. Show your work: Scratchpads for intermediate computation with language\nmodels, 2021. URL https://arxiv.org/abs/2112.00114.\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https:\n//arxiv.org/abs/2205.06175.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1522, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4dc1c486-bc29-41e6-89ca-05fbcb387af1": {"__data__": {"id_": "4dc1c486-bc29-41e6-89ca-05fbcb387af1", "embedding": null, "metadata": {"page_label": "12", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "471671c9-c130-4201-97a2-6c830550b8db", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "2d4236f323c1e85cde978fc93b2cb72dd27edf889fa9fc2d638587f2a9874d15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb5b3aea-572b-4a0b-be39-ce380715c141", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "b2b00fd76c8f21ccfa007895f59c81270011e87abbb33d7c8e843362b1f83784", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08f36c52-7469-4903-ad2b-e19180dd5385", "node_type": "1", "metadata": {}, "hash": "f3a2dc383160349ae6b774356f87b416407c33d108349df26875548b536acb8d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "URL https:\n//arxiv.org/abs/2205.06175.\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,\nLuke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions\nfor everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 10740\u201310749, 2020a.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and Matthew\nHausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv\npreprint arXiv:2010.03768, 2020b.\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston.\nLanguage models that seek for knowledge: Modular search & generation for dialogue and prompt\ncompletion. arXiv preprint arXiv:2203.13224, 2022a.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman\nGoyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3:\na deployed conversational agent that continually learns to responsibly engage, 2022b. URL\nhttps://arxiv.org/abs/2208.03188.\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale\ndataset for fact extraction and veri\ufb01cation. arXiv preprint arXiv:1803.05355, 2018.\nLev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky, 1:39\u2013285, 1987.", "mimetype": "text/plain", "start_char_idx": 1484, "end_char_idx": 2962, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08f36c52-7469-4903-ad2b-e19180dd5385": {"__data__": {"id_": "08f36c52-7469-4903-ad2b-e19180dd5385", "embedding": null, "metadata": {"page_label": "12", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "471671c9-c130-4201-97a2-6c830550b8db", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "2d4236f323c1e85cde978fc93b2cb72dd27edf889fa9fc2d638587f2a9874d15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dc1c486-bc29-41e6-89ca-05fbcb387af1", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "246e16dd6b75b979bf91a72e6586fab9486bd013ce980ed1a7174870346693c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,\n2022a. URL https://arxiv.org/abs/2203.11171.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented\nensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,\nand Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering. arXiv preprint arXiv:1809.09600, 2018.\n12", "mimetype": "text/plain", "start_char_idx": 2963, "end_char_idx": 3791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f5a1dede-4fc9-4fa5-953d-6d13a1221268": {"__data__": {"id_": "f5a1dede-4fc9-4fa5-953d-6d13a1221268", "embedding": null, "metadata": {"page_label": "13", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e76590cc-06e7-4aea-8441-e2dd363f22d9", "node_type": "4", "metadata": {"page_label": "13", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "e36a0b151f93e177a10c680c9e75287da7d6a26a04d6e06bf0d5badbb67cd97b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:\nLanguage models for action generation in text-based games. InProceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP), pp. 8736\u20138754, Online, Novem-\nber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704.\nURL https://aclanthology.org/2020.emnlp-main.704.\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable\nreal-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206,\n2022.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with\nreasoning, 2022. URL https://arxiv.org/abs/2203.14465.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in\nlarge language models, 2022. URL https://arxiv.org/abs/2205.10625.\nYunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information\nseeking for open-domain question answering. arXiv preprint arXiv:2109.06747, 2021.\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1221, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "45d61ee0-cedd-4764-bb2d-59643d40227d": {"__data__": {"id_": "45d61ee0-cedd-4764-bb2d-59643d40227d", "embedding": null, "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4cb70d6-c274-4d05-8f7c-3562a7b5a3c2", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "d959ceac940fca18ffa5ce718f6a723bdf58da5331a4ba935c363de0db105cdf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b524774d-03e3-4331-afc4-b05f840260ff", "node_type": "1", "metadata": {}, "hash": "c1aa088aa4e0a23ec83d58916a513119911fdaaa2a3eaf6011157dccf5748dde", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nA A DDITIONAL RESULTS\nA.1 GPT-3 E XPERIMENTS\nPaLM-540B GPT-3\nHotpotQA (exact match) 29.4 30.8\nALFWorld (success rate %) 70.9 78.4\nTable 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding).\nOn HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all\n134 unseen validation task instances, and use the best prompt set according to PaLM-540B.\nWe run additional GPT-3 (Brown et al., 2020) experiments to con\ufb01rmReAct prompting performance\nis general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002,\ngreedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly\nbecause it is \ufb01netuned with human instruction following. This indicates ReAct prompting is effective\nacross different large language models on different tasks. The code for these experiments are at\nhttps://react-lm.github.io/.\nA.2 REAC TOBTAINS UP -TO-DATE KNOWLEDGE ON HOTPOT QA\n\u000b\u0014D\f\u00036WDQGDUG\n$QVZHU\u001d\u0003\u0016\u000f\u0013\u0013\u0013\n\u000b\u0014E\f\u0003&R7\u0003\u000b5HDVRQ\u00032QO\\\f\n7KRXJKW\u001d\u0003/HW\nV\u0003WKLQN\u0003VWHS\u0003\nE\\\u0003VWHS\u0011\u00037KH\u0003KRWHO\u0003WKDW\u0003LV\u0003\nKRPH\u0003WR\u0003WKH\u0003&LUTXH\u0003GX\u0003\n6ROHLO\u0003VKRZ\u00030\\VWHUH\u0003LV\u0003\n7UHDVXUH\u0003,VODQG\u0011\u00037UHDVXUH\u0003\n,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b524774d-03e3-4331-afc4-b05f840260ff": {"__data__": {"id_": "b524774d-03e3-4331-afc4-b05f840260ff", "embedding": null, "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4cb70d6-c274-4d05-8f7c-3562a7b5a3c2", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "d959ceac940fca18ffa5ce718f6a723bdf58da5331a4ba935c363de0db105cdf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45d61ee0-cedd-4764-bb2d-59643d40227d", "node_type": "1", "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "ecdb29567bf6281f87d806125f6dc48a1af77b94cc0cf6de186162561f25dd03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cc14555-df9d-463b-8d77-b9626057d74b", "node_type": "1", "metadata": {}, "hash": "afc28cdf0acee1fbcd728e6a5d3027bf0ff59b8422988e369d9ead42b6e84fcd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "VODQG\u0011\u00037UHDVXUH\u0003\n,VODQG\u0003KDV\u0003\u0015\u000f\u001b\u001b\u0018\u0003URRPV\u000f\u0003VR\u0003\nWKH\u0003DQVZHU\u0003LV\u0003\u0015\u000f\u001b\u001b\u0018\u0011\n$QVZHU\u001d\u0003\u0015\u000f\u001b\u001b\u0018\n\u000b\u0014F\f\u0003$FW\u00102QO\\\u0003\n$FW\u0003\u0014\u001d\u00036HDUFK>&LUTXH\u0003GX\u00036ROHLO@\u0003\n2EV\u0003\u0014\u001d\u0003&LUTXH\u0003GX\u00036ROHLO\u0003LV\u0003D\u0003\n&DQDGLDQ\u0003HQWHUWDLQPHQW\u0003FRPSDQ\\\u00aa\n$FW\u0003\u0015\u001d\u0003/RRNXS>0\\VWHUH@\u0003\n2EV\u0003\u0015\u001d\u00031R\u0003PRUH\u0003UHVXOWV\n$FW\u0003\u0016\u001d\u00036HDUFK>0\\VWHUH\u0003\u000b&LUTXH\u0003GX\u0003\n6ROHLO\u0003VKRZ\f@\u0003\n2EV\u0003\u0016\u001d\u0003&RXOG\u0003QRW\u0003ILQG\u00aa\n$FW\u0003\u0017\u001d\u00036HDUFK>0\\VWHUH\u0003\u000b&LUTXH\u0003GX\u0003\n6ROHLO\f@\u0003\n2EV\u0003\u0017\u001d\u00030\\VWqUH\u0003\u000bP\\\u0010VWHHU\f\u0003\u0003LV\u0003\nRQH\u0003RI\u0003VL[\u00aa\n$FW\u0003\u0018\u001d\u0003/RRNXS>7UHDVXUH\u0003,VODQG\u0003\n+RWHO\u0003DQG\u0003&DVLQR@\u0003\n\u00aa\u00aa\u000bHQG\u0003ZLWKRXW\u0003DQVZHU\f\n\u000b\u0014G\f\u00035H$FW\u0003\u000b5HDVRQ\u0003\u000e\u0003$FW\f\n7KRXJKW\u0003\u0014\u001d\u0003,", "mimetype": "text/plain", "start_char_idx": 1176, "end_char_idx": 1683, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1cc14555-df9d-463b-8d77-b9626057d74b": {"__data__": {"id_": "1cc14555-df9d-463b-8d77-b9626057d74b", "embedding": null, "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4cb70d6-c274-4d05-8f7c-3562a7b5a3c2", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "d959ceac940fca18ffa5ce718f6a723bdf58da5331a4ba935c363de0db105cdf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b524774d-03e3-4331-afc4-b05f840260ff", "node_type": "1", "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "cb09bd97442fdbe5e64e83b20c1d1dd4d8a2d79c867841c5aa70ce812354bfc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64f25ea0-264c-477c-aef2-3185c548ffb1", "node_type": "1", "metadata": {}, "hash": "b384c1cdf61c1c42cd9f5d51a7545ec05c85b5ea6f824e7a138b30f74b550126", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u0003QHHG\u0003WR\u0003VHDUFK\u0003&LUTXH\u0003GX\u00036ROHLO\u0003VKRZ\u00030\\VWHUH\u000f\u0003\nILQG\u0003WKH\u0003KRWHO\u0003LW\u0003LV\u0003LQ\u000f\u0003WKHQ\u0003ILQG\u0003WKH\u0003QXPEHU\u0003RI\u0003URRPV\u0003LQ\u0003\nWKH\u0003KRWHO\u0011\n$FW\u0003\u0014\u001d\u00036HDUFK>&LUTXH\u0003GX\u00036ROHLO\u0003VKRZ\u00030\\VWHUH@\n2EV\u0003\u0014\u001d\u0003&RXOG\u0003QRW\u0003ILQG\u0003&LUTXH\u0003GX\u00036ROHLO\u0003VKRZ\u00030\\VWHUH\u0011\u0003\n6LPLODU\u001d\u0003>\n&LUTXH\u0003GX\u00036ROHLO\n\u0003\u00aa\n7KRXJKW\u0003\u0015\u001d\u00037R\u0003ILQG\u0003WKH\u0003KRWHO\u000f\u0003,\u0003FDQ\u0003VHDUFK\u00030\\VWHUH\u0003\u000b&LUTXH\u0003\nGX\u00036ROHLO\f\u0011\n$FW\u0003\u0015\u001d\u00036HDUFK>0\\VWHUH\u0003\u000b&LUTXH\u0003GX\u00036ROHLO\f@\n2EV\u0003\u0015\u001d\u00030\\VWqUH\u0003\u000bP\\\u0010VWHHU\f\u0003\u0003LV\u0003\u00aa\u0003+HOG\u0003LQ\u0003D\u0003FXVWRP\u0003WKHDWUH\u0003DW\u0003\nWKH\u00037UHDVXUH\u0003,VODQG\u0003+RWHO\u0003DQG\u0003&DVLQR\u0003\u00aa\n7KRXJKW\u0003\u0016\u001d\u00030\\VWHUH\u0003LV\u0003KHOG\u0003LQ\u0003D\u0003FXVWRP\u0003WKHDWUH\u0003DW\u0003WKH\u0003\n7UHDVXUH\u0003,VODQG\u0003+RWHO\u0003DQG\u0003&DVLQR\u0011\u00036R\u0003,\u0003QHHG\u0003WR\u0003VHDUFK\u0003\n7UHDVXUH\u0003,", "mimetype": "text/plain", "start_char_idx": 1683, "end_char_idx": 2265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64f25ea0-264c-477c-aef2-3185c548ffb1": {"__data__": {"id_": "64f25ea0-264c-477c-aef2-3185c548ffb1", "embedding": null, "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4cb70d6-c274-4d05-8f7c-3562a7b5a3c2", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "d959ceac940fca18ffa5ce718f6a723bdf58da5331a4ba935c363de0db105cdf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cc14555-df9d-463b-8d77-b9626057d74b", "node_type": "1", "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "ddfd19d61770807a6775392e7a4074fcecb7b9e0835c2a3b2a30c8ef05732c02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0e0545a-0ac5-4755-934d-65dfd5170fd5", "node_type": "1", "metadata": {}, "hash": "9bffd02cc10a681dbd76a32e51b9f885e5373fa37e94ec4bdecda5cad1854389", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "VODQG\u0003+RWHO\u0003DQG\u0003&DVLQR\u0003QH[W\u0003DQG\u0003ILQG\u0003WKH\u0003QXPEHU\u0003\nRI\u0003URRPV\u0003LQ\u0003WKH\u0003KRWHO\u0011\n$FW\u0003\u0016\u001d\u00036HDUFK>7UHDVXUH\u0003,VODQG\u0003+RWHO\u0003DQG\u0003&DVLQR@\n2EV\u0003\u0016\u001d\u00037UHDVXUH\u0003,VODQG\u0003+RWHO\u0003DQG\u0003&DVLQR\u0003\u00aa\u0003ZLWK\u0003\u0015\u000f\u001b\u001b\u0017\u0003URRPV\u0003\nDQG\u0003\u0015\u0015\u0013\u0003VXLWHV\u0003\u00aa\n7KRXJKW\u0003\u0017\u001d\u00037UHDVXUH\u0003,VODQG\u0003+RWHO\u0003DQG\u0003&DVLQR\u0003KDV\u0003\u0015\u000f\u001b\u001b\u0017\u0003URRPV\u0003\nDQG\u0003\u0015\u0015\u0013\u0003VXLWHV\u0011\u00036R\u0003WKH\u0003DQVZHU\u0003LV\u0003\u0016\u000f\u0014\u0013\u0017\u0011\n$FW\u0003\u0017\u001d\u0003)LQLVK>\u0016\u000f\u0014\u0013\u0017@ \u04bc\n4XHVWLRQ\u001d\u0003+RZ\u0003PDQ\\\u0003URRPV\u0003DUH\u0003LQ\u0003WKH\u0003KRWHO\u0003WKDW\u0003LV\u0003KRPH\u0003WR\u0003\nWKH\u0003&LUTXH\u0003GX\u00036ROHLO\u0003VKRZ\u00030\\VWHUH\"\n+RWSRW4$\u0003ODEHO\u001d\u0003\u0015\u000f\u0019\u0019\u0017\n\u000b\u0014\f\u0003+RWVSRW\u00034$\n2XWGDWHG\u0003\n8S\u0010WR\u0010GDWH\u0003\nFigure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is\nable to obtain the up-to-date answer thanks to real-world web interaction plus reasoning.\nDuring trajectory inspection, we also \ufb01nd that sometimesReAct does not agree with dataset labels as\nthe labels themselves could be outdated. For example, as shown in Figure 4, the question asks about\nthe size of a hotel, which increased from the HotpotQA construction time.", "mimetype": "text/plain", "start_char_idx": 2265, "end_char_idx": 3199, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0e0545a-0ac5-4755-934d-65dfd5170fd5": {"__data__": {"id_": "d0e0545a-0ac5-4755-934d-65dfd5170fd5", "embedding": null, "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4cb70d6-c274-4d05-8f7c-3562a7b5a3c2", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "d959ceac940fca18ffa5ce718f6a723bdf58da5331a4ba935c363de0db105cdf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64f25ea0-264c-477c-aef2-3185c548ffb1", "node_type": "1", "metadata": {"page_label": "14", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "da0382084c386a3cbc5f934d7706ba15c7b13de8cdbb7f1e85f113e5767d406e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "WhileStandard and CoT\ngive wrong answers due to hallucination, Act fails despite the access of real-world web interaction,\ndue to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to\nretrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better\nincorporation of reasoning abilities might bene\ufb01t recent Internet-augmented language models (Nakano\net al., 2021; Lazaridou et al., 2022; Shuster et al., 2022a) for up-to-date task solving.\nA.3 H UMAN -IN-THE -LOOP BEHAVIOR CORRECTION ON ALFWORLD\nWe also explore human-in-the-loop interaction with ReAct, to allow a human to inspect and edit\nReAct\u2019s reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act\n17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align\nwith these human thought edits and succeed in the task. From a human perspective, solving such a\ntask becomes signi\ufb01cantly easier, from typing tens of actions to only editing a couple of thoughts,\nwhich enables new forms of human-machine collaboration. We note that such a policy edit on-the-go\n14", "mimetype": "text/plain", "start_char_idx": 3200, "end_char_idx": 4358, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1b7ead6-334e-4bab-8b28-92da4c16d369": {"__data__": {"id_": "f1b7ead6-334e-4bab-8b28-92da4c16d369", "embedding": null, "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e12086a-da1a-45f7-a4a2-61acfe5763db", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "45539aa54f34b0bda4ac9202227b7645060841a611897a1be95e0f4e1eaae69b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ce39cc1-62ba-493e-9956-13eaf90e8797", "node_type": "1", "metadata": {}, "hash": "0fb8e90fe2b44c7754cd1882ee8cab8035d1ef7f126ff94b4c168928b3300d6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\n\u000bD\f\u00035H$FW\u0003\n\u0011\u0011\u0011\u0011\u0011\u0011\n2EV\u0003\u0014\u0019\u001d\u0003<RX\u0003RSHQ\u0003WKH\u0003GUDZHU\u0003\u0017\u0011\u00037KH\u0003GUDZHU\u0003\u0017\u0003LV\u0003RSHQ\u0011\u0003,Q\u0003LW\u000f\u0003\\RX\u0003\nVHH\u0003D\u0003NH\\FKDLQ\u0003\u0016\u000f\u0003DQG\u0003D\u0003ZDWFK\u0003\u0015\u0011\u0003\n$FW\u0003\u0014\u001a\u001d\u00037KLQN\u0003>1RZ\u0003,\u0003ILQG\u0003WKH\u0003ILUVW\u0003NH\\FKDLQ\u0003\u000b\u0016\f\u0011\u00031H[W\u000f\u0003,\u0003QHHG\u0003WR\u0003\nWDNH\u0003LW\u0011\u0003,\u0003FDQ\u0003ILQG\u0003WKH\u0003VHFRQG\u0003NH\\FKDLQ\u0003\u000b\u0015\f\u0003ODWHU\u0003LQ\u0003GUDZHU\u0003\u0017\u0011\u0003@\n$FW\u0003\u0014\u001b\u001d\u0003WDNH\u0003NH\\FKDLQ\u0003\u0016\u0003IURP\u0003GUDZHU\u0003\u0017\u0003\n2EV\u0003\u0014\u001b\u001d\u0003<RX\u0003SLFN\u0003XS\u0003WKH\u0003NH\\FKDLQ\u0003\u0016\u0003IURP\u0003WKH\u0003GUDZHU\u0003\u0017\u0011\u0003\n\u0011\u0011\u0011\u0011\u0011\u0011\n$FW\u0003\u0015\u0015\u001d\u0003SXW\u0003NH\\FKDLQ\u0003\u0016\u0003LQ\u0012RQ\u0003VDIH\u0003\u0014\u0003\n2EV\u0003\u0015\u0015\u001d\u0003<RX\u0003SXW\u0003WKH\u0003NH\\FKDLQ\u0003\u0016\u0003LQ\u0012RQ\u0003WKH\u0003VDIH\u0003\u0014\u0011\u0003\n$FW\u0003\u0015\u0016\u001d\u00037KLQN\u0003>1RZ\u0003,\u0003SXW\u0003WKH\u0003ILUVW\u0003NH\\FKDLQ\u0003LQ\u0003VDIH\u0011\u00031H[W\u000f\u0003,\u0003QHHG\u0003\nWR\u0003ILQG\u0003WKH\u0003VHFRQG\u0003NH\\FKDLQ\u0011\u0003,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ce39cc1-62ba-493e-9956-13eaf90e8797": {"__data__": {"id_": "1ce39cc1-62ba-493e-9956-13eaf90e8797", "embedding": null, "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e12086a-da1a-45f7-a4a2-61acfe5763db", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "45539aa54f34b0bda4ac9202227b7645060841a611897a1be95e0f4e1eaae69b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1b7ead6-334e-4bab-8b28-92da4c16d369", "node_type": "1", "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "7e3dd2e4636eff88b2c3b071e2f54c96edbfcd37c3ccc7cb5a091a7f54a1b3e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e24752aa-02a9-4e8c-919b-8ff8784d527e", "node_type": "1", "metadata": {}, "hash": "26d4783cf4ff0ee55a3504f01cfd45b0b0dd6b9796ca8ac82fe098f8e5c72200", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u0003FDQ\u0003GLUHFWO\\\u0003JR\u0003WR\u0003GUDZHU\u0003\u0017\u0011\u0003@\n$FW\u0003\u0015\u0017\u001d\u0003JR\u0003WR\u0003GUDZHU\u0003\u0017\u0003\n2EV\u0003\u0015\u0017\u001d\u00037KH\u0003GUDZHU\u0003\u0017\u0003LV\u0003RSHQ\u0011\u0003,Q\u0003LW\u000f\u0003\\RX\u0003VHH\u0003D\u0003ZDWFK\u0003\u0015\u0011\u0003\n$FW\u0003\u0015\u0018\u001d\u00037KLQN\u0003>1RZ\u0003,\u0003ILQG\u0003WKH\u0003VHFRQG\u0003NH\\FKDLQ\u0003\u000b\u0015\f\u0011\u00031H[W\u000f\u0003,\u0003QHHG\u0003\nWR\u0003WDNH\u0003LW\u0011@\n$FW\u0003\u0015\u0019\u001d\u0003WDNH\u0003NH\\FKDLQ\u0003\u0015\u0003IURP\u0003GUDZHU\u0003\u0017\u0003\n2EV\u0003\u0015\u0019\u001d\u00031RWKLQJ\u0003KDSSHQV\u0011\n\u000bE\f\u00035H$FW\u0003\u000b+XPDQ\u00037KRXJKW\u0003HGLW\f\n\u0011\u0011\u0011\u0011\u0011\u0011\n2EV\u0003\u0014\u0019\u001d\u0003<RX\u0003RSHQ\u0003WKH\u0003GUDZHU\u0003\u0017\u0011\u00037KH\u0003GUDZHU\u0003\u0017\u0003LV\u0003RSHQ\u0011\u0003,Q\u0003LW\u000f\u0003\\RX\u0003VHH\u0003D\u0003\nNH\\FKDLQ\u0003\u0016\u000f\u0003DQG\u0003D\u0003ZDWFK\u0003\u0015\u0011\u0003\n$FW\u0003\u0014\u001a\u001d\u04557KLQN\u0455>1RZ\u0003,\u0003ILQG\u0003WKH\u0003ILUVW\u0003NH\\FKDLQ\u0003\u000b\u0016\f\u0011\u00031H[W\u000f\u0003,\u0003QHHG\u0003WR\u0003WDNH\u0003LW\u0011\u0003,", "mimetype": "text/plain", "start_char_idx": 579, "end_char_idx": 1047, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e24752aa-02a9-4e8c-919b-8ff8784d527e": {"__data__": {"id_": "e24752aa-02a9-4e8c-919b-8ff8784d527e", "embedding": null, "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e12086a-da1a-45f7-a4a2-61acfe5763db", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "45539aa54f34b0bda4ac9202227b7645060841a611897a1be95e0f4e1eaae69b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ce39cc1-62ba-493e-9956-13eaf90e8797", "node_type": "1", "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "26a90d724f6d1d241b5a2e574334dd3f8653c16977b0e7d1695807291cfe8398", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbb1a6d5-e54c-44ee-8c63-a39c78ea30f1", "node_type": "1", "metadata": {}, "hash": "8b890efbcb108b64562724a3f77d77b1a9834c3aa211fc6ed0142b24da64e42f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u0003QHHG\u0003WR\u0003WDNH\u0003LW\u0011\u0003,\u0003\nFDQ\u0003ILQG\u0003WKH\u0003VHFRQG\u0003NH\\FKDLQ\u0003\u000b\u0015\f\u0003ODWHU\u0003LQ\u0003GUDZHU\u0003\u0017\u0011@\n$FW\u0003\u0014\u001b\u001d\u0003WDNH\u0003NH\\FKDLQ\u0003\u0016\u0003IURP\u0003GUDZHU\u0003\u0017\u0003\n2EV\u0003\u0014\u001b\u001d\u0003<RX\u0003SLFN\u0003XS\u0003WKH\u0003NH\\FKDLQ\u0003\u0016\u0003IURP\u0003WKH\u0003GUDZHU\u0003\u0017\u0011\u0003\n\u0011\u0011\u0011\u0011\u0011\u0011\n$FW\u0003\u0015\u0015\u001d\u0003SXW\u0003NH\\FKDLQ\u0003\u0016\u0003LQ\u0012RQ\u0003VDIH\u0003\u0014\u0003\n2EV\u0003\u0015\u0015\u001d\u0003<RX\u0003SXW\u0003WKH\u0003NH\\FKDLQ\u0003\u0016\u0003LQ\u0012RQ\u0003WKH\u0003VDIH\u0003\u0014\u0011\u0003\n$FW\u0003\u0015\u0016\u001d\u00037KLQN\u0455>1RZ\u0003,\u0003SXW\u0003WKH\u0003ILUVW\u0003NH\\FKDLQ\u0003LQ\u0003VDIH\u0011\u00031H[W\u000f\u0003,\u0003QHHG\u0003WR\u0003ILQG\u0003\nWKH\u0003VHFRQG\u0003NH\\FKDLQ\u000f\u0003ZKLFK\u0003LV\u0003PRUH\u0003OLNHO\\\u0003WR\u0003DSSHDU\u0003LQ\u0003GUHVVHU\u0003\u000b\u0014\f\u000f\u0003\nJDUEDJHFDQ\u0003\u000b\u0014\f\u000f\u0003VDIH\u0003\u000b\u0014\f\u000f\u0003VLGHWDEOH\u0003\u000b\u0014\f\u000f\u0003VRID\u0003\u000b\u0014\f\u000f\u0003VKHOI\u0003\u000b\u0014\u0010\u0014\u0015\f\u0011@\n$FW\u0003\u0015\u0017\u001d\u0003JR\u0003WR\u0003GUHVVHU\u0003\u0014\u0003\n2EV\u0003\u0015\u0017\u001d\u00032Q\u0003WKH\u0003GUHVVHU\u0003\u0014\u000f\u0003\\RX\u0003VHH\u0003D\u0003ER[\u0003\u0014\u000f\u0003D\u0003FUHGLWFDUG\u0003\u0014\u000f\u0003D\u0003NH\\FKDLQ\u0003\u0015\u000f\u0003D\u0003\nNH\\FKDLQ\u0003\u0014\u000f\u0003D\u0003ODSWRS\u0003\u0014\u0011\u0011\u0011\n$FW\u0003\u0015\u0018\u001d\u00037KLQN\u0455>1RZ\u0003,", "mimetype": "text/plain", "start_char_idx": 1028, "end_char_idx": 1647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fbb1a6d5-e54c-44ee-8c63-a39c78ea30f1": {"__data__": {"id_": "fbb1a6d5-e54c-44ee-8c63-a39c78ea30f1", "embedding": null, "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e12086a-da1a-45f7-a4a2-61acfe5763db", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "45539aa54f34b0bda4ac9202227b7645060841a611897a1be95e0f4e1eaae69b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e24752aa-02a9-4e8c-919b-8ff8784d527e", "node_type": "1", "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "c00fc0bc922ccf483c904f9f554e2f72e400bae54e1e5f057569c92d9ffa72ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ebdecee-617c-465b-a07a-deb11920bd47", "node_type": "1", "metadata": {}, "hash": "351e4308e6f3d57f5f22c4d87a5760ece767a1d5744311244ce59382de75d23c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u0003ILQG\u0003WKH\u0003VHFRQG\u0003NH\\FKDLQ\u0003\u000b\u0015\f\u0011\u00031H[W\u000f\u0003,\u0003QHHG\u0003WR\u0003WDNH\u0003\nLW\u0011@\n$FW\u0003\u0015\u0019\u001d\u0003WDNH\u0003NH\\FKDLQ\u0003\u0015\u0003IURP\u0003GUHVVHU\u0003\u0014\u0003\n2EV\u0003\u0015\u0019\u001d\u0003<RX\u0003SLFN\u0003XS\u0003WKH\u0003NH\\FKDLQ\u0003\u0015\u0003IURP\u0003WKH\u0003GUHVVHU\u0003\u0014\u0011\u0003\n$FW\u0003\u0015\u001a\u001d\u00037KLQN\u0455>1RZ\u0003,\u0003WDNH\u0003WKH\u0003VHFRQG\u0003NH\\FKDLQ\u0003\u000b\u0015\f\u0011\u00031H[W\u000f\u0003,\u0003QHHG\u0003WR\u0003SXW\u0003LW\u0003\nLQ\u0012RQ\u0003VDIH\u0003\u0014\u0011@\n$FW\u0003\u0015\u001b\u001d\u0003JR\u0003WR\u0003VDIH\u0003\u0014\n2EV\u0003\u0015\u001b\u001d\u00037KH\u0003VDIH\u0003\u0014\u0003LV\u0003RSHQ\u0011\u0003,", "mimetype": "text/plain", "start_char_idx": 1647, "end_char_idx": 1941, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ebdecee-617c-465b-a07a-deb11920bd47": {"__data__": {"id_": "2ebdecee-617c-465b-a07a-deb11920bd47", "embedding": null, "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e12086a-da1a-45f7-a4a2-61acfe5763db", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "45539aa54f34b0bda4ac9202227b7645060841a611897a1be95e0f4e1eaae69b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbb1a6d5-e54c-44ee-8c63-a39c78ea30f1", "node_type": "1", "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "f9aebec557841641a0a47ccc74dadac5597d07ba58eaed71f90dcc68973028e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "125f3631-432f-43ad-b65a-49444f75201b", "node_type": "1", "metadata": {}, "hash": "29bb7617dfc530cdcf6046721c75abec023708e0d6ea051575db112183005ea4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Q\u0003LW\u000f\u0003\\RX\u0003VHH\u0003D\u0003NH\\FKDLQ\u0003\u0016\u000f\u0003D\u0003VWDWXH\u0003\u0015\u000f\u0003DQG\u0003D\u0003\nZDWFK\u0003\u0016\u0011\n$FW\u0003\u0015\u001c\u001d\u0003SXW\u0003NH\\FKDLQ\u0003\u0015\u0003LQ\u0012RQ\u0003VDIH\u0003\u0014\n2EV\u0003\u0015\u001c\u001d\u0003<RX\u0003SXW\u0003WKH\u0003NH\\FKDLQ\u0003\u0015\u0003LQ\u0012RQ\u0003WKH\u0003VDIH\u0003\u0014\u0011 \u04bc\n<RX\u0003DUH\u0003LQ\u0003WKH\u0003PLGGOH\u0003RI\u0003D\u0003URRP\u0011\u0003/RRNLQJ\u0003TXLFNO\\\u0003DURXQG\u0003\\RX\u000f\u0003\n\\RX\u0003VHH\u0003D\u0003DUPFKDLU\u0003\u0014\u000f\u0003D\u0003FDELQHW\u0003\u0017\u000f\u0003\u00aa\u0003D\u0003FDELQHW\u0003\u0014\u000f\u0003D\u0003GUDZHU\u0003\u0018\u000f\u0003\n\u00aa\u000f\u0003D\u0003GUDZHU\u0003\u0014\u000f\u0003D\u0003GUHVVHU\u0003\u0014\u000f\u0003D\u0003JDUEDJHFDQ\u0003\u0014\u000f\u0003D\u0003VDIH\u0003\u0014\u000f\u0003D\u0003VKHOI\u0003\n\u0014\u0015\u000f\u0003\u00aa\u000f\u0003D\u0003VKHOI\u0003\u0014\u000f\u0003D\u0003VLGHWDEOH\u0003\u0014\u000f\u0003DQG\u0003D\u0003VRID\u0003\u0014\u0011\u0003\n<RXU\u0003WDVN\u0003LV\u0003WR\u001d\u0003SXW\u0003WZR\u0003NH\\FKDLQ\u0003LQ\u0003VDIH\u0011\n$OI:RUOG\nFigure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct\ntrajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts\n(Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds.\nis dif\ufb01cult for Act and previous RL methods, as a human cannot change the model parameters, and\nchanging a few actions might not edit the rest of the model behavior. This paradigm is also more than\nhuman dialogue to update the goal or subgoal as in Huang et al.", "mimetype": "text/plain", "start_char_idx": 1941, "end_char_idx": 2923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "125f3631-432f-43ad-b65a-49444f75201b": {"__data__": {"id_": "125f3631-432f-43ad-b65a-49444f75201b", "embedding": null, "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e12086a-da1a-45f7-a4a2-61acfe5763db", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "45539aa54f34b0bda4ac9202227b7645060841a611897a1be95e0f4e1eaae69b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ebdecee-617c-465b-a07a-deb11920bd47", "node_type": "1", "metadata": {"page_label": "15", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "60b2638c57e2b2fa754e89fed95a662ed5a1230928fa92076a14ab707c3634e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2022b) \u2014 while editing ReAct\nthoughts can do these, it can also modify the model\u2019s internal belief, reasoning styles, or anything the\n\ufb02exible thought space supports, for better task solving. We believe this is an exciting direction for\nhuman alignment and leave more systematic study as future work.\nB E XPERIMENT DETAILS\nB.1 H OTPOT QA F INETUNING DETAILS\nFor all \ufb01netuning we use a batch size of 64. On PaLM-8B, we \ufb01netune ReAct and Act methods\nfor 4,000 steps and Standard and CoT methods for 2,000 steps. On PaLM-62B, we \ufb01netune\nReAct and Act methods for 4,000 steps and Standard and CoT methods for 1,000 steps. We\n\ufb01nd ReAct and Act methods generally bene\ufb01t from more training steps (and more training data),\nwhile Standard and CoT methods degrade soon after \ufb01netuning.\nB.2 A LFWORLD IM-S TYLE DETAILS\nFor the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense\nexternal feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1)\ndecomposing the current goal and (2) the current subgoal that needs to be completed. In particular,\nReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the\nnext subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify\nwhere items can be within the environment.\n15", "mimetype": "text/plain", "start_char_idx": 2924, "end_char_idx": 4269, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6076b0eb-bb62-4e5f-bc01-16e2233317e9": {"__data__": {"id_": "6076b0eb-bb62-4e5f-bc01-16e2233317e9", "embedding": null, "metadata": {"page_label": "16", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5d884e3-745e-43d0-a430-d6f985ee2b15", "node_type": "4", "metadata": {"page_label": "16", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "3547e0b2e1992fe1c1b00ecc7962a099e458dba796fafb0c70c9be5d536c7b10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f32e4167-2373-4926-b914-fe6f93b3fbed", "node_type": "1", "metadata": {}, "hash": "b33a06df4d0ff19e51de286fc45c6b21d452beb6fe148a90609292cd8cb211e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nC P ROMPTS\nC.1 H OTPOT QA\nHotpot QA Prompts\nOriginal\nQuestion What is the elevation range for the area that the eastern sector of the\nColorado orogeny extends into?\nAnswer 1,800 to 7,000 ft\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\ncharacter Milhouse, who Matt Groening named after who?\nAnswer Richard Nixon\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\nSaimaa Gesture?\nAnswer The Saimaa Gesture\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\nAnswer director, screenwriter, actor\nQuestion Which magazine was started first Arthur\u2019s Magazine or First for Women?\nAnswer Arthur\u2019s Magazine\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\nAnswer Yes\nAct\nQuestion What is the elevation range for the area that the eastern sector of the\nColorado orogeny extends into?\nAction 1 Search[Colorado orogeny]\nObservation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in\nColorado and surrounding areas.\nAction 2 Lookup[eastern sector]\nObservation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called\nthe Central Plains orogeny.\nAction 3 Search[High Plains]\nObservation 3 High Plains refers to one of two distinct land regions:\nAction 4 Search[High Plains (United States)]\nObservation 4 The High Plains are a subregion of the Great Plains. From east to west, the\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\nm).[3]\nAction 5 Finish[1,800 to 7,000 ft]\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\ncharacter Milhouse, who Matt Groening named after who?\nAction 1 Search[Milhouse]\nObservation 1 Milhouse Mussolini Van Houten is a recurring character in the Fox animated\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\nGroening.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1930, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f32e4167-2373-4926-b914-fe6f93b3fbed": {"__data__": {"id_": "f32e4167-2373-4926-b914-fe6f93b3fbed", "embedding": null, "metadata": {"page_label": "16", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5d884e3-745e-43d0-a430-d6f985ee2b15", "node_type": "4", "metadata": {"page_label": "16", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "3547e0b2e1992fe1c1b00ecc7962a099e458dba796fafb0c70c9be5d536c7b10", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6076b0eb-bb62-4e5f-bc01-16e2233317e9", "node_type": "1", "metadata": {"page_label": "16", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "fb1143783148ce8a96944778e311f07a021d36ca67178ec22a8aa2a85dd101b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Action 2 Lookup[named after]\nObservation 2 (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose\nmiddle name was Milhous.\nAction 3 Finish[Richard Nixon]\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\nSaimaa Gesture?\nAction 1 Search[Adam Clayton Powell]\nObservation 1 Could not find [Adam Clayton Powell]. Similar: [\u2019Adam Clayton Powell\nIII\u2019, \u2019Seventh Avenue (Manhattan)\u2019, \u2019Adam Clayton Powell Jr. State Office\nBuilding\u2019, \u2019Isabel Washington Powell\u2019, \u2019Adam Powell\u2019, \u2019Adam Clayton Powell\n(film)\u2019, \u2019Giancarlo Esposito\u2019].\nAction 2 Search[Adam Clayton Powell (film)]\nObservation 2 Adam Clayton Powell is a 1989 American documentary film directed by\nRichard Kilberg. The film is about the rise and fall of influential\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\nas part of the PBS series The American Experience.\nAction 3 Finish[The Saimaa Gesture]\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\nAction 1 Search[Nicholas Ray]\nObservation 1 Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\n1979) was an American film director, screenwriter, and actor best known for\nthe 1955 film Rebel Without a Cause.\nAction 2 Search[Elia Kazan]\nObservation 2 Elia Kazan was an American film and theatre director, producer, screenwriter\nand actor.\nContinued on next page\n16", "mimetype": "text/plain", "start_char_idx": 1931, "end_char_idx": 3326, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b779898c-20aa-4835-beed-ab538e96b831": {"__data__": {"id_": "b779898c-20aa-4835-beed-ab538e96b831", "embedding": null, "metadata": {"page_label": "17", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e817b24-c86c-41c1-bc96-15c2bd748942", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5a5d19101cbf0c3fb6dd981fd455b14d8ab0f902204fd02c834c61b2e73de7d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc507c24-d73e-4f0e-a68c-ffe149806176", "node_type": "1", "metadata": {}, "hash": "4e0b623bef4bf89cf057cb141e7252eb97ef733b7a4f99ea8f2a2229fe15ea1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nHotpot QA Prompts \u2013 Continued from previous page\nAction 3 Finish[director, screenwriter, actor]\nQuestion Which magazine was started first Arthur\u2019s Magazine or First for Women?\nAction 1 Search[Arthur\u2019s Magazine]\nObservation 1 Arthur\u2019s Magazine (1844-1846) was an American literary periodical published\nin Philadelphia in the 19th century.\nAction 2 Search[First for Women]\nObservation 2 First for Women is a woman\u2019s magazine published by Bauer Media Group in the\nUSA.[1] The magazine was started in 1989.\nAction 3 Finish[Arthur\u2019s Magazine]\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\nAction 1 Search[Pavel Urysohn]\nObservation 1 Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet\nmathematician who is best known for his contributions in dimension theory.\nAction 2 Search[Leonid Levin]\nObservation 2 Leonid Anatolievich Levin is a Soviet-American mathematician and computer\nscientist.\nAction 3 Finish[yes]\nCoT\nQuestion What is the elevation range for the area that the eastern sector of the\nColorado orogeny extends into?\nThought Let\u2019s think step by step. The eastern sector of Colorado orogeny extends\ninto the High Plains. High Plains rise in elevation from around 1,800 to\n7,000 ft, so the answer is 1,800 to 7,000 ft.\nAnswer 1,800 to 7,000 ft\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\ncharacter Milhouse, who Matt Groening named after who?\nThought Let\u2019s think step by step. Milhouse was named after U.S. president Richard\nNixon, so the answer is Richard Nixon.\nAnswer Richard Nixon\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\nSaimaa Gesture?\nThought Let\u2019s think step by step. Adam Clayton Powell (film) is a documentary about\nan African-American politician, not Finnish rock groups. So the documentary\nabout Finnish rock groups must instead be The Saimaa Gesture.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1948, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc507c24-d73e-4f0e-a68c-ffe149806176": {"__data__": {"id_": "cc507c24-d73e-4f0e-a68c-ffe149806176", "embedding": null, "metadata": {"page_label": "17", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e817b24-c86c-41c1-bc96-15c2bd748942", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5a5d19101cbf0c3fb6dd981fd455b14d8ab0f902204fd02c834c61b2e73de7d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b779898c-20aa-4835-beed-ab538e96b831", "node_type": "1", "metadata": {"page_label": "17", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "1a0699c63ee1627b3e282efeceda6916557eaf367059ac06c3ce9b8ba252cc06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65e6d4fa-b0e7-4caf-b68a-040933c97eeb", "node_type": "1", "metadata": {}, "hash": "1b74c399c6a2b8a0e09ec2cfa6b08120e76033a91ab14ae132cfb61304dcd65b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So the documentary\nabout Finnish rock groups must instead be The Saimaa Gesture.\nAnswer The Saimaa Gesture\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\nThought Let\u2019s think step by step. Professions of Nicholas Ray are director,\nscreenwriter, and actor. Professions of Elia Kazan are director, producer,\nscreenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in\ncommon is director, screenwriter, and actor.\nAnswer director, screenwriter, actor\nQuestion Which magazine was started first Arthur\u2019s Magazine or First for Women?\nThought Let\u2019s think step by step. Arthur\u2019s Magazine was started in 1844. First\nfor Women was started in 1989. 1844 (Arthur\u2019s Magazine) < 1989 (First for\nWomen), so Arthur\u2019s Magazine was started first.\nAnswer Arthur\u2019s Magazine\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\nThought Let\u2019s think step by step. Pavel Urysohn is a mathematician. Leonid Levin\nis a mathematician and computer scientist. So Pavel Urysohn and Leonid\nLevin have the same type of work.\nAnswer Yes\nReAct\nQuestion What is the elevation range for the area that the eastern sector of the\nColorado orogeny extends into?\nThought 1 I need to search Colorado orogeny, find the area that the eastern sector\nof the Colorado orogeny extends into, then find the elevation range of the\narea.\nAction 1 Search[Colorado orogeny]\nObservation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in\nColorado and surrounding areas.\nThought 2 It does not mention the eastern sector. So I need to look up eastern\nsector.\nAction 2 Lookup[eastern sector]\nObservation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called\nthe Central Plains orogeny.\nThought 3 The eastern sector of Colorado orogeny extends into the High Plains. So I\nneed to search High Plains and find its elevation range.", "mimetype": "text/plain", "start_char_idx": 1868, "end_char_idx": 3751, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65e6d4fa-b0e7-4caf-b68a-040933c97eeb": {"__data__": {"id_": "65e6d4fa-b0e7-4caf-b68a-040933c97eeb", "embedding": null, "metadata": {"page_label": "17", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e817b24-c86c-41c1-bc96-15c2bd748942", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5a5d19101cbf0c3fb6dd981fd455b14d8ab0f902204fd02c834c61b2e73de7d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc507c24-d73e-4f0e-a68c-ffe149806176", "node_type": "1", "metadata": {"page_label": "17", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "18c149660f5f6eeccc24754efeae6f623645a36307330a0d26d202f53d0fb8c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So I\nneed to search High Plains and find its elevation range.\nAction 3 Search[High Plains]\nObservation 3 High Plains refers to one of two distinct land regions\nThought 4 I need to instead search High Plains (United States).\nAction 4 Search[High Plains (United States)]\nContinued on next page\n17", "mimetype": "text/plain", "start_char_idx": 3690, "end_char_idx": 3984, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30146c2c-f4cd-46f7-9b67-ae6f6b22eee2": {"__data__": {"id_": "30146c2c-f4cd-46f7-9b67-ae6f6b22eee2", "embedding": null, "metadata": {"page_label": "18", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b47afeb6-caa7-410b-ab54-6b7edc2a5571", "node_type": "4", "metadata": {"page_label": "18", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "232ee602112c03e8b4fcd727de86849423659efe9405a89f03f2f6a347a44cc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a0e7340-8945-4915-b980-3e6bf39a930b", "node_type": "1", "metadata": {}, "hash": "f1fffe08da897b29c32c559c4f737b044b3b2c148291589e4e2bc9c60df5d8a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nHotpot QA Prompts \u2013 Continued from previous page\nObservation 4 The High Plains are a subregion of the Great Plains. From east to west, the\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\nm).[3]\nThought 5 High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer\nis 1,800 to 7,000 ft.\nAction 5 Finish[1,800 to 7,000 ft]\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\ncharacter Milhouse, who Matt Groening named after who?\nThought 1 The question simplifies to \"The Simpsons\" character Milhouse is named after\nwho. I only need to search Milhouse and find who it is named after.\nAction 1 Search[Milhouse]\nObservation 1 Milhouse Mussolini Van Houten is a recurring character in the Fox animated\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\nGroening.\nThought 2 The paragraph does not tell who Milhouse is named after, maybe I can look up\n\"named after\".\nAction 2 Lookup[named after]\nObservation 2 (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose\nmiddle name was Milhous.\nThought 3 Milhouse was named after U.S. president Richard Nixon, so the answer is\nRichard Nixon.\nAction 3 Finish[Richard Nixon]\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\nSaimaa Gesture?\nThought 1 I need to search Adam Clayton Powell and The Saimaa Gesture, and find which\ndocumentary is about Finnish rock groups.\nAction 1 Search[Adam Clayton Powell]\nObservation 1 Could not find [Adam Clayton Powell]. Similar: [\u2019Adam Clayton Powell\nIII\u2019, \u2019Seventh Avenue (Manhattan)\u2019, \u2019Adam Clayton Powell Jr. State Office\nBuilding\u2019, \u2019Isabel Washington Powell\u2019, \u2019Adam Powell\u2019, \u2019Adam Clayton Powell\n(film)\u2019, \u2019Giancarlo Esposito\u2019].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a0e7340-8945-4915-b980-3e6bf39a930b": {"__data__": {"id_": "8a0e7340-8945-4915-b980-3e6bf39a930b", "embedding": null, "metadata": {"page_label": "18", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b47afeb6-caa7-410b-ab54-6b7edc2a5571", "node_type": "4", "metadata": {"page_label": "18", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "232ee602112c03e8b4fcd727de86849423659efe9405a89f03f2f6a347a44cc6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30146c2c-f4cd-46f7-9b67-ae6f6b22eee2", "node_type": "1", "metadata": {"page_label": "18", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "d2eab1926f5e418c496f25729a025341d6d31479efa005f16a0c050654da8742", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "892e5084-a8b3-47e2-bdb6-99918e767acc", "node_type": "1", "metadata": {}, "hash": "ae652a13e63f4a54f7751c5eb3c4383076f4428fd3fe6f6c058711de724b3d95", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thought 2 To find the documentary, I can search Adam Clayton Powell (film).\nAction 2 Search[Adam Clayton Powell (film)]\nObservation 2 Adam Clayton Powell is a 1989 American documentary film directed by\nRichard Kilberg. The film is about the rise and fall of influential\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\nas part of the PBS series The American Experience.\nThought 3 Adam Clayton Powell (film) is a documentary about an African-American\npolitician, not Finnish rock groups. So the documentary about Finnish rock\ngroups must instead be The Saimaa Gesture.\nAction 3 Finish[The Saimaa Gesture]\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\nThought 1 I need to search Nicholas Ray and Elia Kazan, find their professions, then\nfind the profession they have in common.\nAction 1 Search[Nicholas Ray]\nObservation 1 Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\n1979) was an American film director, screenwriter, and actor best known for\nthe 1955 film Rebel Without a Cause.\nThought 2 Professions of Nicholas Ray are director, screenwriter, and actor. I need\nto search Elia Kazan next and find his professions.\nAction 2 Search[Elia Kazan]\nObservation 2 Elia Kazan was an American film and theatre director, producer, screenwriter\nand actor.\nThought 3 Professions of Elia Kazan are director, producer, screenwriter, and actor.\nSo profession Nicholas Ray and Elia Kazan have in common is director,\nscreenwriter, and actor.\nAction 3 Finish[director, screenwriter, actor]\nQuestion Which magazine was started first Arthur\u2019s Magazine or First for Women?\nThought 1 I need to search Arthur\u2019s Magazine and First for Women, and find which was\nstarted first.\nAction 1 Search[Arthur\u2019s Magazine]\nObservation 1 Arthur\u2019s Magazine (1844-1846) was an American literary periodical published\nin Philadelphia in the 19th century.\nThought 2 Arthur\u2019s Magazine was started in 1844. I need to search First for Women\nnext.", "mimetype": "text/plain", "start_char_idx": 1801, "end_char_idx": 3787, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "892e5084-a8b3-47e2-bdb6-99918e767acc": {"__data__": {"id_": "892e5084-a8b3-47e2-bdb6-99918e767acc", "embedding": null, "metadata": {"page_label": "18", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b47afeb6-caa7-410b-ab54-6b7edc2a5571", "node_type": "4", "metadata": {"page_label": "18", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "232ee602112c03e8b4fcd727de86849423659efe9405a89f03f2f6a347a44cc6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a0e7340-8945-4915-b980-3e6bf39a930b", "node_type": "1", "metadata": {"page_label": "18", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "43cafa6f898fb34698821bdd89da419c4d66de151fbd1c0262f82bdd6133dacd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I need to search First for Women\nnext.\nAction 2 Search[First for Women]\nObservation 2 First for Women is a woman\u2019s magazine published by Bauer Media Group in the\nUSA.[1] The magazine was started in 1989.\nThought 3 First for Women was started in 1989. 1844 (Arthur\u2019s Magazine) < 1989 (First\nfor Women), so Arthur\u2019s Magazine was started first.\nAction 3 Finish[Arthur\u2019s Magazine]\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\nThought 1 I need to search Pavel Urysohn and Leonid Levin, find their types of work,\nthen find if they are the same.\nAction 1 Search[Pavel Urysohn]\nContinued on next page\n18", "mimetype": "text/plain", "start_char_idx": 3749, "end_char_idx": 4377, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2467112-7946-45a5-aa5e-248b162cc546": {"__data__": {"id_": "a2467112-7946-45a5-aa5e-248b162cc546", "embedding": null, "metadata": {"page_label": "19", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6540d239-e9b8-4be6-9477-7c22b6f40b05", "node_type": "4", "metadata": {"page_label": "19", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "2093335f16a6b5b0c31c56d9f3e67753e73799d787a9e0a5a5f7f3550cf32fec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nHotpot QA Prompts \u2013 Continued from previous page\nObservation 1 Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet\nmathematician who is best known for his contributions in dimension theory.\nThought 2 Pavel Urysohn is a mathematician. I need to search Leonid Levin next and\nfind its type of work.\nAction 2 Search[Leonid Levin]\nObservation 2 Leonid Anatolievich Levin is a Soviet-American mathematician and computer\nscientist.\nThought 3 Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn\nand Leonid Levin have the same type of work.\nAction 3 Finish[yes]\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee6372f7-f846-4363-b88a-b87a2aaae5c0": {"__data__": {"id_": "ee6372f7-f846-4363-b88a-b87a2aaae5c0", "embedding": null, "metadata": {"page_label": "20", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e9928d3-6a8a-4719-bdcf-e1f85560b13e", "node_type": "4", "metadata": {"page_label": "20", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "fc04ffb97ab35f3ef70346e15ddfc63cc5a3ae6b973ac3b26c7e46de9a04b818", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5e5c86e-07d4-4867-baf1-de1868428f42", "node_type": "1", "metadata": {}, "hash": "154f523f52142ec41fb462eafe913555d9158876bbe8fb90edeb7110d6c17dd3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nC.2 F EVER\nFEVER Prompts\nOriginal Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\nthere is NOT ENOUGH INFORMATION.\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\nAnswer SUPPORTS\nClaim Stranger Things is set in Bloomington, Indiana.\nAnswer REFUTES\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\nAnswer NOT ENOUGH INFO\nAct Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\nthere is NOT ENOUGH INFORMATION.\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\nAction2 Search[Nikolaj Coster-Waldau]\nObservation 1 Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and\nproducer. He graduated from the Danish National School of Performing Arts\nin Copenhagen in 1993,[1] and had his breakthrough role in Denmark with\nthe film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy\ndrama series Game of Thrones, for which he received two Primetime Emmy\nAward nominations for Outstanding Supporting Actor in a Drama Series..\nCoster-Waldau has appeared in numerous films in his native Denmark and\nScandinavia, including Headhunters (2011) and A Thousand Times Good Night\n(2013). In the U.S, his debut film role was in the war film Black Hawk Down\n(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a\ndetective in the short-lived Fox television series New Amsterdam (2008), and\nappeared in the 2009 Fox television film Virtuality, originally intended as\na pilot.\nAction 2 Finish[SUPPORTS]\nClaim Stranger Things is set in Bloomington, Indiana.\nAction 1 Search[Stranger Things]\nObservation 1 Stranger Things is an American science fiction horror drama television\nseries created by the Duffer Brothers. Set in the 1980s, primarily in\nthe fictional town of Hawkins, Indiana, the series centers on a number of\nmysteries and supernatural events occurring around the town and their impact\non an ensemble of child and adult characters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2020, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a5e5c86e-07d4-4867-baf1-de1868428f42": {"__data__": {"id_": "a5e5c86e-07d4-4867-baf1-de1868428f42", "embedding": null, "metadata": {"page_label": "20", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e9928d3-6a8a-4719-bdcf-e1f85560b13e", "node_type": "4", "metadata": {"page_label": "20", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "fc04ffb97ab35f3ef70346e15ddfc63cc5a3ae6b973ac3b26c7e46de9a04b818", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee6372f7-f846-4363-b88a-b87a2aaae5c0", "node_type": "1", "metadata": {"page_label": "20", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "6943fd96ff27a2f5112b912a43df4e52b47c5ef8d5fb8382b08d911c24f329ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Action 2 Finish[REFUTES]\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\nAction 1 Search[Beautiful]\nObservation 1 Could not find [Beautiful]. Similar: [\u2019Beautiful\u2019, \u2019Beautiful, Beautiful\u2019,\n\u2019A Beautiful Mind (film)\u2019, \u2019Beautiful (Christina Aguilera song)\u2019, \u2019Life Is\nBeautiful\u2019].\nAction 2 Search[Beautiful (Christina Aguilera song)\nObservation 2 \"Beautiful\" is a song recorded by American singer Christina Aguilera for her\nfourth studio album, Stripped (2002).\nAction 3 Lookup[Billboard Hot 100]\nObservation 3 (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the\nUnited States, where it was certified Gold for 500,000 units shipped.\nAction 4 Finish[NOT ENOUGH INFO]\nCoT Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\nthere is NOT ENOUGH INFORMATION.\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\nThought Nikolaj William Coster-Waldau appeared in the 2009 Fox television film\nVirtuality, so he has worked with the Fox Broadcasting Company.\nAnswer SUPPORTS\nClaim Stranger Things is set in Bloomington, Indiana.\nThought Stranger Things is in the fictional town of Hawkins, Indiana, not in\nBloomington, Indiana.\nAnswer REFUTES\nContinued on next page\n20", "mimetype": "text/plain", "start_char_idx": 2021, "end_char_idx": 3262, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de4269f2-1ff5-46f8-b0e8-c293802a3d5f": {"__data__": {"id_": "de4269f2-1ff5-46f8-b0e8-c293802a3d5f", "embedding": null, "metadata": {"page_label": "21", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fd83d10-afc0-4ab9-8da9-a2babf1b04fa", "node_type": "4", "metadata": {"page_label": "21", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "86418691bb4a64404112e853a070aa0dda8753d31beedf110b3a4038f3599f1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7e4a0db-c132-4cf1-a17b-1b49a202d6b0", "node_type": "1", "metadata": {}, "hash": "70d208bb1513a7c4dedd26a25f2b9d68fd367d1f55f00ff5b2f965c2a5f27de2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nFever Prompts \u2013 Continued from previous page\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\nThought The song peaked at number two on the Billboard Hot 100 in the United States,\nbut not sure if it was in 2003.\nAnswer NOT ENOUGH INFO\nReAct Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\nthere is NOT ENOUGH INFORMATION.\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\nThought 1 I need to search Nikolaj Coster-Waldau and find if he has worked with the\nFox Broadcasting Company.\nAction 1 Search[Nikolaj Coster-Waldau]\nObservation 1 Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and\nproducer. He graduated from the Danish National School of Performing Arts\nin Copenhagen in 1993,[1] and had his breakthrough role in Denmark with\nthe film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy\ndrama series Game of Thrones, for which he received two Primetime Emmy\nAward nominations for Outstanding Supporting Actor in a Drama Series..\nCoster-Waldau has appeared in numerous films in his native Denmark and\nScandinavia, including Headhunters (2011) and A Thousand Times Good Night\n(2013). In the U.S, his debut film role was in the war film Black Hawk Down\n(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a\ndetective in the short-lived Fox television series New Amsterdam (2008), and\nappeared in the 2009 Fox television film Virtuality, originally intended as\na pilot.\nThought 2 Because he \"appeared in the 2009 Fox television film Virtuality\", he should\nhave worked with the Fox Broadcasting Company.\nAction 2 Finish[SUPPORTS]\nClaim Stranger Things is set in Bloomington, Indiana.\nThought 1 I should search for Stranger Things, and see if it is set in Bloomington,\nIndiana.\nAction 1 Search[Stranger Things]\nObservation 1 Stranger Things is an American science fiction horror drama television\nseries created by the Duffer Brothers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1997, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7e4a0db-c132-4cf1-a17b-1b49a202d6b0": {"__data__": {"id_": "a7e4a0db-c132-4cf1-a17b-1b49a202d6b0", "embedding": null, "metadata": {"page_label": "21", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fd83d10-afc0-4ab9-8da9-a2babf1b04fa", "node_type": "4", "metadata": {"page_label": "21", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "86418691bb4a64404112e853a070aa0dda8753d31beedf110b3a4038f3599f1b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de4269f2-1ff5-46f8-b0e8-c293802a3d5f", "node_type": "1", "metadata": {"page_label": "21", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "9012b839c1964e06ed483ca4e15860a6ec6fab68e1f11a72b046a0ce7dc7c8f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Set in the 1980s, primarily in\nthe fictional town of Hawkins, Indiana, the series centers on a number of\nmysteries and supernatural events occurring around the town and their impact\non an ensemble of child and adult characters.\nThought 2 The observation says that it is set in a \"fictional town of Hawkins,\nIndiana\", so it is not set in Bloomington.\nAction 2 Finish[REFUTES]\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\nThought 1 I need to search the song Beautiful and find if it reached number two on the\nBillboard Hot 100 in 2003.\nAction 1 Search[Beautiful]\nObservation 1 Could not find [Beautiful]. Similar: [\u2019Beautiful\u2019, \u2019Beautiful, Beautiful\u2019,\n\u2019A Beautiful Mind (film)\u2019, \u2019Beautiful (Christina Aguilera song)\u2019, \u2019Life Is\nBeautiful\u2019].\nThought 2 From suggestions, I should search \"Beautiful (Christina Aguilera song)\" to\nfind the song.\nAction 2 Search[Beautiful (Christina Aguilera song)]\nObservation 2 \"Beautiful\" is a song recorded by American singer Christina Aguilera for her\nfourth studio album, Stripped (2002).\nThought 3 It does not mention Billboard, so I need to look up \"Billboard Hot 100\" to\nfind if it reached number two on it in 2003.\nAction 3 Lookup[Billboard Hot 100]\nObservation 3 (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the\nUnited States, where it was certified Gold for 500,000 units shipped.\nThought 4 It only says the song peaked at number two on the Billboard Hot 100, but not\nif it was in 2003. I am not sure if this claim is true or not.\nAction 4 Finish[NOT ENOUGH INFO]\n21", "mimetype": "text/plain", "start_char_idx": 1998, "end_char_idx": 3553, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70ca7e62-0f41-4481-b881-29dbf8fa668a": {"__data__": {"id_": "70ca7e62-0f41-4481-b881-29dbf8fa668a", "embedding": null, "metadata": {"page_label": "22", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df4c8a79-9c9e-414e-83f1-2c0133ca69d7", "node_type": "4", "metadata": {"page_label": "22", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "87d66d35ca66ae8e54c553f71b66d4ec5afedcac8fac6dd6fa19351d6735b69e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1efaf9f5-0d96-438b-bb38-554f9275dab4", "node_type": "1", "metadata": {}, "hash": "0d7ad51bd29cd02f400f28d8f0a8bd00f9d914ecf2b7e6f4667be4ccae596931", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nC.3 W EBSHOP\nTable 6: Prompts used for the Webshop experiments in Section 4. We use the prompt on the left for\nAct, and use the prompt on the right (add reasoning) for ReAct.\nInstruction: i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars\nAct ReAct\nAction: search\n[\n3 ounce bright citrus deodorant sensitive skin\n]\nObservation:[\nBack to Search\n]\nPage 1 (Total results: 50)[\nNext\n]\n[\nB078GWRC1J\n]\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99[\nB078GTKVXY\n]\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99[\nB08KBVJ4XN\n]\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men\n, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar\n& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,\n2.7 oz, 2-Pack)\n$15.95\nAction: click\n[\nB078GWRC1J\n]\nObservation:[\nBack to Search\n]\n[\nPrev\n]\nscent [assorted scents][bright citrus][calming lavender][ginger\n[fresh] simply non-scents]\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2\n-pack)]\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\nPrice: $10.99\nRating: N.A.\n[Description]\n[Features]\n[Reviews]\n[Buy Now]\nAction: click[bright citrus]\nObservation: You have clicked bright citrus.\nAction: click[3 ounce (pack of 1)]\nObservation: You have clicked 3 ounce (pack of 1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1627, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1efaf9f5-0d96-438b-bb38-554f9275dab4": {"__data__": {"id_": "1efaf9f5-0d96-438b-bb38-554f9275dab4", "embedding": null, "metadata": {"page_label": "22", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df4c8a79-9c9e-414e-83f1-2c0133ca69d7", "node_type": "4", "metadata": {"page_label": "22", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "87d66d35ca66ae8e54c553f71b66d4ec5afedcac8fac6dd6fa19351d6735b69e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70ca7e62-0f41-4481-b881-29dbf8fa668a", "node_type": "1", "metadata": {"page_label": "22", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "9e4bbe54dcdc073b5b05ca3eaca9d5ebd5b87c177aef9692d20d4afff951c12e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46f985eb-eda5-46c7-8e45-444ca4bc37b6", "node_type": "1", "metadata": {}, "hash": "2e28dd53ef6a0468158763d77025b265f175318aaa5054c3668f4c8ea0fd98b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Action: click[Buy Now]\nAction: search\n[\n3 ounce bright citrus deodorant sensitive skin\nObservation:[\nBack to Search\n]\nPage 1 (Total results: 50)[\nNext\n]\n[\nB078GWRC1J\n]\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99[\nB078GTKVXY\n]\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99[\nB08KBVJ4XN\n]\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men\n, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar\n& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,\n2.7 oz, 2-Pack)\n$15.95\nAction: think[B078GWRC1J and B078GTKVXY are bright citrus\ndeodorant less then 50 dollars. I can check B078GWRC1J \ufb01rst.\nObservation: OK.\nAction: click\n[\nB078GWRC1J\n]\nObservation:[\nBack to Search\n]\n[\nPrev\n]\nscent [assorted scents][bright citrus][calming lavender][ginger\n[fresh] simply non-scents]\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2\n-pack)]\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\nPrice: $10.99\nRating: N.A.\n[Description]\n[Features]\n[Reviews]\n[Buy Now]\nAction: think[For 3 ounce bottle of bright citrus deodorant\nfor sensitive skin, the item has options \u2019bright citrus\u2019\nand \u20193 ounce (pack of 1)\u2019 and seems good to buy.]\nObservation: OK.\nAction: click[bright citrus]\nObservation: You have clicked bright citrus.\nAction: click[3 ounce (pack of 1)]\nObservation: You have clicked 3 ounce (pack of 1).", "mimetype": "text/plain", "start_char_idx": 1628, "end_char_idx": 3241, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46f985eb-eda5-46c7-8e45-444ca4bc37b6": {"__data__": {"id_": "46f985eb-eda5-46c7-8e45-444ca4bc37b6", "embedding": null, "metadata": {"page_label": "22", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df4c8a79-9c9e-414e-83f1-2c0133ca69d7", "node_type": "4", "metadata": {"page_label": "22", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "87d66d35ca66ae8e54c553f71b66d4ec5afedcac8fac6dd6fa19351d6735b69e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1efaf9f5-0d96-438b-bb38-554f9275dab4", "node_type": "1", "metadata": {"page_label": "22", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4b72b1ff9caf435da57197038637ccf37c482fdbd7d7e7815705b338e61ce83b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Action: click[Buy Now]\n22", "mimetype": "text/plain", "start_char_idx": 3242, "end_char_idx": 3267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e8210cf-2387-4d76-a7b5-988a1a6cbb6d": {"__data__": {"id_": "6e8210cf-2387-4d76-a7b5-988a1a6cbb6d", "embedding": null, "metadata": {"page_label": "23", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75de8181-2894-433e-97ab-01c1f0071eca", "node_type": "4", "metadata": {"page_label": "23", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "ab7e949c11b583951aba18447ecf6abd5b6463778b2598e8e0d065fb78518740", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9efea2b-46de-49fe-9bd0-1815101b30be", "node_type": "1", "metadata": {}, "hash": "78655db96bca7f199e14275529c6a5cfa93f5d875092987750b22bcfed70947e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nC.4 ALFW ORLD\nTable 7: An Act prompt on the ALFWorld clean task. No thoughts are provided.\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a clean lettuce in diningtable.\n> go to fridge 1\nThe fridge 1 is closed.\n> open fridge 1\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n> go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n> take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n> go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1258, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9efea2b-46de-49fe-9bd0-1815101b30be": {"__data__": {"id_": "c9efea2b-46de-49fe-9bd0-1815101b30be", "embedding": null, "metadata": {"page_label": "23", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75de8181-2894-433e-97ab-01c1f0071eca", "node_type": "4", "metadata": {"page_label": "23", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "ab7e949c11b583951aba18447ecf6abd5b6463778b2598e8e0d065fb78518740", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e8210cf-2387-4d76-a7b5-988a1a6cbb6d", "node_type": "1", "metadata": {"page_label": "23", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "22ec621b32b8faf348179d614f402ffe2e214733b6f55138a377e4c484b915a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "> clean lettuce 1 with sinkbasin 1\nYou clean the lettuce 1 using the sinkbasin 1.\n> think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1.\nOK.\n> go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\nspatula 3, a tomato 2, and a tomato 1.\n> put lettuce 1 in/on diningtable 1\nYou put the lettuce 1 in/on the diningtable 1.\n23", "mimetype": "text/plain", "start_char_idx": 1259, "end_char_idx": 1739, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "289320ac-592b-474b-8e7f-6cec29db816d": {"__data__": {"id_": "289320ac-592b-474b-8e7f-6cec29db816d", "embedding": null, "metadata": {"page_label": "24", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8925ba19-3aed-42ac-9f53-68b59387da55", "node_type": "4", "metadata": {"page_label": "24", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "3df9de45b35f19ff611670a4bd1838d5a634b7094c73911026ff8a0b2bedbb92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32fdd2c6-5d40-4a8c-8683-613efa20df90", "node_type": "1", "metadata": {}, "hash": "f4fc602c656d0e355fe3e12fa2f384cc4729d9d28c452f6efa8cb220f8064b2e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nTable 8: An ReAct prompt on the ALFWorld clean task.\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a clean lettuce in diningtable.\n> think: To solve the task, I need to find and take a lettuce, then clean it with\nsinkbasin, then put it in diningtable.\nOK.\n> think: First I need to find a lettuce. A lettuce is more likely to appear in\nfridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13). I\ncan check one by one, starting with fridge 1.\nOK.\n> go to fridge 1\nThe fridge 1 is closed.\n> open fridge 1\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1130, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32fdd2c6-5d40-4a8c-8683-613efa20df90": {"__data__": {"id_": "32fdd2c6-5d40-4a8c-8683-613efa20df90", "embedding": null, "metadata": {"page_label": "24", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8925ba19-3aed-42ac-9f53-68b59387da55", "node_type": "4", "metadata": {"page_label": "24", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "3df9de45b35f19ff611670a4bd1838d5a634b7094c73911026ff8a0b2bedbb92", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "289320ac-592b-474b-8e7f-6cec29db816d", "node_type": "1", "metadata": {"page_label": "24", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4cb1bdbc90a9c810f029b8a6833974178d66397e5949b77c4d4747e4d31d00c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "> go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n> think: Now I find a lettuce (1). Next, I need to take it.\nOK.\n> take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n> think: Now I take a lettuce (1). Next, I need to go to sinkbasin (1) and clean\nit.\nOK.\n> go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n> clean lettuce 1 with sinkbasin 1\nYou clean the lettuce 1 using the sinkbasin 1.\n> think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1.\nOK.\n> go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\nspatula 3, a tomato 2, and a tomato 1.\n> put lettuce 1 in/on diningtable 1\nYou put the lettuce 1 in/on the diningtable 1.\n24", "mimetype": "text/plain", "start_char_idx": 1131, "end_char_idx": 2189, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4088ee3f-49e7-4cd8-8ba2-35c961116bb1": {"__data__": {"id_": "4088ee3f-49e7-4cd8-8ba2-35c961116bb1", "embedding": null, "metadata": {"page_label": "25", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e72937d4-f3d9-4e98-b516-d8e7d3952d0b", "node_type": "4", "metadata": {"page_label": "25", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4a41528c78a27cddc81a5ccb4b901a5b768ad407e95fd78fcddddd000a4bc090", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e511fbc-5a81-4644-b301-d501d7a53f17", "node_type": "1", "metadata": {}, "hash": "0150fd6453f70b88dfa5fff403174709a7b6d40cf1b341bff0fcf95bc279f4b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nTable 9: An ReAct-IM prompt on the ALFWorld clean task.\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a clean lettuce in diningtable.\n> think: To solve the task, I need to find and take a lettuce, then clean it with\nsinkbasin, then put it in diningtable.\nOK.\n> think: First I need to find a lettuce.\nOK.\n> go to fridge 1\nThe fridge 1 is closed.\n> think: First I need to find a lettuce.\nOK.\n> open fridge 1\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n> think: First I need to find a lettuce.\nOK.\n> go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n> think: First I need to find a lettuce.\nOK.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1345, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e511fbc-5a81-4644-b301-d501d7a53f17": {"__data__": {"id_": "2e511fbc-5a81-4644-b301-d501d7a53f17", "embedding": null, "metadata": {"page_label": "25", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e72937d4-f3d9-4e98-b516-d8e7d3952d0b", "node_type": "4", "metadata": {"page_label": "25", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4a41528c78a27cddc81a5ccb4b901a5b768ad407e95fd78fcddddd000a4bc090", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4088ee3f-49e7-4cd8-8ba2-35c961116bb1", "node_type": "1", "metadata": {"page_label": "25", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "01f0a78ed2023883251279e526714fe07500ce369cc319357e284831f1381c00", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "> think: First I need to find a lettuce.\nOK.\n> take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n> think: I need to clean this lettuce (1) using sinkbasin 1.\nOK.\n> go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n> think: I need to clean this lettuce (1) using sinkbasin 1.\nOK.\n> clean lettuce 1 with sinkbasin 1\nYou clean the lettuce 1 using the sinkbasin 1.\n> think: I need to put this cleaned lettuce (1) in/on diningtable 1.\nOK.\n> go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\nspatula 3, a tomato 2, and a tomato 1.\n> think: I need to put this cleaned lettuce (1) in/on diningtable 1.\nOK.\n> put lettuce 1 in/on diningtable 1\nYou put the lettuce 1 in/on the diningtable 1.\nD T RAJECTORIES\nD.1 FEVER T RAJECTORIES\nWe provide some trajectories produced by ReAct, CoTand Acton several randomly selected exam-\nples from the FEVER dev set below. Search results are omitted to space.\n25", "mimetype": "text/plain", "start_char_idx": 1301, "end_char_idx": 2406, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "641e96e8-b42f-40b1-9b65-77d69356bad8": {"__data__": {"id_": "641e96e8-b42f-40b1-9b65-77d69356bad8", "embedding": null, "metadata": {"page_label": "26", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0cffa38-b905-4946-bb97-d96a8f789334", "node_type": "4", "metadata": {"page_label": "26", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "39dca19b14e1c8b7d5b5c33732b06061ebe54b0203dd6aceb246d9bd9381b731", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3cad464-7862-4346-b54f-b50c14c485e9", "node_type": "1", "metadata": {}, "hash": "31fb8698f1e045e7314f2aed1c73db861bcbe80eb5b09963a318924d406c5c29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nExample 2491 (gt: REFUTES)\nReAct\nClaim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.\nThought 1: I need to search Bermuda Triangle and find if it is in the Pacific Ocean.\nAction 1: Search[Bermuda Triangle]\nObservation 1: Reportedly haunted locations...\nThought 2: The observation says that it is in the western part of the North Atlantic\nOcean, so it is not in the Pacific Ocean.\nAction 2: Finish[REFUTES]\nObservation 2: Episode finished\nAct\nClaim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.\nAction 1 : Search[Bermuda Triangle]\nObservation 1 : Reportedly haunted locations:...\nAction 2: Finish[REFUTES]\nObservation 2: Episode finished\nCoT\nClaim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.\nThought: Let\u2019s think step by step. The Bermuda Triangle is a loosely-defined region\nin the Atlantic Ocean. The Pacific Ocean is not the Atlantic Ocean.\nAnswer: REFUTES\nExample 5908 (gt: SUPPORTS)\nReAct\nClaim: Princess Mononoke is a film.\nThought 1: I need to search Princess Mononoke and find if it is a film.\nAction 1: Search[Princess Mononoke]\nObservation 1: Princess Mononoke ...\nThought 2: From the observation, it says that Princess Mononoke is a film.\nAction 2: Finish[SUPPORTS]\nObservation 2: Episode finished\nAct\nClaim: Princess Mononoke is a film.\nAction 1: Search[Princess Mononoke]\nObservation 1: Princess Mononoke ...\nAction 2: Finish[SUPPORTS]\nObservation 2: Episode finished\nCoT\nClaim: Princess Mononoke is a film.\nThought: Let\u2019s think step by step. Princess Mononoke is a movie, and movies are\nfilms.\nAnswer: SUPPORTS\nExample 1951 (gt: REFUTES)\nReAct\nClaim: Soyuz was part of the American space program.\nThought 1: I need to search Soyuz and find if it was part of the American space\nprogram.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1810, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3cad464-7862-4346-b54f-b50c14c485e9": {"__data__": {"id_": "a3cad464-7862-4346-b54f-b50c14c485e9", "embedding": null, "metadata": {"page_label": "26", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0cffa38-b905-4946-bb97-d96a8f789334", "node_type": "4", "metadata": {"page_label": "26", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "39dca19b14e1c8b7d5b5c33732b06061ebe54b0203dd6aceb246d9bd9381b731", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "641e96e8-b42f-40b1-9b65-77d69356bad8", "node_type": "1", "metadata": {"page_label": "26", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "8d2f82d9caa42265aadc45169c6c46f35c8831fe3f2a4a49f3b3ee185b3c36e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Action 1: Search[Soyuz]\nObservation 1: Soyuz is a transliteration...\nThought 2: It does not mention the American space program, so I need to search\n\"American space program\" to find if it was part of it.\nAction 2: Search[American space program]\nObservation 2: The space policy...\nThought 3: It does not mention Soyuz, so I am not sure if this claim is true or not.\nAction 3: Finish[NOT ENOUGH INFO]\nObservation 3: Episode finished\nAct\nClaim: Soyuz was part of the American space program.\nAction 1: Search[Soyuz]\nObservation 1: Soyuz is a transliteration...\nAction 2: Finish[NOT ENOUGH INFO]\nObservation 2: Episode finished\nCoT\nClaim: Soyuz was part of the American space program.\nThought: Let\u2019s think step by step. Soyuz is a Russian spacecraft. The American\nspace program is NASA. NASA and Russia have worked together on the\nInternational Space Station.\nAnswer: SUPPORTS\n26", "mimetype": "text/plain", "start_char_idx": 1811, "end_char_idx": 2684, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49d78103-c38b-4c64-a8a1-0c4e7495f6b8": {"__data__": {"id_": "49d78103-c38b-4c64-a8a1-0c4e7495f6b8", "embedding": null, "metadata": {"page_label": "27", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8c82527-1f8c-4b4f-a4cf-fbb04c346a5c", "node_type": "4", "metadata": {"page_label": "27", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "6b5854671b37204df0e96283cbae3bd775b19760aa55e3d82b106917014a6f9d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nExample 3208 (gt: REFUTES)\nReAct\nClaim: Reign Over Me is an American film made in 2010.\nThought 1: I need to search for Reign Over Me and find if it is an American film made\nin 2010.\nAction 1: Search[Reign Over Me]\nObservation 1: Reign Over Me ...\nThought 2: The observation says that it is an American film made in 2007, so it is not\nmade in 2010.\nAction 2: Finish[REFUTES]\nObservation 2: Episode finished\nAct\nClaim: Reign Over Me is an American film made in 2010.\nAction 1: Search[Reign Over Me]\nAction 2: Finish[SUPPORTS]\nObservation 2: Episode finished\nCoT\nClaim: Reign Over Me is an American film made in 2010.\nThought: Let\u2019s think step by step. First, is Reign Over Me an American film? Yes,\nit is. Second, was it made in 2010? Yes, it was.\nAnswer: SUPPORTS\nD.2 ALFW ORLD TRAJECTORIES\nWe provide an example trajectory ofReAct, Act, and ReAct-IM on the same ALFWorld game,\npick_clean_then_place_in_recep-Knife-None-CounterTop-10/\ntrial_T20190909_110445_675754.\nD.2.1 REAC TTRAJECTORY\nReAct is able to correctly \ufb01nd the knife, clean it, and then place it on a countertop.\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e26484bf-e9a7-4849-821b-a9c8ff7cadb2": {"__data__": {"id_": "e26484bf-e9a7-4849-821b-a9c8ff7cadb2", "embedding": null, "metadata": {"page_label": "28", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d51fe16e-98e8-4162-be95-60fa61ff96db", "node_type": "4", "metadata": {"page_label": "28", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "261457212014ccbb02d291c3abf0a76e1ad12b7994665094d56c5827ed90f69e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c01a7198-fa33-49f0-8f2a-5e4e68c1ae68", "node_type": "1", "metadata": {}, "hash": "1d85b9aacaf67c65b2a261d24b8ac8d9725cb68b258500a51bdfb3ee428c8ee3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nReAct ALFWorld trajectory\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 6, a\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\n1.\nYour task is to: put a clean knife in countertop.\n> think: To solve the task, I need to find and take a knife, then clean it with\nsinkbasin, then put it in countertop.\nOK.\n> think: First I need to find a knife. A knife is more likely to appear in cabinet\n(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),\nsinkbasin (1), stoveburner (1-4), toaster (1). I can check one by one, starting with\ncabinet 1.\nOK.\n> go to cabinet 1\nOn the cabinet 1, you see a bowl 1.\n> go to cabinet 2\nThe cabinet 2 is closed.\n> go to cabinet 3\nOn the cabinet 3, you see a glassbottle 1.\n> go to cabinet 4\nOn the cabinet 4, you see a mug 1.\n> go to cabinet 5\nThe cabinet 5 is closed.\n> go to cabinet 6\nThe cabinet 6 is closed.\n> go to drawer 1\nThe drawer 1 is closed.\n> go to drawer 2\nThe drawer 2 is closed.\n> go to drawer 3\nThe drawer 3 is closed.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1351, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c01a7198-fa33-49f0-8f2a-5e4e68c1ae68": {"__data__": {"id_": "c01a7198-fa33-49f0-8f2a-5e4e68c1ae68", "embedding": null, "metadata": {"page_label": "28", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d51fe16e-98e8-4162-be95-60fa61ff96db", "node_type": "4", "metadata": {"page_label": "28", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "261457212014ccbb02d291c3abf0a76e1ad12b7994665094d56c5827ed90f69e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e26484bf-e9a7-4849-821b-a9c8ff7cadb2", "node_type": "1", "metadata": {"page_label": "28", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "627f175c20075b548b5b8b0c06178309a5c8c9849f0ed9ea0893fdb79649bc14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "> go to drawer 3\nThe drawer 3 is closed.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n> go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\nplate 2, a potato 3, and a statue 1.\n> think: Now I find a knife (1). Next, I need to take it.\nOK.\n> take knife 1 from countertop 2\nYou pick up the knife 1 from the countertop 2.\n> think: Now I take a knife (1). Next, I need to go to sinkbasin (1) and clean it.\nOK.\n> go to sinkbasin 1\nOn the sinkbasin 1, you see a fork 3, a lettuce 3, and a spatula 2.\n> clean knife 1 with sinkbasin 1\nYou clean the knife 1 using the sinkbasin 1.\n> think: Now I clean a knife (1). Next, I need to put it in/on countertop 1.\nOK.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n> put knife 1 in/on countertop 1\nYou put the knife 1 in/on the countertop 1.\nD.2.2 AC TTRAJECTORY\nAct is able to correctly \ufb01nd the knife, but tries to clean it using a sinkbasin before actually going\nto the sinkbasin. From there on, the model is unable to recover, and is stuck repeating the same\nsequence of commands. With ReAct, an existing thought would have concretely identi\ufb01ed that the\nknife had been taken, and that the next subgoal was to go to a sinkbasin to clean it.\n28", "mimetype": "text/plain", "start_char_idx": 1311, "end_char_idx": 2660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9026eceb-3364-40ac-bbca-3e7eb22ea7ad": {"__data__": {"id_": "9026eceb-3364-40ac-bbca-3e7eb22ea7ad", "embedding": null, "metadata": {"page_label": "29", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3268defb-8a93-41fd-84a8-5fb4b8633000", "node_type": "4", "metadata": {"page_label": "29", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "3fc64cb09a99979f4547ed3dd42bb3425b96b81c7f7ed2290f160b022c670231", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7239e08d-d0a6-4ee3-ac36-bc79310b3029", "node_type": "1", "metadata": {}, "hash": "fb749f4290bb6b9323f255e0d6f672ac249dd0e80b587be03e575437f4ca6f7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nAct ALFWorld trajectory\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 6, a\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\n1.\nYour task is to: put a clean knife in countertop.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n> go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\nplate 2, a potato 3, and a statue 1.\n> take knife 1 from countertop 2\nYou pick up the knife 1 from the countertop 2.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n> clean knife 1 with sinkbasin 1\nNothing happens.\n> go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n> take knife 2 from countertop 3\nNothing happens.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1297, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7239e08d-d0a6-4ee3-ac36-bc79310b3029": {"__data__": {"id_": "7239e08d-d0a6-4ee3-ac36-bc79310b3029", "embedding": null, "metadata": {"page_label": "29", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3268defb-8a93-41fd-84a8-5fb4b8633000", "node_type": "4", "metadata": {"page_label": "29", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "3fc64cb09a99979f4547ed3dd42bb3425b96b81c7f7ed2290f160b022c670231", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9026eceb-3364-40ac-bbca-3e7eb22ea7ad", "node_type": "1", "metadata": {"page_label": "29", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "e556b0c8f3fdb39ffd800da4313a930c55e9af762e3d18a0aed0a06c1788cb9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc95d600-23b2-4404-810b-64d7740631ab", "node_type": "1", "metadata": {}, "hash": "84ed1dad543221e4642d8aa2ec9379a98ced33271f535778a8e9b4b811307552", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "> take knife 2 from countertop 3\nNothing happens.\n> go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n> take knife 1 from countertop 2\nNothing happens.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n> clean knife 1 with sinkbasin 1\nNothing happens.\n> go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n> take knife 2 from countertop 3\nNothing happens.\n> go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n> take knife 1 from countertop 2\nNothing happens.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n> clean knife 1 with sinkbasin 1\nNothing happens.\n> go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n> take knife 2 from countertop 3\nNothing happens.", "mimetype": "text/plain", "start_char_idx": 1248, "end_char_idx": 2511, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc95d600-23b2-4404-810b-64d7740631ab": {"__data__": {"id_": "bc95d600-23b2-4404-810b-64d7740631ab", "embedding": null, "metadata": {"page_label": "29", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3268defb-8a93-41fd-84a8-5fb4b8633000", "node_type": "4", "metadata": {"page_label": "29", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "3fc64cb09a99979f4547ed3dd42bb3425b96b81c7f7ed2290f160b022c670231", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7239e08d-d0a6-4ee3-ac36-bc79310b3029", "node_type": "1", "metadata": {"page_label": "29", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "f22a1889889fceea92ea5a234fd8ac8c602c77e8c1ae60fb377cf870b71e7a34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "> take knife 2 from countertop 3\nNothing happens.\n> go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n> take knife 1 from countertop 2\nNothing happens.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n> clean knife 1 with sinkbasin 1\nNothing happens.\nD.2.3 REAC T-IM TRAJECTORY\nReAct-IM is able to correctly \ufb01nd the knife, but is unable to clean it as well. The LLM then\ngets stuck trying to place this knife on a countertop. One possible issue is the incorrect thought\nthink: I need to find a clean knife. , which seems to trick the LLM in believing\nthat the knife is already cleaned. In ReAct, this sequence of subgoals would be concretely captured\nby each thought.\n29", "mimetype": "text/plain", "start_char_idx": 2462, "end_char_idx": 3267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8c6f3b2-e9ad-4527-8a36-6bdb5bfac9b1": {"__data__": {"id_": "f8c6f3b2-e9ad-4527-8a36-6bdb5bfac9b1", "embedding": null, "metadata": {"page_label": "30", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "34634f5b-ab87-4b7b-ac38-2b858e403710", "node_type": "4", "metadata": {"page_label": "30", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "d7cf4aec17ad12bb5d612f5bd2245892178352e886a96d72e11811ecf19e0c59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "651819ab-c8c4-450f-ab10-75abb1c9112d", "node_type": "1", "metadata": {}, "hash": "06f3291de9cbc12f5a40473592d04146c2ea9346b3202b19542a128a7e05aab8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nReAct-IM ALFWorld trajectory\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 6, a\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\n1.\nYour task is to: put a clean knife in countertop.\n> think: To solve the task, I need to find and take a clean knife, then put it in\ncountertop.\nOK.\n> think: I need to find a clean knife.\nOK.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n> think: I need to find a clean knife.\nOK.\n> go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\nplate 2, a potato 3, and a statue 1.\n> think: I need to find a clean knife.\nOK.\n> take knife 1 from countertop 2\nYou pick up the knife 1 from the countertop 2.\n> think: I need to put this knife (1) in/on countertop 1.\nOK.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n> think: I need to put this knife (1) in/on countertop 1.\nOK.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1332, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "651819ab-c8c4-450f-ab10-75abb1c9112d": {"__data__": {"id_": "651819ab-c8c4-450f-ab10-75abb1c9112d", "embedding": null, "metadata": {"page_label": "30", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "34634f5b-ab87-4b7b-ac38-2b858e403710", "node_type": "4", "metadata": {"page_label": "30", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "d7cf4aec17ad12bb5d612f5bd2245892178352e886a96d72e11811ecf19e0c59", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8c6f3b2-e9ad-4527-8a36-6bdb5bfac9b1", "node_type": "1", "metadata": {"page_label": "30", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "a21cb85e09322e29e37629e8ab5b93c1ccf9a145f96e392a24fbcf0d03e4fba1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "OK.\n> put knife 1 in/on countertop 1\nYou put the knife 1 in/on the countertop 1.\n> think: I need to put this knife (1) in/on countertop 1.\nOK.\n> put knife 1 in/on countertop 1\nNothing happens.\n> think: I need to put this knife (1) in/on countertop 1.\nOK.\n> put knife 1 in/on countertop 1\nNothing happens.\n> think: I need to put this knife (1) in/on countertop 1.\nOK.\n> put knife 1 in/on countertop 1\nNothing happens.\n> think: I need to put this knife (1) in/on countertop 1.\nOK.\n> put knife 1 in/on countertop 1\nNothing happens.\n> think: I need to put this knife (1) in/on countertop 1.\nOK.\n> put knife 1 in/on countertop 1\nNothing happens.\n> think: I need to put this knife (1) in/on countertop 1.\nOK.\n30", "mimetype": "text/plain", "start_char_idx": 1329, "end_char_idx": 2034, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf22cb2c-eb68-4fe8-ad0c-6f38cb315979": {"__data__": {"id_": "cf22cb2c-eb68-4fe8-ad0c-6f38cb315979", "embedding": null, "metadata": {"page_label": "31", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d55b8927-c92e-45c5-a5c8-cf27bf74860b", "node_type": "4", "metadata": {"page_label": "31", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4339d5d13420b8f6e55038938402f615e6e3045e200fa511ccac39dfd8aecd17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "384c3914-52cf-45e4-afbe-aaa39e9e27a8", "node_type": "1", "metadata": {}, "hash": "20399b9f780d8176975313cb1323ba27458bb3f5659d9ff74efb1b513df19820", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nD.3 W EBSHOP TRAJECTORIES\nTable 10: Example trajectories for Webshop predicted by Act and ReAct. Compared to Act,\nReAct uses reasoning to \ufb01nd products that satisfy all target attributes.\nInstruction: get me a sixteen pack of apple cinnamon freeze dried banana chips, and price lower than 50.00 dollars\nAct ReAct\nAction: search\n[\nsixteen pack apple cinnamon freeze dried banana\nchips\n]\nObservation:[\nBack to Search\n]\nPage 1 (Total results: 50)[\nNext\n]\n[\nB0061IVFZE\n]\nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -\nOunce (Pack of 100)\n$85.0[\nB096H2P6G2\n]\nMoon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -\n(Variety Pack)\n$18.99[\nB092JLLYK6\n]\nNature\u2019s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\nFor School Lunches or an On-The-Go Snack - No Sugar Added,\nNon GMO, Gluten Free, Nothing Arti\ufb01cial (0.53oz) 6-Pack\n$12.99\nAction: click\n[\nB0061IVFZE\n]\nObservation:[\nBack to Search\n]\n[\nPrev\n]\n\ufb02avor name [asian pear][banana][fuji apple & cinnamon][strawberry\nbanana] Brothers-ALL-Natural Fruit Crisps, Strawberry Banana,\n0.42 Ounce (Pack of 100)\nPrice: $85.0\nRating: N.A.\n[Description]\n[Features]\n[Reviews]\n[Buy Now]\nAction: click[Buy Now]\nAction: search\n[\nsixteen pack apple cinnamon freeze dried banana\nchips\n]\nObservation:[\nBack to Search\n]\nPage 1 (Total results: 50)[\nNext\n]\n[\nB0061IVFZE\n]\nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -\nOunce (Pack of 100)\n$85.0[\nB096H2P6G2\n]\nMoon Fruit Freeze Dried Fruit Snacks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1506, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "384c3914-52cf-45e4-afbe-aaa39e9e27a8": {"__data__": {"id_": "384c3914-52cf-45e4-afbe-aaa39e9e27a8", "embedding": null, "metadata": {"page_label": "31", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d55b8927-c92e-45c5-a5c8-cf27bf74860b", "node_type": "4", "metadata": {"page_label": "31", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4339d5d13420b8f6e55038938402f615e6e3045e200fa511ccac39dfd8aecd17", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf22cb2c-eb68-4fe8-ad0c-6f38cb315979", "node_type": "1", "metadata": {"page_label": "31", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "689abee2caaa0dcf9e1a762131e79f9b51ad56c9e3f6a763f64ed5f4b0fa9714", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fruit Snacks for Kids -\n(Variety Pack)\n$18.99[\nB092JLLYK6\n]\nNature\u2019s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\nFor School Lunches or an On-The-Go Snack - No Sugar Added,\nNon GMO, Gluten Free, Nothing Arti\ufb01cial (0.53oz) 6-Pack\n$12.99\nAction: think[B0061IVFZE is strawberry banana, not apple cinnamon.\nB096H2P6G2 is fruit snacks, not freeze dried banana chips.\nB092JLLYK6 is banana crisps, not apple cinnamon. I can check\nB092JLLYK6 \ufb01rst.\nObservation: OK.\nAction: click\n[\nB092JLLYK6\n]\nObservation:[\nBack to Search\n]\n[\nPrev\n]\n\ufb02avor name [apple][apple cinnamon][banana][cantaloupe][peach][pear]\n[strawberry][strawberry banana][sampler variety pack][mega variety\npack][orchard variety pack]\nsize [0.53 ounce (pack of 6)][0.53 ounce (pack of 8)][0.53 ounce (pack\nof 16)]\nNature\u2019s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\nFor School Lunches or an On-The-Go Snack - No Sugar Added, Non\nGMO, Gluten Free, Nothing Arti\ufb01cial (0.53oz) 6-Pack\nPrice: $12.99\nRating: N.A.\n[Description]\n[Features]\n[Reviews]\n[Buy Now]\nAction: think[For sixteen pack of apple cinnamon freeze dried banana\nchips, the item has options \u2019apple cinnamon\u2019 and \u20190.53 ounce (pack of\n16)\u2019 and seems good to buy.]\nObservation: OK.\nAction: click[apple cinnamon]\nObservation: You have clicked apple cinnamon.\nAction: click[0.53 ounce (pack of 16)]\nObservation: You have clicked 0.53 ounce (pack of 16).\nAction: click[Buy Now]\nScore: 0.125 Score: 1.0\n31", "mimetype": "text/plain", "start_char_idx": 1507, "end_char_idx": 2944, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9bf31843-d616-494f-b39f-7d0678d4ddd1": {"__data__": {"id_": "9bf31843-d616-494f-b39f-7d0678d4ddd1", "embedding": null, "metadata": {"page_label": "32", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0177ad22-60e4-4176-95bd-1f4d92675c7a", "node_type": "4", "metadata": {"page_label": "32", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "27a82698d8860f22b0a465dafdc8c9effee324549d74782561c2c6b54709922b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b991390-00f7-40c4-ba5b-fdc07433044e", "node_type": "1", "metadata": {}, "hash": "a08a2622e39da02c90c7cb563eb3618500ac1368b7276e6d661901cc45d57c18", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nE M ORE ANALYSIS\nE.1 S UCCESS AND FAILURE MODES ANALYSIS\nWe provide some examples corresponding to the success and error mode analysis given in Sec. 3.3.\nSearch results and non-representative steps are omitted to reduce space.\nSuccess: True positive\nReAct\nQuestion: Author David Chanoff has collaborated with a U.S. Navy admiral who served as\nthe ambassador to the United Kingdom under which President?\nThought 1: I need to search David Chanoff and find the U.S. Navy admiral he\ncollaborated with.\nAction 1: Search[David Chanoff]\nObservation 1: David Chanoff is a noted author...\nAction 2: Search[William J. Crowe]\nObservation 2: William J. Crowe is...\nThought 3: William J. Crowe served as the ambassador to the United Kingdom under\nPresident Bill Clinton. So the answer is Bill Clinton.\nAction 3: Finish[Bill Clinton]\nObservation 3: Episode finished\nCoT\nQuestion: What was the name of the team that represented an Arizona university in\nthe conference that covers a broad expanse of the western United States,\nincluding Arizona, California, and New Mexico?\nThought: Let\u2019s think step by step. The name of the team that represented an Arizona\nuniversity in the conference that covers a broad expanse of the western\nUnited States, including Arizona, California, and New Mexico is Arizona\nState Sun Devils.\nAnswer: Arizona State Sun Devils\nSuccess: False positive\nReAct\nQuestion: Did the board game San Marco or About Time come out first?\nThought 1: I need to search San Marco and About Time, and find which came out first.\nAction 1: Search[San Marco]\nObservation 1: San Marco is one of the ... The was built in venice.\nThought 2: San Marco is a sestiere of Venice. I need to search About Time next.\nAction 2: Search[About Time]\nObservation 2: Could not find [About Time]. Similar: [...].\nThought 3: About Time is a 2013 film. So San Marco came out first.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b991390-00f7-40c4-ba5b-fdc07433044e": {"__data__": {"id_": "8b991390-00f7-40c4-ba5b-fdc07433044e", "embedding": null, "metadata": {"page_label": "32", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0177ad22-60e4-4176-95bd-1f4d92675c7a", "node_type": "4", "metadata": {"page_label": "32", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "27a82698d8860f22b0a465dafdc8c9effee324549d74782561c2c6b54709922b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bf31843-d616-494f-b39f-7d0678d4ddd1", "node_type": "1", "metadata": {"page_label": "32", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "c367b4ee9d182fd44f0065aca4ee251c040cb7c04aba57376dbe94f27ea583e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So San Marco came out first.\nAction 3: Finish[San Marco]\nObservation 3: Episode finished\nCoT\nQuestion: Was Vanderbilt University or Emory University founded first?\nThought: Let\u2019s think step by step. Vanderbilt University was founded in 1873. Emory\nUniversity was founded in 1836. 1873 (Vanderbilt University) > 1836 (Emory\nUniversity), so Vanderbilt University was founded first.\nAnswer: Emory University\nFailure: Reasoning error\nReAct\nQuestion: What was the middle name of the actress who was cast alongside Harvey\nFierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura\nBell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in\nHairspray?\nThought 1: I need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell,\nMary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and\nMarissa Jaret Winokur, find the actress that was cast alongside them in\nHairspray, then find its middle name.\nAction 1: Search[Harvey Fierstein]\nObservation 1: Harvey Forbes Fierstein ...\nThought 2: Harvey Fierstein is an actor, playwright, and screenwriter. I need to\nsearch Linda Hart next...\nAction 2: Search[Linda Hart]...\nCoT\nQuestion: Who is older Danny Green or James Worthy?\nThought: Let\u2019s think step by step. Danny Green was born in 1987. James Worthy was\nborn in 1961. 1987 (Danny Green) > 1961 (James Worthy), so Danny Green is\nolder.\nAnswer: Danny Green\n32", "mimetype": "text/plain", "start_char_idx": 1869, "end_char_idx": 3276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30b419c3-c04b-47bc-8250-bceaf6b41f5f": {"__data__": {"id_": "30b419c3-c04b-47bc-8250-bceaf6b41f5f", "embedding": null, "metadata": {"page_label": "33", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5dc6c946-8754-4f1c-a10e-1b24399902a0", "node_type": "4", "metadata": {"page_label": "33", "file_name": "ReAct.pdf", "file_path": "/Users/betopia/LlamaIndex/data/ReAct.pdf", "file_type": "application/pdf", "file_size": 633805, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "913c5a0b4f3d01a961bf6bcbdcedc760120d31e83d64477b4fb6a43c631ba64e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2023\nFailure: Search error\nReAct\nQuestion: What does the goddess associated with the goddess frigg consists of what\ntales?\nThought 1: I need to search goddess frigg and find its associated goddess.\nAction 1: Search[goddess frigg]\nObservation 1: Could not find goddess frigg. Similar: [\u2019Frigg\u2019, \u2019Frigg and Freyja common\norigin hypothesis\u2019, \u2019Freyja (section Relation to Frigg and other goddesses\nand figures)\u2019, \u2019Eir (redirect from Eir (goddess))\u2019, \u2019Baldr\u2019].\nThought 2: I need to search Frigg instead...\nFailure: Hallucination\nCoT\nQuestion: What was the first year a scientific journal published by an organization\nlocated in the Bronx was published?\nThought: Let\u2019s think step by step. The first year a scientific journal published by\nan organization located in the Bronx was published was 1916.\nAnswer: 1916 (Label: 1909)\nFailure: Label ambiguity\nReAct\nQuestion: What nationality is the man who developed the dictator game?\nThought 1: I need to search dictator game, find the man who developed it, then find his\nnationality.\nAction 1: Search[dictator game]\nObservation 1: The dictator game is a popular experimental instrument in social psychology\nand economics...\nAction 3: Search[Amos Tversky]\nObservation 3: Amos Nathan Tversky...\nThought 4: Amos Tversky is an Israeli cognitive and mathematical psychologist. So the\nman who developed the dictator game is Israeli.\nAction 4: Finish[Israeli] (label: Israel-American)\nCoT\nQuestion: Kurt Fuller from Stockton, CA appeared in which American Detective Comedy\nDrama television series created by Steve Franks?\nThought: Let\u2019s think step by step. Kurt Fuller from Stockton, CA appeared in Psych.\nPsych is an American Detective Comedy Drama television series created by\nSteve Franks.\nAnswer: Psych (label: Psych is an American detective comedy-drama)\n33", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72c0fc9b-2702-45a5-bca5-7d23c2a001c7": {"__data__": {"id_": "72c0fc9b-2702-45a5-bca5-7d23c2a001c7", "embedding": null, "metadata": {"page_label": "1", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "16fd2e34-8661-4563-83ba-97061daab5f6", "node_type": "4", "metadata": {"page_label": "1", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "bcd0778702703429a78110ed2331a970e917e208a598007e24a8961b45f395f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b836885-ede9-49d0-9a7f-e6812763e8a6", "node_type": "1", "metadata": {}, "hash": "e611fc320e6ce38cda5400c6424c669775d47d45cde19cf45bb2b3fe05f0c7ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.com\nNoam Shazeer\u2217\nGoogle Brain\nnoam@google.com\nNiki Parmar\u2217\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.com\nAidan N. Gomez\u2217 \u2020\nUniversity of Toronto\naidan@cs.toronto.edu\n\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217 \u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b836885-ede9-49d0-9a7f-e6812763e8a6": {"__data__": {"id_": "0b836885-ede9-49d0-9a7f-e6812763e8a6", "embedding": null, "metadata": {"page_label": "1", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "16fd2e34-8661-4563-83ba-97061daab5f6", "node_type": "4", "metadata": {"page_label": "1", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "bcd0778702703429a78110ed2331a970e917e208a598007e24a8961b45f395f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72c0fc9b-2702-45a5-bca5-7d23c2a001c7", "node_type": "1", "metadata": {"page_label": "1", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "81d97f905cd87b4e8555b6ba7b0480146a37e204ea55cd4dca072e5318bb6d65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023", "mimetype": "text/plain", "start_char_idx": 1884, "end_char_idx": 2859, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe966015-65d2-4a9c-93a8-ce6a55c4014e": {"__data__": {"id_": "fe966015-65d2-4a9c-93a8-ce6a55c4014e", "embedding": null, "metadata": {"page_label": "2", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91bbd494-66ea-419c-a8c5-0e2e99927248", "node_type": "4", "metadata": {"page_label": "2", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "8e393ca48c4603a15e8889ebd8e0df81de3a39f873953c7e62512df23b6780e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb9098f4-7962-483e-a2e5-15d2734619fa", "node_type": "1", "metadata": {}, "hash": "5ae02022e45861d6ccd0a50c4c385c3dd0f0a195443d89d45d33ac8f40f5a6df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2429, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb9098f4-7962-483e-a2e5-15d2734619fa": {"__data__": {"id_": "fb9098f4-7962-483e-a2e5-15d2734619fa", "embedding": null, "metadata": {"page_label": "2", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91bbd494-66ea-419c-a8c5-0e2e99927248", "node_type": "4", "metadata": {"page_label": "2", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "8e393ca48c4603a15e8889ebd8e0df81de3a39f873953c7e62512df23b6780e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe966015-65d2-4a9c-93a8-ce6a55c4014e", "node_type": "1", "metadata": {"page_label": "2", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "5503dbfd78efdddc328f6a130d24f67786341397081a01508de85e3e1410bb01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2", "mimetype": "text/plain", "start_char_idx": 2430, "end_char_idx": 4257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d5a0bc12-e0f9-42d5-b45e-bba001ffa39a": {"__data__": {"id_": "d5a0bc12-e0f9-42d5-b45e-bba001ffa39a", "embedding": null, "metadata": {"page_label": "3", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95ed5c99-8f17-452b-bb14-0a0a89cd33af", "node_type": "4", "metadata": {"page_label": "3", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "c5a55e78add30aa9cb75664e7d146bc4da61d3c48b86ab93f1f87adb49d45f09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c8b5395-dc88-472a-8191-fba27a69cc96": {"__data__": {"id_": "3c8b5395-dc88-472a-8191-fba27a69cc96", "embedding": null, "metadata": {"page_label": "4", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebd047c6-66b7-4795-aed0-6b02b1413e60", "node_type": "4", "metadata": {"page_label": "4", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "f4c6ac5b5709bc324a0425b13a5ed6e673b4fec6db2eb9ef6424cf3b0bc916d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5eee747-b453-40e5-9801-b54f469a6d94", "node_type": "1", "metadata": {}, "hash": "3821c10eec9ec63c872dc9b448a7d84de39fbc3012496ce0eaf158c013985a2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V) = softmax(QKT\n\u221adk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1\u221adk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\u221adk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2143, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d5eee747-b453-40e5-9801-b54f469a6d94": {"__data__": {"id_": "d5eee747-b453-40e5-9801-b54f469a6d94", "embedding": null, "metadata": {"page_label": "4", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebd047c6-66b7-4795-aed0-6b02b1413e60", "node_type": "4", "metadata": {"page_label": "4", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "f4c6ac5b5709bc324a0425b13a5ed6e673b4fec6db2eb9ef6424cf3b0bc916d0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c8b5395-dc88-472a-8191-fba27a69cc96", "node_type": "1", "metadata": {"page_label": "4", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "a5a724a6d2a9885f7337107474eef2c0286f144b388f350dfbb44f4262420fdc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4", "mimetype": "text/plain", "start_char_idx": 2144, "end_char_idx": 2505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56d067d4-ae0d-4b0f-a9d5-4f60096b3b45": {"__data__": {"id_": "56d067d4-ae0d-4b0f-a9d5-4f60096b3b45", "embedding": null, "metadata": {"page_label": "5", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad3ac395-2e77-48ec-b51f-efba9b6658e7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "cf230a35be6511800806b3e9b34520a68dd3101fedcc3f082545ec085f89423a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c7fa795-7e64-449e-89a1-fb95fa8bd3cd", "node_type": "1", "metadata": {}, "hash": "75e3df23b5eb9d39de59449f3413feae73bffea82fbb28a0503ae514ecfdb4ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nWhere the projections are parameter matricesWQ\ni \u2208 Rdmodel\u00d7dk , WK\ni \u2208 Rdmodel\u00d7dk , WV\ni \u2208 Rdmodel\u00d7dv\nand WO \u2208 Rhdv\u00d7dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2004, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c7fa795-7e64-449e-89a1-fb95fa8bd3cd": {"__data__": {"id_": "4c7fa795-7e64-449e-89a1-fb95fa8bd3cd", "embedding": null, "metadata": {"page_label": "5", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad3ac395-2e77-48ec-b51f-efba9b6658e7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "cf230a35be6511800806b3e9b34520a68dd3101fedcc3f082545ec085f89423a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56d067d4-ae0d-4b0f-a9d5-4f60096b3b45", "node_type": "1", "metadata": {"page_label": "5", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "b78dd415dd7e978e66e4e9f0537c8fe5be99f957043f3931c88204e18dd6e734", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221admodel.\n5", "mimetype": "text/plain", "start_char_idx": 1991, "end_char_idx": 3188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "136f7dd4-eb94-4700-a805-d14e2826b8b3": {"__data__": {"id_": "136f7dd4-eb94-4700-a805-d14e2826b8b3", "embedding": null, "metadata": {"page_label": "6", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af2f2297-262b-4c2a-a05e-dd5d1f356408", "node_type": "4", "metadata": {"page_label": "6", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "e44cd7c0297e6e54fcbe814fb58c2896d891e989c284755daee6b951d954b0fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c2d90cb-b27b-4870-82a9-d73c62260e1d", "node_type": "1", "metadata": {}, "hash": "25e24ff7ff003773f64d648b52abaa64ce5bc84c76804e6eb2919678d0466fe6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 \u00b7 d) O(1) O(1)\nRecurrent O(n \u00b7 d2) O(n) O(n)\nConvolutional O(k \u00b7 n \u00b7 d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r \u00b7 n \u00b7 d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1938, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c2d90cb-b27b-4870-82a9-d73c62260e1d": {"__data__": {"id_": "4c2d90cb-b27b-4870-82a9-d73c62260e1d", "embedding": null, "metadata": {"page_label": "6", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af2f2297-262b-4c2a-a05e-dd5d1f356408", "node_type": "4", "metadata": {"page_label": "6", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "e44cd7c0297e6e54fcbe814fb58c2896d891e989c284755daee6b951d954b0fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "136f7dd4-eb94-4700-a805-d14e2826b8b3", "node_type": "1", "metadata": {"page_label": "6", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "2d35ed330bb2c11179a5bd49f2cd6ed132ba7b739c330ee64827620762ff3e34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208 Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6", "mimetype": "text/plain", "start_char_idx": 1939, "end_char_idx": 3479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a382839-bead-4c3a-92dc-720364648d83": {"__data__": {"id_": "8a382839-bead-4c3a-92dc-720364648d83", "embedding": null, "metadata": {"page_label": "7", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5198786b-0ad4-4039-a302-42457a51128a", "node_type": "4", "metadata": {"page_label": "7", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "8e7e14abb4aca958853ed66063a835fd22c4e57fe9348afa3da288a508d0f09e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ff918b0-f6a4-420f-8ac3-d9e771decf91", "node_type": "1", "metadata": {}, "hash": "1de3a4f8d2877e26e74fab33901e92f607c37ee18664fb779cc36abb22f83adf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "length n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2233, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ff918b0-f6a4-420f-8ac3-d9e771decf91": {"__data__": {"id_": "7ff918b0-f6a4-420f-8ac3-d9e771decf91", "embedding": null, "metadata": {"page_label": "7", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5198786b-0ad4-4039-a302-42457a51128a", "node_type": "4", "metadata": {"page_label": "7", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "8e7e14abb4aca958853ed66063a835fd22c4e57fe9348afa3da288a508d0f09e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a382839-bead-4c3a-92dc-720364648d83", "node_type": "1", "metadata": {"page_label": "7", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "31f261a89d06a324e61e5a5323728a9ac25bd481fac1168422b2d77580307af7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d\u22120.5\nmodel \u00b7 min(step_num\u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7", "mimetype": "text/plain", "start_char_idx": 2165, "end_char_idx": 3322, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9c8e0ee-8a64-466b-9130-7d10d9747c88": {"__data__": {"id_": "e9c8e0ee-8a64-466b-9130-7d10d9747c88", "embedding": null, "metadata": {"page_label": "8", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f50d6d98-bda7-475d-8ac4-b72bbb409b90", "node_type": "4", "metadata": {"page_label": "8", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "e5d67a181429e69789b9af4871d85fa02afd52f1d947f17a8ae0f241d629f139", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73a3a64e-46ef-4a18-81b6-bd8febbfee64", "node_type": "1", "metadata": {}, "hash": "18028f55b81ee7f1530de23b47cd9a242dbe71f316f792013b6aead9465d22fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 \u00b7 1020\nGNMT + RL [38] 24.6 39.92 2.3 \u00b7 1019 1.4 \u00b7 1020\nConvS2S [9] 25.16 40.46 9.6 \u00b7 1018 1.5 \u00b7 1020\nMoE [32] 26.03 40.56 2.0 \u00b7 1019 1.2 \u00b7 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \u00b7 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 \u00b7 1020 1.1 \u00b7 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 \u00b7 1019 1.2 \u00b7 1021\nTransformer (base model) 27.3 38.1 3.3 \u00b7 1018\nTransformer (big) 28.4 41.8 2.3 \u00b7 1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1185, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "73a3a64e-46ef-4a18-81b6-bd8febbfee64": {"__data__": {"id_": "73a3a64e-46ef-4a18-81b6-bd8febbfee64", "embedding": null, "metadata": {"page_label": "8", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f50d6d98-bda7-475d-8ac4-b72bbb409b90", "node_type": "4", "metadata": {"page_label": "8", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "e5d67a181429e69789b9af4871d85fa02afd52f1d947f17a8ae0f241d629f139", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9c8e0ee-8a64-466b-9130-7d10d9747c88", "node_type": "1", "metadata": {"page_label": "8", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "c0c0cf74575a35c7eda0ef643071356f3b4d42d141b3fff3acfb603583e0e97d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c13cf12-d797-40e9-9e12-5cb01332da8b", "node_type": "1", "metadata": {}, "hash": "f0a660e147a9d5cdf0aa5c55bb65afbd07992903cf668d3863f3ae25f3f93df8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.", "mimetype": "text/plain", "start_char_idx": 1186, "end_char_idx": 2889, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c13cf12-d797-40e9-9e12-5cb01332da8b": {"__data__": {"id_": "5c13cf12-d797-40e9-9e12-5cb01332da8b", "embedding": null, "metadata": {"page_label": "8", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f50d6d98-bda7-475d-8ac4-b72bbb409b90", "node_type": "4", "metadata": {"page_label": "8", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "e5d67a181429e69789b9af4871d85fa02afd52f1d947f17a8ae0f241d629f139", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73a3a64e-46ef-4a18-81b6-bd8febbfee64", "node_type": "1", "metadata": {"page_label": "8", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "942d6f58a8223556b1d25c718c03395f0f1cce0c0ef32e78afedaa9c3f67b11f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8", "mimetype": "text/plain", "start_char_idx": 2890, "end_char_idx": 3193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ec93aa3-c377-497f-a7a6-7794e78892fe": {"__data__": {"id_": "7ec93aa3-c377-497f-a7a6-7794e78892fe", "embedding": null, "metadata": {"page_label": "9", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f76df3b-fa06-411e-a400-291d0701813b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "8c07a5205f6df87675fa5fc2476c19013f486b8a9926194d78764ff17a72325d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5bb93ee-1434-4942-8594-a80297295025", "node_type": "1", "metadata": {}, "hash": "85e45de686221237142e9936d6de6a13bac86538cf920f0dfeeb2e73843a4f0a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop \u03f5ls\ntrain PPL BLEU params\nsteps (dev) (dev) \u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5bb93ee-1434-4942-8594-a80297295025": {"__data__": {"id_": "c5bb93ee-1434-4942-8594-a80297295025", "embedding": null, "metadata": {"page_label": "9", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f76df3b-fa06-411e-a400-291d0701813b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "8c07a5205f6df87675fa5fc2476c19013f486b8a9926194d78764ff17a72325d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ec93aa3-c377-497f-a7a6-7794e78892fe", "node_type": "1", "metadata": {"page_label": "9", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "5d6d88b203dbe2da778d78f424526f97d700c658d7d43e4003d44484ca9cdfe9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9", "mimetype": "text/plain", "start_char_idx": 981, "end_char_idx": 2973, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3932b202-d880-4662-a53e-01526a108b31": {"__data__": {"id_": "3932b202-d880-4662-a53e-01526a108b31", "embedding": null, "metadata": {"page_label": "10", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c02e1ec3-1955-4c54-b611-61d8fdb8e6bc", "node_type": "4", "metadata": {"page_label": "10", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "ad02b814b82706c2a358d5291e861eaec0d293a7d9b4e848cbe53b8d95983935", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d61012a7-f300-4b23-80c6-1520c01e440d", "node_type": "1", "metadata": {}, "hash": "859b5dc0803cd4e50e0ba27fe19a1182023b62929a018e24edf1d72052a517c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d61012a7-f300-4b23-80c6-1520c01e440d": {"__data__": {"id_": "d61012a7-f300-4b23-80c6-1520c01e440d", "embedding": null, "metadata": {"page_label": "10", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c02e1ec3-1955-4c54-b611-61d8fdb8e6bc", "node_type": "4", "metadata": {"page_label": "10", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "ad02b814b82706c2a358d5291e861eaec0d293a7d9b4e848cbe53b8d95983935", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3932b202-d880-4662-a53e-01526a108b31", "node_type": "1", "metadata": {"page_label": "10", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "bf94adc2dde05bd6be6647682da94ee3cbc8c54621bbc700fab857337c2c6cf4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10", "mimetype": "text/plain", "start_char_idx": 1676, "end_char_idx": 3112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1a99e637-1d66-4f76-be63-0d5f0076db53": {"__data__": {"id_": "1a99e637-1d66-4f76-be63-0d5f0076db53", "embedding": null, "metadata": {"page_label": "11", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b7f06ce-bb26-4fa5-9531-9fade38e867c", "node_type": "4", "metadata": {"page_label": "11", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "878edbf03b60e50d425c81576e8a047e969cf65959d7838665fa20de97cb11af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "469a47cc-bfe4-4664-9e96-47515fb59d92", "node_type": "1", "metadata": {}, "hash": "02d8c66b036ab1e090655a847c233db786dfa770fb0e59feb9bc16cc513d77d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1472, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "469a47cc-bfe4-4664-9e96-47515fb59d92": {"__data__": {"id_": "469a47cc-bfe4-4664-9e96-47515fb59d92", "embedding": null, "metadata": {"page_label": "11", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b7f06ce-bb26-4fa5-9531-9fade38e867c", "node_type": "4", "metadata": {"page_label": "11", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "878edbf03b60e50d425c81576e8a047e969cf65959d7838665fa20de97cb11af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a99e637-1d66-4f76-be63-0d5f0076db53", "node_type": "1", "metadata": {"page_label": "11", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "6b8bf6004c1ee19629ff98e74b162cdd39f815ff4ecc12e90b05a7b32db45f7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe9505f8-f97a-4a55-80f9-218eb747c05f", "node_type": "1", "metadata": {}, "hash": "4b1443950a56844f083825839d6025f79d5fae06717e8053155c5b19dbcb4199", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.", "mimetype": "text/plain", "start_char_idx": 1434, "end_char_idx": 2962, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe9505f8-f97a-4a55-80f9-218eb747c05f": {"__data__": {"id_": "fe9505f8-f97a-4a55-80f9-218eb747c05f", "embedding": null, "metadata": {"page_label": "11", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b7f06ce-bb26-4fa5-9531-9fade38e867c", "node_type": "4", "metadata": {"page_label": "11", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "878edbf03b60e50d425c81576e8a047e969cf65959d7838665fa20de97cb11af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "469a47cc-bfe4-4664-9e96-47515fb59d92", "node_type": "1", "metadata": {"page_label": "11", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "7f05ec52fa5a8a751d0ce48ed527fb1b1c14a299bc581c01115a0243b14768e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11", "mimetype": "text/plain", "start_char_idx": 2963, "end_char_idx": 3215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ca6c63c5-a7ba-417b-990f-4e8ca4cab9a4": {"__data__": {"id_": "ca6c63c5-a7ba-417b-990f-4e8ca4cab9a4", "embedding": null, "metadata": {"page_label": "12", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab2ee5d2-a4f9-45f7-ba9f-ef20d4af81ca", "node_type": "4", "metadata": {"page_label": "12", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "58a9539081699f32cc4272ed0651999120ac0b7e721b31656f29fe7766ae65a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d3efd38-624c-418a-a875-3edc6ece41c3", "node_type": "1", "metadata": {}, "hash": "205e3bbc2363088ea4751575c3bd1c79d3d4a3da8ff1775f541b88cd7f8ec39c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1493, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d3efd38-624c-418a-a875-3edc6ece41c3": {"__data__": {"id_": "4d3efd38-624c-418a-a875-3edc6ece41c3", "embedding": null, "metadata": {"page_label": "12", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab2ee5d2-a4f9-45f7-ba9f-ef20d4af81ca", "node_type": "4", "metadata": {"page_label": "12", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "58a9539081699f32cc4272ed0651999120ac0b7e721b31656f29fe7766ae65a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca6c63c5-a7ba-417b-990f-4e8ca4cab9a4", "node_type": "1", "metadata": {"page_label": "12", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "b94fda0613b903905a680da40917cc114c7ad08cfb033cdcb713368e04f10f1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3adb667e-d9a1-4a66-acd8-03e6888a536c", "node_type": "1", "metadata": {}, "hash": "0e3d20af4a117848e26fef0f3861b6ce89b1f794b127b2ee1bf70b376e014303", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation.", "mimetype": "text/plain", "start_char_idx": 1455, "end_char_idx": 2950, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3adb667e-d9a1-4a66-acd8-03e6888a536c": {"__data__": {"id_": "3adb667e-d9a1-4a66-acd8-03e6888a536c", "embedding": null, "metadata": {"page_label": "12", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab2ee5d2-a4f9-45f7-ba9f-ef20d4af81ca", "node_type": "4", "metadata": {"page_label": "12", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "58a9539081699f32cc4272ed0651999120ac0b7e721b31656f29fe7766ae65a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d3efd38-624c-418a-a875-3edc6ece41c3", "node_type": "1", "metadata": {"page_label": "12", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "e30048566e36c12c1f0a4944569f1134f50dacd0ecc61e7748027a91c4d330b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434\u2013443. ACL, August 2013.\n12", "mimetype": "text/plain", "start_char_idx": 2867, "end_char_idx": 3213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eebe6332-9e51-40a2-9c4b-5e8becf069bf": {"__data__": {"id_": "eebe6332-9e51-40a2-9c4b-5e8becf069bf", "embedding": null, "metadata": {"page_label": "13", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c1d8cb9-5d2c-453c-aef1-63ae2b74be30", "node_type": "4", "metadata": {"page_label": "13", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "7226a5ceb668851f525dab4fef4dd311a6511679c468beae998c916ffdd28ddc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "904d540f-6917-4864-b1cc-2d08e99f7b7a": {"__data__": {"id_": "904d540f-6917-4864-b1cc-2d08e99f7b7a", "embedding": null, "metadata": {"page_label": "14", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e54825e2-f79d-4230-9479-782e0e6d891b", "node_type": "4", "metadata": {"page_label": "14", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "72b06f0c111dfe2006bbd10484066ddc6e4e376c6363f6d8647fcbf2cd788d1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 815, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7442f82b-cb88-412b-8aff-6261d773d4b1": {"__data__": {"id_": "7442f82b-cb88-412b-8aff-6261d773d4b1", "embedding": null, "metadata": {"page_label": "15", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a30ac3c8-8693-4b03-bacc-0c9fd5839ef1", "node_type": "4", "metadata": {"page_label": "15", "file_name": "attention.pdf", "file_path": "/Users/betopia/LlamaIndex/data/attention.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2025-10-19", "last_modified_date": "2025-11-03"}, "hash": "ca0fd9caf00302c82031697ac4a2771eced1668d2c9943c3adae7f0dd6ec0834", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec3d26ba-59eb-44db-9645-e2a3d6e429b5": {"__data__": {"id_": "ec3d26ba-59eb-44db-9645-e2a3d6e429b5", "embedding": null, "metadata": {"page_label": "1", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d86b379-a6e5-4a09-b263-48cc07e41376", "node_type": "4", "metadata": {"page_label": "1", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "c9650401b492d58d2aca7c79574538e705ba9f46a5330b894ec59f06b022951a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0a2ff6a-610b-4262-94cf-f2b683fe305a", "node_type": "1", "metadata": {}, "hash": "edf155611751b953ca8d9cb940eadabef429bd3916bf6831b5912c2c23d6a8c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "RoboFlow: a Data-centric Work\ufb02ow Management\nSystem for Developing AI-enhanced Robots\nQinjie Lin\u2217, Guo Ye\u2217, Jiayi Wang, Han Liu\nDepartment of Computer Science, Northwestern University\nAbstract: We propose RoboFlow, a cloud-based work\ufb02ow management sys-\ntem orchestrating the pipelines of developing AI-enhanced robots. Unlike most\ntraditional robotic development processes that are essentially process-centric,\nRoboFlow is data-centric. This striking property makes it especially suitable for\ndeveloping AI-enhanced robots in which data play a central role. More speci\ufb01-\ncally, RoboFlow models the whole robotic development process into 4 building\nmodules (1. data processing, 2. algorithmic development, 3. back testing and\n4. application adaptation) interacting with a centralized data engine. All these\nbuilding modules are containerized and orchestrated under a uni\ufb01ed interfacing\nframework. Such an architectural design greatly increases the maintainability and\nre-usability of all the building modules and enables us to develop them in a fully\nparallel fashion. To demonstrate the ef\ufb01cacy of the developed system, we ex-\nploit it to develop two prototype systems named \u201cEgomobility\u201d and \u201cEgoplan\u201d.\nEgomobility provides general-purpose navigation functionalities for a wide vari-\nety of mobile robots and Egoplan solves path planning problems in high dimen-\nsional continuous state and action spaces for robot arms. Our result shows that\nRoboFlow can signi\ufb01cantly streamline the whole development lifecycle and the\nsame work\ufb02ow is applicable to numerous intelligent robotic applications2.\nKeywords: AI-enhanced robots, robotic development work\ufb02ow management,\ndata-centric development, cloud-based robotic development\n1 Introduction\nWe propose RoboFlow, a cloud-based work\ufb02ow management system for developing data-centric and\nAI-enhanced robots. This work is done in the context that signi\ufb01cant progresses have been made\nin robotics development and a paradigm shift from process-centric development to data-centric de-\nvelopment is being witnessed, especially for learning robots.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2083, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0a2ff6a-610b-4262-94cf-f2b683fe305a": {"__data__": {"id_": "a0a2ff6a-610b-4262-94cf-f2b683fe305a", "embedding": null, "metadata": {"page_label": "1", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d86b379-a6e5-4a09-b263-48cc07e41376", "node_type": "4", "metadata": {"page_label": "1", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "c9650401b492d58d2aca7c79574538e705ba9f46a5330b894ec59f06b022951a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec3d26ba-59eb-44db-9645-e2a3d6e429b5", "node_type": "1", "metadata": {"page_label": "1", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "7c4c43aefc84410ff42254f7325d1b0f7b2f409cdf8b54c0a79c8c0a62c49b35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Speci\ufb01cally, traditional robot develop-\nment work\ufb02ow [1, 2, 3, 4, 5, 6] is essentially process-centric, which emphasizes more on designing,\ndeveloping and integrating different \u201cprocessing modules\u201d interacting and inter-operating with each\nother in a complex fashion. Such a process-centric robotic development model, though natural for\nhumans, is not suitable for developing modern AI-enhanced robotic systems (aka., learning robots)\nthat are essentially data-centric [7, 8, 9, 10, 11, 12]. Some key reasons are that the development\nprocesses of AI-enhanced robots generally involve managing and interacting with massive amounts\nof data, and even after the systems have been deployed, continuous modi\ufb01cation and improvement\nare still needed when more data get acquired. Such an extra \u201cdata-centric\u201d dimension of learning\nrobots causes a dramatic increase in both coding complexity and maintaining complexity of the tra-\nditional process-centric robotic development work\ufb02ow, thus a new data-centric robotic development\nwork\ufb02ow is crucially is needed. To bridge this gap, we propose RoboFlow.\nA high-level overview of RoboFlow is illustrated in Figure 1. In the most abstract fashion, the\nRoboFlow system divides the whole pipeline of developing AI-enhanced robots into 4 building\nmodules (1. data processing, 2. algorithmic development, 3. back testing and 4. application adap-\ntation) interacting with a centralized data engine. Speci\ufb01cally, the data engine can be viewed as an\n\u201coracle\u201d that abstracts out all the data management details and is interacting (e.g., being queried\nor manipulated) with all the 4 building modules in an asynchronous fashion. Centered around the\n\u2217Authors contributed equally\n2Project site: https://sites.google.com/u.northwestern.edu/robo\ufb02ow\nBlue Sky Papers, 5th Conference on Robot Learning (CoRL 2021), London, UK.", "mimetype": "text/plain", "start_char_idx": 2084, "end_char_idx": 3929, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "59f99bc7-2b82-4a92-8818-a4ec813e631e": {"__data__": {"id_": "59f99bc7-2b82-4a92-8818-a4ec813e631e", "embedding": null, "metadata": {"page_label": "2", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9cd65c3-9bf8-4711-81fd-42e5a3549eb0", "node_type": "4", "metadata": {"page_label": "2", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "7f94571c244971009db5ccf097b21df4157dc87a2acc11770e4e081a3799be5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecda83d1-b0d6-472c-aa67-22b4b6216cba", "node_type": "1", "metadata": {}, "hash": "5b2d06a726b2d9ac7f63211ae35540e604ac30e57466b4e2e2fb79547077993b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "****\nFigure 1: An overview of RoboFlow. The robot development pipeline consists of 4 modules interacting with\na centralized data engine. The data engine manages the large-scale dataset, and publishes data from robot. The\ndata preprocess module encodes raw input to data that algorithm development module can easily parse. The\nalgorithms development module develops customized control policy. The back testing module tests the policy\nin various environments. The application adaption module deploys the learned policy in real world.\ndata engine, the 4 building modules follow an iterative spiral model. Unlike the classical spiral\nmodel[13] for software development, these building blocks mainly interact with data engine, thus\ncan be developed in a fully parallel fashion and each module could have different \u201cversions\u201d. Such a\ndata-centric design dramatically decreases the developing and maintaining complexities, making it\nespecially suitable for developing AI-enhanced robots. The framework in Figure 1 is quite generic\nand described in a fully abstract way. To understand this framework better, we put it in a concrete\ncontext of developing learning robots. From a learning perspective, the data processing module\ntransforms and encodes data into a state that other modules can easily parse. The algorithm module\nrepresents the planning policy of the robots, which is in charge of the robots\u2019 actions. The back\ntesting module evaluates the policy of the learning robot based on large-scale collected or simulated\ndata. The application adaptation module puts the developed policy into real-world environments\nonce the back testing meets a designed criterion. More details about these modules will be provided\nin Section 3.\nThis paper has 3 major contributions: (i) we proposed a novel data-centric robot development model\nwhich improves upon the traditional routines in terms of development \ufb02exibility and maintain-\nability. (ii) We implement a \ufb01rst prototype system using containerization techniques (speci\ufb01cally,\nDocker[14] and Kubernetes [15]). (iii) Using this system, we develop two platforms: Egomobil-\nity and Egoplan. Egomobility exploits deep reinforcement learning to provide navigation ability\nfor turtlebot and Egoplan uses imitation learning to solve complex motion planning problems for\nsawyer robots.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ecda83d1-b0d6-472c-aa67-22b4b6216cba": {"__data__": {"id_": "ecda83d1-b0d6-472c-aa67-22b4b6216cba", "embedding": null, "metadata": {"page_label": "2", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9cd65c3-9bf8-4711-81fd-42e5a3549eb0", "node_type": "4", "metadata": {"page_label": "2", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "7f94571c244971009db5ccf097b21df4157dc87a2acc11770e4e081a3799be5d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59f99bc7-2b82-4a92-8818-a4ec813e631e", "node_type": "1", "metadata": {"page_label": "2", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "a2dc84acc2ab181030922d3492da7419238570f2152ceccfe1a8d6476897371c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These studies demonstrate the ef\ufb01cacy of RoboFlow in various intelligent robotics\napplications. The rest of this paper is organized as below. Section 2 introduces related work. Sec-\ntion 3 describes implementation of RoboFlow. Section 4 presents two platforms to showcase the\nusefulness of RoboFlow.\n2 Related Work\nThe two most relevant lines of work related to RoboFlow are the work\ufb02ow management system\nand data management engine for data-centric robot systems. For the work\ufb02ow management system,\nearlier robot development pipelines try to simplify the work\ufb02ow of robotics development and re-\nduce re-programming efforts. For example, Fetch robotics[16] launches a Work\ufb02ow Builder which\nallows customers to design, implement, and redesign their own work\ufb02ows. But their available tools\nare not designed for large-scale dataset processing. In the machine learning \ufb01eld, frameworks like\nKube\ufb02ow[17] and MLFlow[18] have been developed to manage the work\ufb02ow of model develop-\nment but they are not designed for robot development. For the data management engine, researchers\nin bioinformatics community developed system tools to manage data work\ufb02ows in an end-to-end\nfashion[19, 20]. Nevertheless, in robotics, few works emphasize on the data-centric aspect of the\ndevelopment processes except some proposals related to cyber-physical systems which view robots\nas data-gathering nodes [21, 22, 23, 24, 25, 26, 21]. In recent work, Farzad et al. [27] proposed a\ncloud framework aiming to facilitate the development of IoT applications. but it is not straightfor-\nward on how to apply it to the more heavy-weighted data-centric robotic systems.\n2", "mimetype": "text/plain", "start_char_idx": 2320, "end_char_idx": 3960, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "917304a0-405e-422a-8adb-b5fc2ca91a4d": {"__data__": {"id_": "917304a0-405e-422a-8adb-b5fc2ca91a4d", "embedding": null, "metadata": {"page_label": "3", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e884046-6e61-4d08-a9ac-dfd68dc5aa65", "node_type": "4", "metadata": {"page_label": "3", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "40f697dcb8a0f4a19742820845e0d1032995158425fbf98a17b859f3a6c68496", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed4c610b-5d77-4c05-baec-75bd54659204", "node_type": "1", "metadata": {}, "hash": "85edba7302fd84e12375eb519b93d85cd8053df20bec9c79ca5a47e2dfaad891", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3 System Architecture and Software Implementation\nThe RoboFlow architecture builds upon the containerization and container orchestration techniques.\nMore speci\ufb01cally, a container platform (e.g., the Docker) packages applications so that they can\naccess a speci\ufb01c set of resources on a physical or virtual host. The main bene\ufb01t, especially for\ndevelopers, is that containers isolate different applications and are elastic, i.e., come and go as\ndemanded by need. This is particularly useful for developing massive robotic systems in which\ndevelopers may contribute code in different programming languages and application frameworks.\nIn such scenarios, we could exploit a container platform to establish many containers to isolate and\nmanage all the developed applications. To manage containers at scale, we can utilize a container\norchestrate system (e.g., Kubernetes, Docker Swarm) to automate the deployment, management,\nscaling, networking, and availability of all the containers. In the rest of this section, we describe the\nsystem architecture and software implementation of the RoboFlow system.\nSystem Architecture. Figure 2 illustrates the system architecture of RoboFlow. It has 4 essential\nmodules(1.Data Preprocess 2.Algorithm Development 3.Back Testing 4.Application Adaptation) in-\nteracting with a centralized data engine. Each module is employed into a containerized environment\nby bundling it together with all related con\ufb01guration \ufb01les, libraries and dependencies. These module\ncontainers run isolated processes on the system, thus enabling RoboFlow to be developed in a fully\nparallel fashion. In addition, any change in these module containers is recorded, making version\ncontrol easy to implement. These containers exploit DDS (Data Distribution Service) to manage\nreal-time communication between them and the data engine. DDS implements a publish-subscribe\npattern for sending and receiving data and each process running in RoboFlow are considered as\nDDS nodes to communicate data with other process. Also, a networked \ufb01lesystem named Glusterfs\nis also ultilized for sharing large-scale \ufb01les (e.g., large neural network models or training datasets)\nbetween modules. Such integration of DDS and Glusterfs make RoboFlow suitable for developing\ndata-driven methods on robotics.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed4c610b-5d77-4c05-baec-75bd54659204": {"__data__": {"id_": "ed4c610b-5d77-4c05-baec-75bd54659204", "embedding": null, "metadata": {"page_label": "3", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e884046-6e61-4d08-a9ac-dfd68dc5aa65", "node_type": "4", "metadata": {"page_label": "3", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "40f697dcb8a0f4a19742820845e0d1032995158425fbf98a17b859f3a6c68496", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "917304a0-405e-422a-8adb-b5fc2ca91a4d", "node_type": "1", "metadata": {"page_label": "3", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "0312ec8af41faa4b36bb91cf1323c70815bfc2dca7b4e5c65fc95c27199e42e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "RoboFlow also provides a web-based frontend to ease developers\nto monitor, analyze, and manage the robotic development process.\nversion1\nversion2\nversion3\nversion1\nversion2\nversion3\nversion1\nversion2\nversion3\nversion1\nversion2\nversion3\nGlusterfs\nsimulation\n real robot\ndataset\nmodel\n real robotreal robot\nTopic A Topic B Topic C Storage A Storage B Storage C\nData Preprocess Algorithm Development Back Testing Application Adaptation\nFigure 2: The architecture of RoboFlow. RoboFlow provides a graphical user interface for developers to\naccess all containerized modules. These modules interact with the data engine through shared storage and DDS\ntopics. The data engine consists of large-scale datasets and model data stored in the shared folder, simulation\ncontainers and robots connected to the RoboFlow system through DDS topics.\nSoftware Implementation. To implement RoboFlow, we wrap each module as a docker image.\nFor this, we specify the software environment (e.g., the operating system distribution and pre-\ninstalled packages) of an image in a Dockerfile. The obtained module images are installed with\njuypter lab and ROS2. Some modules may need additional packages. For example, the algorithm\ndevelopment module is equipped with the deep learning libraries tensorflow and pytorch, while\nthe data engine image utilizes several robot simulators like Gazebo, Stage and OpenRave.The suc-\ncessfully built images are stored in a cloud-based storage space namedcargo, which can be viewed\nas on-premise dockerhub of the RoboFlow system. During the run time, each instance of a built\nimage is deployed as a container and RoboFlow utilizes Kubernetes to deploy and manage these\ncontainers. We exploit the React javascript library to implement a graphical user interface (GUI).\nThrough this GUI users can choose desired versions of the docker images and allocate computational\nresources (e.g., CPUs or GPUs) to the container being created. Once such con\ufb01guration information\nis submitted, it is turned into a YAML \ufb01le which will be serialized and deployed by Kubernetes.\n3", "mimetype": "text/plain", "start_char_idx": 2293, "end_char_idx": 4363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "797fae54-f6e0-4047-9572-63fe0a9e6d01": {"__data__": {"id_": "797fae54-f6e0-4047-9572-63fe0a9e6d01", "embedding": null, "metadata": {"page_label": "4", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d34b3cb-958e-44cf-9a8e-dbdb09012c87", "node_type": "4", "metadata": {"page_label": "4", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "eafc39a0b64acfb57c26974222b06491859f643efb822a45506338a6f3f41873", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9198871-fcac-409e-8dda-a566b1d157ac", "node_type": "1", "metadata": {}, "hash": "3a3c099146178bcfa6afaaa7105316de793ecca979aa8aa18b5454d8eb2b0121", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 Two Case Studies and Performance Evalaution\nIn this section, we exploit RoboFlow to develop two prototype systems named \u201cEgomobility\u201d and\n\u201cEgoplan\u201d. Egomobility provides a general-purpose navigation platform for managing a wide vari-\nety of mobile robots and Egoplan is a motion planning platform for robot arms. To demonstrate the\nef\ufb01cacy of RoboFlow, we also conduct some performance analysis of the two case studies.\na. Process Module b. Algorithm Module\nc. Egomobility d. Egoplan\nFigure 3: Graphical User interface of RoboFlow. Figure a illustrates the edit page of preprocess module, which\nallows developers to specify the image name, version, required CPU number, memory size, and GPU number.\nFigure b illustrates the algorithm module, enabling users to access the code editor and simulator view, monitor\nthe progress of a learning robot training process. Figure c illustrates the Egomobility platform. This adaptation\npage provides real-time camera stream and current position, velocity and battery usage. Figure d illustrate the\nEgoplan, which provides the simulation views, rviz view and status panel.\nCase Study 1: The Egomobility platform for Mobile Robots. In this study, we exploit RoboFlow\nto develop a mobile robot navigation platform named Egomobility, which is a data-centric AI-\nenhanced robot system using data from a Stage simulator [28] as training data. Within this en-\nvironment, a mobile robot takes 3 raw laser frames and its velocity as input. Our goal is to train a\nreinforcement learning policy that outputs a velocity guiding the robot avoiding dynamic obstacles.\nThe deployment page of Egomobility is provided in Figure 3.\nCase Study 2: The Egoplan Platform for Arm Robots. In this study, we exploit RoboFlow to\ndevelop a robot arm motion planning Platform named EgoPlan. More speci\ufb01cally, we exploit the\nNEXT (Neural Exploration-Exploitation Trees) algorithm proposed in[29] to learn a motion plan\npolicy for solving path planning problems in 7-dimensional state space and 7-dimensional action\nspace.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2033, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9198871-fcac-409e-8dda-a566b1d157ac": {"__data__": {"id_": "b9198871-fcac-409e-8dda-a566b1d157ac", "embedding": null, "metadata": {"page_label": "4", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d34b3cb-958e-44cf-9a8e-dbdb09012c87", "node_type": "4", "metadata": {"page_label": "4", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "eafc39a0b64acfb57c26974222b06491859f643efb822a45506338a6f3f41873", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "797fae54-f6e0-4047-9572-63fe0a9e6d01", "node_type": "1", "metadata": {"page_label": "4", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "5fd21bae56fa7d3e2861f79d16d507220d5dc9dc7d46a72e11e790a803c066bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In each planning task, we simulate a robot arm and a shelf in theopenrave [30] simulator in\nour data engine. Each level of the shelf is horizontally divided into multiple bins. The task for the\nrobot arm is to plan a collision-free path from an initial pose to grab an object placed in a shelf. The\nmain strategy is to exploit BIT* [31] to solve the planning problem in a brutal force way but collect\nthe data to learn a smarter motion planning policy.\nPerformance. Through the above two case studies, the bene\ufb01t of RoboFlow for managing a data-\ncentric robotic system development pipeline is quite obvious: RoboFlow enables developers to de-\nvelop all the component modules in a fully-parallel fashion. For example, in developing Egomo-\nbility, algorithm developers can join in the RoboFlow to develop the navigation algorithm indepen-\ndently without interfering with each other. The developers for the application adaptation module can\nsimultaneously test many learned policies without the need of complex communication processes. In\naddition, due to the usage of sophisticated container-orchestration techniques, the resulting systems\ndeveloped by RoboFlow is much more reliable and maintainable.\nConclusion and Discussion. We propose RoboFlow, a data-centric work\ufb02ow management sys-\ntem orchestrating the pipelines of developing AI-enhanced robots. The data-centric features of\nRoboFlow illustrate high maintainability and re-usability of each module. We hope this data-centric\nmodel brings a new approach to the community.\n4", "mimetype": "text/plain", "start_char_idx": 2034, "end_char_idx": 3563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b76221d-f6e0-4b4b-9d12-5fdce7620aa1": {"__data__": {"id_": "5b76221d-f6e0-4b4b-9d12-5fdce7620aa1", "embedding": null, "metadata": {"page_label": "5", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35b475bd-e3f8-4fd9-bc50-96c6617246ef", "node_type": "4", "metadata": {"page_label": "5", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "6a6f570003895789ac9f03bcd1705a129227513aef56744037c0144ddc0fe3a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "529b2a6e-43e1-42c8-9be8-df971716cbc3", "node_type": "1", "metadata": {}, "hash": "0c95db72ad87b3233483ed78961d59b9c9833836242b97e3fc57b85925f8c592", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Acknowledgments\nHan Liu\u2019s research is supported by the NSF BIGDATA 1840866, NSF CAREER 1841569, NSF\nTRIPODS 1740735, DARPA-PA-18-02-09-QED-RML-FP-003, along with an Alfred P Sloan Fel-\nlowship and a PECASE award.\nReferences\n[1] R. Arumugam, V . R. Enti, L. Bingbing, W. Xiaojun, K. Baskaran, F. F. Kong, A. S. Kumar,\nK. D. Meng, and G. W. Kit. Davinci: A cloud computing framework for service robots. In2010\nIEEE international conference on robotics and automation, pages 3084\u20133089. IEEE, 2010.\n[2] R. Bouziane, L. S. Terrissa, S. Ayad, J.-F. Brethe, and O. Kazar. A web services based solution\nfor the nao robot in cloud robotics environment. In 2017 4th International Conference on\nControl, Decision and Information Technologies (CoDIT), pages 0809\u20130814. IEEE, 2017.\n[3] S. Kato, S. Tokunaga, Y . Maruyama, S. Maeda, M. Hirabayashi, Y . Kitsukawa, A. Monrroy,\nT. Ando, Y . Fujii, and T. Azumi. Autoware on board: Enabling autonomous vehicles with em-\nbedded systems. In 2018 ACM/IEEE 9th International Conference on Cyber-Physical Systems\n(ICCPS), pages 287\u2013296. IEEE, 2018.\n[4] Y . Lei, Z. Fengyu, W. Yugang, Y . Xianfeng, Z. Yang, and C. Zhumin. Design of a cloud\nrobotics visual platform. In 2016 Sixth International Conference on Instrumentation & Mea-\nsurement, Computer, Communication and Control (IMCCC), pages 1039\u20131043. IEEE, 2016.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1342, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "529b2a6e-43e1-42c8-9be8-df971716cbc3": {"__data__": {"id_": "529b2a6e-43e1-42c8-9be8-df971716cbc3", "embedding": null, "metadata": {"page_label": "5", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35b475bd-e3f8-4fd9-bc50-96c6617246ef", "node_type": "4", "metadata": {"page_label": "5", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "6a6f570003895789ac9f03bcd1705a129227513aef56744037c0144ddc0fe3a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b76221d-f6e0-4b4b-9d12-5fdce7620aa1", "node_type": "1", "metadata": {"page_label": "5", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "08f98188f6215bf7c7562de3d4a5f4911c95e303d8b9f19162a97e084b6f43cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af4fd4c8-c17c-45d9-9b9d-73a30271553a", "node_type": "1", "metadata": {}, "hash": "0addba35391cf0888b032652bb3db568e0f367a64ddcd6e3d1f3d9a6298a50fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE, 2016.\n[5] S. Dennis, L. Alex, L. Matthias, and S. Christian. The smartmdsd toolchain: An integrated\nmdsd work\ufb02ow and integrated development environment (ide) for robotics software. 2016.\n[6] G. Huang, P. S. Rao, M.-H. Wu, X. Qian, S. Y . Nof, K. Ramani, and A. J. Quinn. Vipo: Spatial-\nvisual programming with functions for robot-iot work\ufb02ows. In Proceedings of the 2020 CHI\nConference on Human Factors in Computing Systems, pages 1\u201313, 2020.\n[7] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine. One-shot visual imitation learning via\nmeta-learning. arXiv preprint arXiv:1709.04905, 2017.\n[8] S. Gu, E. Holly, T. Lillicrap, and S. Levine. Deep reinforcement learning for robotic manipula-\ntion with asynchronous off-policy updates. In 2017 IEEE international conference on robotics\nand automation (ICRA), pages 3389\u20133396. IEEE, 2017.\n[9] H. Kretzschmar, M. Spies, C. Sprunk, and W. Burgard. Socially compliant mobile robot navi-\ngation via inverse reinforcement learning. The International Journal of Robotics Research, 35\n(11):1289\u20131307, 2016.\n[10] A. Zeng, S. Song, S. Welker, J. Lee, A. Rodriguez, and T. Funkhouser. Learning synergies\nbetween pushing and grasping with self-supervised deep reinforcement learning. In 2018\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4238\u2013\n4245. IEEE, 2018.\n[11] F. Ebert, C. Finn, S. Dasari, A. Xie, A. Lee, and S. Levine. Visual foresight: Model-based deep\nreinforcement learning for vision-based robotic control.", "mimetype": "text/plain", "start_char_idx": 1331, "end_char_idx": 2829, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af4fd4c8-c17c-45d9-9b9d-73a30271553a": {"__data__": {"id_": "af4fd4c8-c17c-45d9-9b9d-73a30271553a", "embedding": null, "metadata": {"page_label": "5", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35b475bd-e3f8-4fd9-bc50-96c6617246ef", "node_type": "4", "metadata": {"page_label": "5", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "6a6f570003895789ac9f03bcd1705a129227513aef56744037c0144ddc0fe3a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "529b2a6e-43e1-42c8-9be8-df971716cbc3", "node_type": "1", "metadata": {"page_label": "5", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "827293358757e65a0d389fc31ce0fc502428c03d29a62393eefa202f00cc6a77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Visual foresight: Model-based deep\nreinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568,\n2018.\n[12] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg. Dex-\nnet 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp\nmetrics. arXiv preprint arXiv:1703.09312, 2017.\n[13] B. W. Boehm. A spiral model of software development and enhancement. Computer, 21(5):\n61\u201372, 1988.\n[14] Docker. https://www.docker.com/.\n[15] Kubernetes. https://kubernetes.io/.\n[16] F. Robotics. Fetch research robot.\n5", "mimetype": "text/plain", "start_char_idx": 2738, "end_char_idx": 3331, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "66a98e4a-95d5-46e4-af7f-f234f5fcac99": {"__data__": {"id_": "66a98e4a-95d5-46e4-af7f-f234f5fcac99", "embedding": null, "metadata": {"page_label": "6", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e384ff7-5141-423a-bc92-69856bab59e5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "a793d2ca4da74c7f54aa510cadffb19aa34a5f251521201b04184d678f40d374", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf687506-631b-4419-9f9c-ef3f37a164c8", "node_type": "1", "metadata": {}, "hash": "f7f173fdbcd50622270fbd49a5efe09783413c0133af60db34b5c1091c3d7102", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[17] Kube\ufb02ow. https://www.kubeflow.org/.\n[18] Ml\ufb02ow. https://mlflow.org/.\n[19] A. M. Comeau, G. M. Douglas, and M. G. Langille. Microbiome helper: a custom and stream-\nlined work\ufb02ow for microbiome research. MSystems, 2(1), 2017.\n[20] T. W. Backman and T. Girke. systempiper: Ngs work\ufb02ow and report generation environment.\nBMC bioinformatics, 17(1):388, 2016.\n[21] P. Gil, I. Maza, A. Ollero, and P. Marr \u00b4on. Data centric middleware for the integration of\nwireless sensor networks and mobile robots. In proc. 7th Conference on Mobile Robots and\nCompetitions, ROBOTICA. Citeseer, 2007.\n[22] K. R \u00a8omer, O. Kasten, and F. Mattern. Middleware challenges for wireless sensor networks.\nACM SIGMOBILE Mobile Computing and Communications Review, 6(4):59\u201361, 2002.\n[23] Y . Yu, B. Krishnamachari, and V . K. Prasanna. Issues in designing middleware for wireless\nsensor networks. IEEE network, 18(1):15\u201321, 2004.\n[24] T. Abdelzaher, B. Blum, Q. Cao, Y . Chen, D. Evans, J. George, S. George, L. Gu, T. He, S. Kr-\nishnamurthy, et al. Envirotrack: Towards an environmental computing paradigm for distributed\nsensor networks. In 24th International Conference on Distributed Computing Systems, 2004.\nProceedings., pages 582\u2013589. IEEE, 2004.\n[25] S. L. Remy and M. B. Blake. Distributed service-oriented robotics. IEEE Internet Computing,\n15(2):70\u201374, 2011.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf687506-631b-4419-9f9c-ef3f37a164c8": {"__data__": {"id_": "cf687506-631b-4419-9f9c-ef3f37a164c8", "embedding": null, "metadata": {"page_label": "6", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e384ff7-5141-423a-bc92-69856bab59e5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "a793d2ca4da74c7f54aa510cadffb19aa34a5f251521201b04184d678f40d374", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66a98e4a-95d5-46e4-af7f-f234f5fcac99", "node_type": "1", "metadata": {"page_label": "6", "file_name": "roboflow.pdf", "file_path": "/Users/betopia/LlamaIndex/data/roboflow.pdf", "file_type": "application/pdf", "file_size": 654293, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "74bd7dbcee11a74195327547fc6fc295b8ecd6a9678cca9b4459e4fd516359ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Internet Computing,\n15(2):70\u201374, 2011.\n[26] N. Mohamed, J. Al-Jaroodi, and I. Jawhar. Middleware for robotics: A survey. In 2008 IEEE\nConference on Robotics, Automation and Mechatronics, pages 736\u2013742. Ieee, 2008.\n[27] F. Khodadadi, R. N. Calheiros, and R. Buyya. A data-centric framework for development and\ndeployment of internet of things applications in clouds. In 2015 IEEE Tenth International\nConference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP),\npages 1\u20136. IEEE, 2015.\n[28] R. Vaughan. Massively multi-robot simulation in stage. Swarm intelligence, 2(2-4):189\u2013208,\n2008.\n[29] B. Chen, B. Dai, Q. Lin, G. Ye, H. Liu, and L. Song. Learning to plan in high dimensions\nvia neural exploration-exploitation trees. In International Conference on Learning Represen-\ntations, 2020. URL https://openreview.net/forum?id=rJgJDAVKvB.\n[30] R. Diankov and J. Kuffner. Openrave: A planning architecture for autonomous robotics.\nRobotics Institute, Pittsburgh, PA, Tech. Rep. CMU-RI-TR-08-34, 79, 2008.\n[31] J. D. Gammell, S. S. Srinivasa, and T. D. Barfoot. Bit*: Batch informed trees for optimal\nsampling-based planning via dynamic programming on implicit random geometric graphs.\narXiv preprint arXiv:1405.5848, 2014.\n6", "mimetype": "text/plain", "start_char_idx": 1300, "end_char_idx": 2551, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "586221dd-d100-4885-9d2a-5ca9b79b131b": {"__data__": {"id_": "586221dd-d100-4885-9d2a-5ca9b79b131b", "embedding": null, "metadata": {"page_label": "1", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d4a0209-60cc-472a-82c6-62d405bb3988", "node_type": "4", "metadata": {"page_label": "1", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "07dd6e8181b64c66e749e057febd615598b4ca9e91c0f48f8045279d4091c24d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d14f7d6-1c9e-4a57-ad87-6f38afdbb2ed", "node_type": "1", "metadata": {}, "hash": "7ffd43de15f28aee31289a0b3e9e85882f916da383cb714e8acac99686663f51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You Only Look Once:\nUni\ufb01ed, Real-Time Object Detection\nJoseph Redmon\u2217, Santosh Divvala\u2217\u2020, Ross Girshick\u00b6, Ali Farhadi\u2217\u2020\nUniversity of Washington\u2217, Allen Institute for AI\u2020, Facebook AI Research\u00b6\nhttp://pjreddie.com/yolo/\nAbstract\nWe present YOLO, a new approach to object detection.\nPrior work on object detection repurposes classi\ufb01ers to per-\nform detection. Instead, we frame object detection as a re-\ngression problem to spatially separated bounding boxes and\nassociated class probabilities. A single neural network pre-\ndicts bounding boxes and class probabilities directly from\nfull images in one evaluation. Since the whole detection\npipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance.\nOur uni\ufb01ed architecture is extremely fast. Our base\nYOLO model processes images in real-time at 45 frames\nper second. A smaller version of the network, Fast YOLO,\nprocesses an astounding 155 frames per second while\nstill achieving double the mAP of other real-time detec-\ntors. Compared to state-of-the-art detection systems, YOLO\nmakes more localization errors but is less likely to predict\nfalse positives on background. Finally, YOLO learns very\ngeneral representations of objects. It outperforms other de-\ntection methods, including DPM and R-CNN, when gener-\nalizing from natural images to other domains like artwork.\n1. Introduction\nHumans glance at an image and instantly know what ob-\njects are in the image, where they are, and how they inter-\nact. The human visual system is fast and accurate, allow-\ning us to perform complex tasks like driving with little con-\nscious thought. Fast, accurate algorithms for object detec-\ntion would allow computers to drive cars without special-\nized sensors, enable assistive devices to convey real-time\nscene information to human users, and unlock the potential\nfor general purpose, responsive robotic systems.\nCurrent detection systems repurpose classi\ufb01ers to per-\nform detection. To detect an object, these systems take a\nclassi\ufb01er for that object and evaluate it at various locations\nand scales in a test image.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2096, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d14f7d6-1c9e-4a57-ad87-6f38afdbb2ed": {"__data__": {"id_": "8d14f7d6-1c9e-4a57-ad87-6f38afdbb2ed", "embedding": null, "metadata": {"page_label": "1", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d4a0209-60cc-472a-82c6-62d405bb3988", "node_type": "4", "metadata": {"page_label": "1", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "07dd6e8181b64c66e749e057febd615598b4ca9e91c0f48f8045279d4091c24d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "586221dd-d100-4885-9d2a-5ca9b79b131b", "node_type": "1", "metadata": {"page_label": "1", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "bee77c4028cc1bec5273a7c33909b88ea537f089258f00a0f695e630e0a88030", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd63ddcd-3581-43d2-b28c-ab98caf2ce09", "node_type": "1", "metadata": {}, "hash": "dbd2bebe027ea1d92fd2243892463440496832aa67089edbe75f4630723ade32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Systems like deformable parts\nmodels (DPM) use a sliding window approach where the\nclassi\ufb01er is run at evenly spaced locations over the entire\nimage [10].\nMore recent approaches like R-CNN use region proposal\n1. Resize image.\n2. Run convolutional network.\n3. Non-max suppression.\nDog: 0.30\nPerson: 0.64\nHorse: 0.28\nFigure 1: The YOLO Detection System. Processing images\nwith YOLO is simple and straightforward. Our system (1) resizes\nthe input image to 448 \u00d7 448, (2) runs a single convolutional net-\nwork on the image, and (3) thresholds the resulting detections by\nthe model\u2019s con\ufb01dence.\nmethods to \ufb01rst generate potential bounding boxes in an im-\nage and then run a classi\ufb01er on these proposed boxes. After\nclassi\ufb01cation, post-processing is used to re\ufb01ne the bound-\ning boxes, eliminate duplicate detections, and rescore the\nboxes based on other objects in the scene [13]. These com-\nplex pipelines are slow and hard to optimize because each\nindividual component must be trained separately.\nWe reframe object detection as a single regression prob-\nlem, straight from image pixels to bounding box coordi-\nnates and class probabilities. Using our system, you only\nlook once (YOLO) at an image to predict what objects are\npresent and where they are.\nYOLO is refreshingly simple: see Figure 1. A sin-\ngle convolutional network simultaneously predicts multi-\nple bounding boxes and class probabilities for those boxes.\nYOLO trains on full images and directly optimizes detec-\ntion performance. This uni\ufb01ed model has several bene\ufb01ts\nover traditional methods of object detection.\nFirst, YOLO is extremely fast. Since we frame detection\nas a regression problem we don\u2019t need a complex pipeline.\nWe simply run our neural network on a new image at test\ntime to predict detections. Our base network runs at 45\nframes per second with no batch processing on a Titan X\nGPU and a fast version runs at more than 150 fps.", "mimetype": "text/plain", "start_char_idx": 2097, "end_char_idx": 4004, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd63ddcd-3581-43d2-b28c-ab98caf2ce09": {"__data__": {"id_": "bd63ddcd-3581-43d2-b28c-ab98caf2ce09", "embedding": null, "metadata": {"page_label": "1", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d4a0209-60cc-472a-82c6-62d405bb3988", "node_type": "4", "metadata": {"page_label": "1", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "07dd6e8181b64c66e749e057febd615598b4ca9e91c0f48f8045279d4091c24d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d14f7d6-1c9e-4a57-ad87-6f38afdbb2ed", "node_type": "1", "metadata": {"page_label": "1", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "587991b271f8f96c1a19e9553aacb9a79a7f28b9f6b3f0a08ae04aa6f46ff200", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This\nmeans we can process streaming video in real-time with\nless than 25 milliseconds of latency. Furthermore, YOLO\nachieves more than twice the mean average precision of\nother real-time systems. For a demo of our system running\nin real-time on a webcam please see our project webpage:\nhttp://pjreddie.com/yolo/.\nSecond, YOLO reasons globally about the image when\n1\narXiv:1506.02640v5  [cs.CV]  9 May 2016", "mimetype": "text/plain", "start_char_idx": 4005, "end_char_idx": 4410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1d4b10e-2d7c-43cf-82ba-939e6582616c": {"__data__": {"id_": "e1d4b10e-2d7c-43cf-82ba-939e6582616c", "embedding": null, "metadata": {"page_label": "2", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a4e0c2a-841d-4c8e-8900-45c38addbff8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "e9b2577ef9d62b142c6b84fdf08d0854861b439cfcdfcafe280c33ea0f38d790", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8800e22-7291-4f96-88f5-351af77e1ba5", "node_type": "1", "metadata": {}, "hash": "e7b1ac385953a401e928312bc23c2a914672cab69f69fd6da32221b71e9659d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "making predictions. Unlike sliding window and region\nproposal-based techniques, YOLO sees the entire image\nduring training and test time so it implicitly encodes contex-\ntual information about classes as well as their appearance.\nFast R-CNN, a top detection method [14], mistakes back-\nground patches in an image for objects because it can\u2019t see\nthe larger context. YOLO makes less than half the number\nof background errors compared to Fast R-CNN.\nThird, YOLO learns generalizable representations of ob-\njects. When trained on natural images and tested on art-\nwork, YOLO outperforms top detection methods like DPM\nand R-CNN by a wide margin. Since YOLO is highly gen-\neralizable it is less likely to break down when applied to\nnew domains or unexpected inputs.\nYOLO still lags behind state-of-the-art detection systems\nin accuracy. While it can quickly identify objects in im-\nages it struggles to precisely localize some objects, espe-\ncially small ones. We examine these tradeoffs further in our\nexperiments.\nAll of our training and testing code is open source. A\nvariety of pretrained models are also available to download.\n2. Uni\ufb01ed Detection\nWe unify the separate components of object detection\ninto a single neural network. Our network uses features\nfrom the entire image to predict each bounding box. It also\npredicts all bounding boxes across all classes for an im-\nage simultaneously. This means our network reasons glob-\nally about the full image and all the objects in the image.\nThe YOLO design enables end-to-end training and real-\ntime speeds while maintaining high average precision.\nOur system divides the input image into an S\u00d7S grid.\nIf the center of an object falls into a grid cell, that grid cell\nis responsible for detecting that object.\nEach grid cell predictsBbounding boxes and con\ufb01dence\nscores for those boxes. These con\ufb01dence scores re\ufb02ect how\ncon\ufb01dent the model is that the box contains an object and\nalso how accurate it thinks the box is that it predicts. For-\nmally we de\ufb01ne con\ufb01dence as Pr(Object) \u2217IOUtruth\npred .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2047, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d8800e22-7291-4f96-88f5-351af77e1ba5": {"__data__": {"id_": "d8800e22-7291-4f96-88f5-351af77e1ba5", "embedding": null, "metadata": {"page_label": "2", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a4e0c2a-841d-4c8e-8900-45c38addbff8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "e9b2577ef9d62b142c6b84fdf08d0854861b439cfcdfcafe280c33ea0f38d790", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1d4b10e-2d7c-43cf-82ba-939e6582616c", "node_type": "1", "metadata": {"page_label": "2", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "dc5b11467618df8429fb7434b9f88ae0cbfd214dd7f905a8d384f6eeff033236", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "774e0607-d24b-44b1-ac79-517ff73dde79", "node_type": "1", "metadata": {}, "hash": "95131c60a8c2bdd6aa461d9c3d51e954ba3c21b29b85a208ca90279084ebcb47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If no\nobject exists in that cell, the con\ufb01dence scores should be\nzero. Otherwise we want the con\ufb01dence score to equal the\nintersection over union (IOU) between the predicted box\nand the ground truth.\nEach bounding box consists of 5 predictions: x, y, w, h,\nand con\ufb01dence. The (x,y) coordinates represent the center\nof the box relative to the bounds of the grid cell. The width\nand height are predicted relative to the whole image. Finally\nthe con\ufb01dence prediction represents the IOU between the\npredicted box and any ground truth box.\nEach grid cell also predicts C conditional class proba-\nbilities, Pr(Classi|Object). These probabilities are condi-\ntioned on the grid cell containing an object. We only predict\none set of class probabilities per grid cell, regardless of the\nnumber of boxes B.\nAt test time we multiply the conditional class probabili-\nties and the individual box con\ufb01dence predictions,\nPr(Classi|Object) \u2217Pr(Object) \u2217IOUtruth\npred = Pr(Classi) \u2217IOUtruth\npred (1)\nwhich gives us class-speci\ufb01c con\ufb01dence scores for each\nbox. These scores encode both the probability of that class\nappearing in the box and how well the predicted box \ufb01ts the\nobject.\nS \u00d7 S grid on input\nBounding boxes + confidence\nClass probability map\nFinal detections\nFigure 2: The Model. Our system models detection as a regres-\nsion problem. It divides the image into an S \u00d7 S grid and for each\ngrid cell predicts B bounding boxes, con\ufb01dence for those boxes,\nand C class probabilities. These predictions are encoded as an\nS \u00d7 S \u00d7 (B \u2217 5 +C) tensor.\nFor evaluating YOLO on PASCAL VOC, we use S = 7,\nB = 2. P ASCAL VOC has 20 labelled classes so C = 20.\nOur \ufb01nal prediction is a 7 \u00d77 \u00d730 tensor.\n2.1.", "mimetype": "text/plain", "start_char_idx": 2048, "end_char_idx": 3732, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "774e0607-d24b-44b1-ac79-517ff73dde79": {"__data__": {"id_": "774e0607-d24b-44b1-ac79-517ff73dde79", "embedding": null, "metadata": {"page_label": "2", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a4e0c2a-841d-4c8e-8900-45c38addbff8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "e9b2577ef9d62b142c6b84fdf08d0854861b439cfcdfcafe280c33ea0f38d790", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8800e22-7291-4f96-88f5-351af77e1ba5", "node_type": "1", "metadata": {"page_label": "2", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "b8a59abde89fae5afd4643ca045ef6cd116814365ee2008035eae1a262118ca3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Our \ufb01nal prediction is a 7 \u00d77 \u00d730 tensor.\n2.1. Network Design\nWe implement this model as a convolutional neural net-\nwork and evaluate it on the PASCAL VOC detection dataset\n[9]. The initial convolutional layers of the network extract\nfeatures from the image while the fully connected layers\npredict the output probabilities and coordinates.\nOur network architecture is inspired by the GoogLeNet\nmodel for image classi\ufb01cation [34]. Our network has 24\nconvolutional layers followed by 2 fully connected layers.\nInstead of the inception modules used by GoogLeNet, we\nsimply use 1 \u00d71 reduction layers followed by 3 \u00d73 convo-\nlutional layers, similar to Lin et al [22]. The full network is\nshown in Figure 3.\nWe also train a fast version of YOLO designed to push\nthe boundaries of fast object detection. Fast YOLO uses a\nneural network with fewer convolutional layers (9 instead\nof 24) and fewer \ufb01lters in those layers. Other than the size\nof the network, all training and testing parameters are the\nsame between YOLO and Fast YOLO.", "mimetype": "text/plain", "start_char_idx": 3686, "end_char_idx": 4714, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e70a1c3e-0601-4cd1-992d-eeded998e664": {"__data__": {"id_": "e70a1c3e-0601-4cd1-992d-eeded998e664", "embedding": null, "metadata": {"page_label": "3", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c7c4363-4a3c-40ca-8522-72ade53f0968", "node_type": "4", "metadata": {"page_label": "3", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "a65f74bf68e4b224d1297ec29dce04c4272f87515fcc25ad5d0430b8272b5b9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87defc6e-a0a2-4b5c-9966-74a1f8154c97", "node_type": "1", "metadata": {}, "hash": "e1bf0bb02888ed2895b90c27f08eb8527f3447ea7122b7b76d0af21583a6673a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "448\n448\n3\n7\n7\nConv. Layer\n7x7x64-s-2\nMaxpool Layer\n2x2-s-2\n3\n3\n112\n112\n192\n3\n3\n56\n56\n256\nConn. Layer\n4096\nConn. LayerConv. Layer\n3x3x192\nMaxpool Layer\n2x2-s-2\nConv. Layers\n1x1x128\n3x3x256\n1x1x256\n3x3x512\nMaxpool Layer\n2x2-s-2\n3\n3\n28\n28\n512\nConv. Layers\n1x1x256\n3x3x512\n1x1x512\n3x3x1024\nMaxpool Layer\n2x2-s-2\n3\n3\n14\n14\n1024\nConv. Layers\n1x1x512\n3x3x1024\n3x3x1024\n3x3x1024-s-2\n3\n3\n7\n7\n1024\n7\n7\n1024\n7\n7\n30\n} \u00d74 } \u00d72\nConv. Layers\n3x3x1024\n3x3x1024\nFigure 3: The Architecture. Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating 1 \u00d7 1\nconvolutional layers reduce the features space from preceding layers. We pretrain the convolutional layers on the ImageNet classi\ufb01cation\ntask at half the resolution (224 \u00d7 224 input image) and then double the resolution for detection.\nThe \ufb01nal output of our network is the 7 \u00d77 \u00d730 tensor\nof predictions.\n2.2. Training\nWe pretrain our convolutional layers on the ImageNet\n1000-class competition dataset [30]. For pretraining we use\nthe \ufb01rst 20 convolutional layers from Figure 3 followed by a\naverage-pooling layer and a fully connected layer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1122, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87defc6e-a0a2-4b5c-9966-74a1f8154c97": {"__data__": {"id_": "87defc6e-a0a2-4b5c-9966-74a1f8154c97", "embedding": null, "metadata": {"page_label": "3", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c7c4363-4a3c-40ca-8522-72ade53f0968", "node_type": "4", "metadata": {"page_label": "3", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "a65f74bf68e4b224d1297ec29dce04c4272f87515fcc25ad5d0430b8272b5b9c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e70a1c3e-0601-4cd1-992d-eeded998e664", "node_type": "1", "metadata": {"page_label": "3", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "228d301939daeab5982ff578e210994ecc97455e3c4382b842e50445631f37d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11443c17-7442-4c57-baea-779e91715b1b", "node_type": "1", "metadata": {}, "hash": "5bb24e25ed9a19b4cdd0e59a37c8301170ec0e31c1e8ce472784ec6becbea9bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We train\nthis network for approximately a week and achieve a single\ncrop top-5 accuracy of 88% on the ImageNet 2012 valida-\ntion set, comparable to the GoogLeNet models in Caffe\u2019s\nModel Zoo [24]. We use the Darknet framework for all\ntraining and inference [26].\nWe then convert the model to perform detection. Ren et\nal. show that adding both convolutional and connected lay-\ners to pretrained networks can improve performance [29].\nFollowing their example, we add four convolutional lay-\ners and two fully connected layers with randomly initialized\nweights. Detection often requires \ufb01ne-grained visual infor-\nmation so we increase the input resolution of the network\nfrom 224 \u00d7224 to 448 \u00d7448.\nOur \ufb01nal layer predicts both class probabilities and\nbounding box coordinates. We normalize the bounding box\nwidth and height by the image width and height so that they\nfall between 0 and 1. We parametrize the bounding box x\nand ycoordinates to be offsets of a particular grid cell loca-\ntion so they are also bounded between 0 and 1.\nWe use a linear activation function for the \ufb01nal layer and\nall other layers use the following leaky recti\ufb01ed linear acti-\nvation:\n\u03c6(x) =\n{\nx, if x> 0\n0.1x, otherwise (2)\nWe optimize for sum-squared error in the output of our\nmodel. We use sum-squared error because it is easy to op-\ntimize, however it does not perfectly align with our goal of\nmaximizing average precision. It weights localization er-\nror equally with classi\ufb01cation error which may not be ideal.\nAlso, in every image many grid cells do not contain any\nobject. This pushes the \u201ccon\ufb01dence\u201d scores of those cells\ntowards zero, often overpowering the gradient from cells\nthat do contain objects. This can lead to model instability,\ncausing training to diverge early on.\nTo remedy this, we increase the loss from bounding box\ncoordinate predictions and decrease the loss from con\ufb01-\ndence predictions for boxes that don\u2019t contain objects. We\nuse two parameters, \u03bbcoord and \u03bbnoobj to accomplish this.", "mimetype": "text/plain", "start_char_idx": 1123, "end_char_idx": 3113, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11443c17-7442-4c57-baea-779e91715b1b": {"__data__": {"id_": "11443c17-7442-4c57-baea-779e91715b1b", "embedding": null, "metadata": {"page_label": "3", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c7c4363-4a3c-40ca-8522-72ade53f0968", "node_type": "4", "metadata": {"page_label": "3", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "a65f74bf68e4b224d1297ec29dce04c4272f87515fcc25ad5d0430b8272b5b9c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87defc6e-a0a2-4b5c-9966-74a1f8154c97", "node_type": "1", "metadata": {"page_label": "3", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "2b2ed26493c0a27991549ffbccfa55396a0901fe94edb057370ee4f7bd31c56c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We\nuse two parameters, \u03bbcoord and \u03bbnoobj to accomplish this. We\nset \u03bbcoord = 5and \u03bbnoobj = .5.\nSum-squared error also equally weights errors in large\nboxes and small boxes. Our error metric should re\ufb02ect that\nsmall deviations in large boxes matter less than in small\nboxes. To partially address this we predict the square root\nof the bounding box width and height instead of the width\nand height directly.\nYOLO predicts multiple bounding boxes per grid cell.\nAt training time we only want one bounding box predictor\nto be responsible for each object. We assign one predictor\nto be \u201cresponsible\u201d for predicting an object based on which\nprediction has the highest current IOU with the ground\ntruth. This leads to specialization between the bounding box\npredictors. Each predictor gets better at predicting certain\nsizes, aspect ratios, or classes of object, improving overall\nrecall.\nDuring training we optimize the following, multi-part", "mimetype": "text/plain", "start_char_idx": 3053, "end_char_idx": 3988, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d8bb0ad-4d0e-45f3-a78f-fe083ad7958d": {"__data__": {"id_": "1d8bb0ad-4d0e-45f3-a78f-fe083ad7958d", "embedding": null, "metadata": {"page_label": "4", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "707b5854-4e98-4c49-bb14-5c58767834b9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "737f3598b5566cd72646a258e683c4f78cf1ac6f603fa7cd9fa2daccdba180d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a285257-a731-4a55-adcb-7a900738db4e", "node_type": "1", "metadata": {}, "hash": "90fc0b8f7770a9a2fcadb575b43cb4151d5826e8893ac2d074e43f73c11dce71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "loss function:\n\u03bbcoord\nS2\n\u2211\ni=0\nB\u2211\nj=0\n1 obj\nij\n[\n(xi \u2212\u02c6xi)2 + (yi \u2212\u02c6yi)2\n]\n+ \u03bbcoord\nS2\n\u2211\ni=0\nB\u2211\nj=0\n1 obj\nij\n[(\u221awi \u2212\n\u221a\n\u02c6wi\n)2\n+\n(\u221a\nhi \u2212\n\u221a\n\u02c6hi\n)2]\n+\nS2\n\u2211\ni=0\nB\u2211\nj=0\n1 obj\nij\n(\nCi \u2212\u02c6Ci\n)2\n+ \u03bbnoobj\nS2\n\u2211\ni=0\nB\u2211\nj=0\n1 noobj\nij\n(\nCi \u2212\u02c6Ci\n)2\n+\nS2\n\u2211\ni=0\n1 obj\ni\n\u2211\nc\u2208classes\n(pi(c) \u2212\u02c6pi(c))2 (3)\nwhere 1 obj\ni denotes if object appears in cell i and 1 obj\nij de-\nnotes that the jth bounding box predictor in cell i is \u201cre-\nsponsible\u201d for that prediction.\nNote that the loss function only penalizes classi\ufb01cation\nerror if an object is present in that grid cell (hence the con-\nditional class probability discussed earlier). It also only pe-\nnalizes bounding box coordinate error if that predictor is\n\u201cresponsible\u201d for the ground truth box (i.e. has the highest\nIOU of any predictor in that grid cell).\nWe train the network for about 135 epochs on the train-\ning and validation data sets from P ASCAL VOC 2007 and\n2012. When testing on 2012 we also include the VOC 2007\ntest data for training. Throughout training we use a batch\nsize of 64, a momentum of 0.9 and a decay of 0.0005.\nOur learning rate schedule is as follows: For the \ufb01rst\nepochs we slowly raise the learning rate from10\u22123 to 10\u22122.\nIf we start at a high learning rate our model often diverges\ndue to unstable gradients.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a285257-a731-4a55-adcb-7a900738db4e": {"__data__": {"id_": "2a285257-a731-4a55-adcb-7a900738db4e", "embedding": null, "metadata": {"page_label": "4", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "707b5854-4e98-4c49-bb14-5c58767834b9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "737f3598b5566cd72646a258e683c4f78cf1ac6f603fa7cd9fa2daccdba180d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d8bb0ad-4d0e-45f3-a78f-fe083ad7958d", "node_type": "1", "metadata": {"page_label": "4", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "ab0925322db907902e494bc4003f007b3aca60201a0fc18550757c6302a90de8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dcf26f26-1430-4911-993e-4c88f18121e6", "node_type": "1", "metadata": {}, "hash": "550d19c938833a71ce7efb44332b9063ae3efd840dde79e06f64ae2d6d1302bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If we start at a high learning rate our model often diverges\ndue to unstable gradients. We continue training with 10\u22122\nfor 75 epochs, then 10\u22123 for 30 epochs, and \ufb01nally 10\u22124\nfor 30 epochs.\nTo avoid over\ufb01tting we use dropout and extensive data\naugmentation. A dropout layer with rate = .5 after the \ufb01rst\nconnected layer prevents co-adaptation between layers [18].\nFor data augmentation we introduce random scaling and\ntranslations of up to 20% of the original image size. We\nalso randomly adjust the exposure and saturation of the im-\nage by up to a factor of 1.5 in the HSV color space.\n2.3. Inference\nJust like in training, predicting detections for a test image\nonly requires one network evaluation. On PASCAL VOC the\nnetwork predicts 98 bounding boxes per image and class\nprobabilities for each box. YOLO is extremely fast at test\ntime since it only requires a single network evaluation, un-\nlike classi\ufb01er-based methods.\nThe grid design enforces spatial diversity in the bound-\ning box predictions. Often it is clear which grid cell an\nobject falls in to and the network only predicts one box for\neach object. However, some large objects or objects near\nthe border of multiple cells can be well localized by multi-\nple cells. Non-maximal suppression can be used to \ufb01x these\nmultiple detections. While not critical to performance as it\nis for R-CNN or DPM, non-maximal suppression adds 2-\n3% in mAP.\n2.4. Limitations of YOLO\nYOLO imposes strong spatial constraints on bounding\nbox predictions since each grid cell only predicts two boxes\nand can only have one class. This spatial constraint lim-\nits the number of nearby objects that our model can pre-\ndict. Our model struggles with small objects that appear in\ngroups, such as \ufb02ocks of birds.\nSince our model learns to predict bounding boxes from\ndata, it struggles to generalize to objects in new or unusual\naspect ratios or con\ufb01gurations.", "mimetype": "text/plain", "start_char_idx": 1185, "end_char_idx": 3081, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dcf26f26-1430-4911-993e-4c88f18121e6": {"__data__": {"id_": "dcf26f26-1430-4911-993e-4c88f18121e6", "embedding": null, "metadata": {"page_label": "4", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "707b5854-4e98-4c49-bb14-5c58767834b9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "737f3598b5566cd72646a258e683c4f78cf1ac6f603fa7cd9fa2daccdba180d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a285257-a731-4a55-adcb-7a900738db4e", "node_type": "1", "metadata": {"page_label": "4", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "910180b7088b2a15cd2ff35931c55903f60b2909394f9e096fa223a0981953cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Our model also uses rela-\ntively coarse features for predicting bounding boxes since\nour architecture has multiple downsampling layers from the\ninput image.\nFinally, while we train on a loss function that approxi-\nmates detection performance, our loss function treats errors\nthe same in small bounding boxes versus large bounding\nboxes. A small error in a large box is generally benign but a\nsmall error in a small box has a much greater effect on IOU.\nOur main source of error is incorrect localizations.\n3. Comparison to Other Detection Systems\nObject detection is a core problem in computer vision.\nDetection pipelines generally start by extracting a set of\nrobust features from input images (Haar [25], SIFT [23],\nHOG [4], convolutional features [6]). Then, classi\ufb01ers\n[36, 21, 13, 10] or localizers [1, 32] are used to identify\nobjects in the feature space. These classi\ufb01ers or localizers\nare run either in sliding window fashion over the whole im-\nage or on some subset of regions in the image [35, 15, 39].\nWe compare the YOLO detection system to several top de-\ntection frameworks, highlighting key similarities and differ-\nences.\nDeformable parts models. Deformable parts models\n(DPM) use a sliding window approach to object detection\n[10]. DPM uses a disjoint pipeline to extract static features,\nclassify regions, predict bounding boxes for high scoring\nregions, etc. Our system replaces all of these disparate parts\nwith a single convolutional neural network. The network\nperforms feature extraction, bounding box prediction, non-\nmaximal suppression, and contextual reasoning all concur-\nrently. Instead of static features, the network trains the fea-\ntures in-line and optimizes them for the detection task. Our\nuni\ufb01ed architecture leads to a faster, more accurate model\nthan DPM.\nR-CNN. R-CNN and its variants use region proposals in-\nstead of sliding windows to \ufb01nd objects in images. Selective", "mimetype": "text/plain", "start_char_idx": 3082, "end_char_idx": 4992, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "979b24b7-0e7c-4aad-93a3-2b05f03816c5": {"__data__": {"id_": "979b24b7-0e7c-4aad-93a3-2b05f03816c5", "embedding": null, "metadata": {"page_label": "5", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9afffa19-c059-480c-af5a-8e971b57381e", "node_type": "4", "metadata": {"page_label": "5", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "38cf6a8251ecc318f33b21d7e961592a5e78145becbb4704911f9eb885067c25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbe1bce6-84ea-4eb7-bc63-dfed44c50517", "node_type": "1", "metadata": {}, "hash": "53a478ec5435871535b8523fb4ed928678dd71bfffcce62058e24552f11e3480", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Search [35] generates potential bounding boxes, a convolu-\ntional network extracts features, an SVM scores the boxes, a\nlinear model adjusts the bounding boxes, and non-max sup-\npression eliminates duplicate detections. Each stage of this\ncomplex pipeline must be precisely tuned independently\nand the resulting system is very slow, taking more than 40\nseconds per image at test time [14].\nYOLO shares some similarities with R-CNN. Each grid\ncell proposes potential bounding boxes and scores those\nboxes using convolutional features. However, our system\nputs spatial constraints on the grid cell proposals which\nhelps mitigate multiple detections of the same object. Our\nsystem also proposes far fewer bounding boxes, only 98\nper image compared to about 2000 from Selective Search.\nFinally, our system combines these individual components\ninto a single, jointly optimized model.\nOther Fast Detectors Fast and Faster R-CNN focus on\nspeeding up the R-CNN framework by sharing computa-\ntion and using neural networks to propose regions instead\nof Selective Search [14] [28]. While they offer speed and\naccuracy improvements over R-CNN, both still fall short of\nreal-time performance.\nMany research efforts focus on speeding up the DPM\npipeline [31] [38] [5]. They speed up HOG computation,\nuse cascades, and push computation to GPUs. However,\nonly 30Hz DPM [31] actually runs in real-time.\nInstead of trying to optimize individual components of\na large detection pipeline, YOLO throws out the pipeline\nentirely and is fast by design.\nDetectors for single classes like faces or people can be\nhighly optimized since they have to deal with much less\nvariation [37]. YOLO is a general purpose detector that\nlearns to detect a variety of objects simultaneously.\nDeep MultiBox. Unlike R-CNN, Szegedy et al. train a\nconvolutional neural network to predict regions of interest\n[8] instead of using Selective Search. MultiBox can also\nperform single object detection by replacing the con\ufb01dence\nprediction with a single class prediction. However, Multi-\nBox cannot perform general object detection and is still just\na piece in a larger detection pipeline, requiring further im-\nage patch classi\ufb01cation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2189, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fbe1bce6-84ea-4eb7-bc63-dfed44c50517": {"__data__": {"id_": "fbe1bce6-84ea-4eb7-bc63-dfed44c50517", "embedding": null, "metadata": {"page_label": "5", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9afffa19-c059-480c-af5a-8e971b57381e", "node_type": "4", "metadata": {"page_label": "5", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "38cf6a8251ecc318f33b21d7e961592a5e78145becbb4704911f9eb885067c25", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "979b24b7-0e7c-4aad-93a3-2b05f03816c5", "node_type": "1", "metadata": {"page_label": "5", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "f4f90c3bf1f1d7058953fd15f4da2ae9869b68c279a68fcf7df79d23dccaff21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f78ad04-fc98-443e-ac0c-6a42369faef8", "node_type": "1", "metadata": {}, "hash": "0ccc58a1c195c1c0ac507006d46c967de4b2438686b2af7c1c9e625240519a51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Both YOLO and MultiBox use a\nconvolutional network to predict bounding boxes in an im-\nage but YOLO is a complete detection system.\nOverFeat. Sermanet et al. train a convolutional neural\nnetwork to perform localization and adapt that localizer to\nperform detection [32]. OverFeat ef\ufb01ciently performs slid-\ning window detection but it is still a disjoint system. Over-\nFeat optimizes for localization, not detection performance.\nLike DPM, the localizer only sees local information when\nmaking a prediction. OverFeat cannot reason about global\ncontext and thus requires signi\ufb01cant post-processing to pro-\nduce coherent detections.\nMultiGrasp. Our work is similar in design to work on\ngrasp detection by Redmon et al [27]. Our grid approach to\nbounding box prediction is based on the MultiGrasp system\nfor regression to grasps. However, grasp detection is a much\nsimpler task than object detection. MultiGrasp only needs\nto predict a single graspable region for an image containing\none object. It doesn\u2019t have to estimate the size, location,\nor boundaries of the object or predict it\u2019s class, only \ufb01nd a\nregion suitable for grasping. YOLO predicts both bounding\nboxes and class probabilities for multiple objects of multi-\nple classes in an image.\n4. Experiments\nFirst we compare YOLO with other real-time detection\nsystems on PASCAL VOC 2007. To understand the differ-\nences between YOLO and R-CNN variants we explore the\nerrors on VOC 2007 made by YOLO and Fast R-CNN, one\nof the highest performing versions of R-CNN [14]. Based\non the different error pro\ufb01les we show that YOLO can be\nused to rescore Fast R-CNN detections and reduce the er-\nrors from background false positives, giving a signi\ufb01cant\nperformance boost. We also present VOC 2012 results and\ncompare mAP to current state-of-the-art methods. Finally,\nwe show that YOLO generalizes to new domains better than\nother detectors on two artwork datasets.\n4.1.", "mimetype": "text/plain", "start_char_idx": 2190, "end_char_idx": 4105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f78ad04-fc98-443e-ac0c-6a42369faef8": {"__data__": {"id_": "1f78ad04-fc98-443e-ac0c-6a42369faef8", "embedding": null, "metadata": {"page_label": "5", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9afffa19-c059-480c-af5a-8e971b57381e", "node_type": "4", "metadata": {"page_label": "5", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "38cf6a8251ecc318f33b21d7e961592a5e78145becbb4704911f9eb885067c25", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbe1bce6-84ea-4eb7-bc63-dfed44c50517", "node_type": "1", "metadata": {"page_label": "5", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "c227fe426042bccb3f2c15200e4b47af20b10fa53b16bf917da1dbf8fa199c7e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.1. Comparison to Other Real-Time Systems\nMany research efforts in object detection focus on mak-\ning standard detection pipelines fast. [5] [38] [31] [14] [17]\n[28] However, only Sadeghi et al. actually produce a de-\ntection system that runs in real-time (30 frames per second\nor better) [31]. We compare YOLO to their GPU imple-\nmentation of DPM which runs either at 30Hz or 100Hz.\nWhile the other efforts don\u2019t reach the real-time milestone\nwe also compare their relative mAP and speed to examine\nthe accuracy-performance tradeoffs available in object de-\ntection systems.\nFast YOLO is the fastest object detection method on\nPASCAL ; as far as we know, it is the fastest extant object\ndetector. With 52.7% mAP, it is more than twice as accurate\nas prior work on real-time detection. YOLO pushes mAP to\n63.4% while still maintaining real-time performance.\nWe also train YOLO using VGG-16. This model is more\naccurate but also signi\ufb01cantly slower than YOLO. It is use-\nful for comparison to other detection systems that rely on\nVGG-16 but since it is slower than real-time the rest of the\npaper focuses on our faster models.\nFastest DPM effectively speeds up DPM without sacri-\n\ufb01cing much mAP but it still misses real-time performance\nby a factor of 2 [38]. It also is limited by DPM\u2019s relatively\nlow accuracy on detection compared to neural network ap-\nproaches.\nR-CNN minus R replaces Selective Search with static\nbounding box proposals [20]. While it is much faster than", "mimetype": "text/plain", "start_char_idx": 4101, "end_char_idx": 5576, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c217c2e2-8542-453f-829a-017a3c8a55e0": {"__data__": {"id_": "c217c2e2-8542-453f-829a-017a3c8a55e0", "embedding": null, "metadata": {"page_label": "6", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ea657a7-0df6-4d32-9c27-b3384842c752", "node_type": "4", "metadata": {"page_label": "6", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "75371ea3ef4bf6f5e3ccb07fe6551d8228889670f055fa8828d08d9e2aa45a8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6508351a-2c61-4275-b22d-446bec30cedf", "node_type": "1", "metadata": {}, "hash": "a21dc9b64e7987ed8e34fbc3462b187d6db1c70c23bb79ebbd5d9dd3fc559b9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Real-Time Detectors Train mAP FPS\n100Hz DPM [31] 2007 16.0 100\n30Hz DPM [31] 2007 26.1 30\nFast YOLO 2007+2012 52.7 155\nYOLO 2007+2012 63.4 45\nLess Than Real-Time\nFastest DPM [38] 2007 30.4 15\nR-CNN Minus R [20] 2007 53.5 6\nFast R-CNN [14] 2007+2012 70.0 0.5\nFaster R-CNN VGG-16[28] 2007+2012 73.2 7\nFaster R-CNN ZF [28] 2007+2012 62.1 18\nYOLO VGG-16 2007+2012 66.4 21\nTable 1: Real-Time Systems on PASCAL VOC 2007. Compar-\ning the performance and speed of fast detectors. Fast YOLO is\nthe fastest detector on record for P ASCAL VOC detection and is\nstill twice as accurate as any other real-time detector. YOLO is\n10 mAP more accurate than the fast version while still well above\nreal-time in speed.\nR-CNN, it still falls short of real-time and takes a signi\ufb01cant\naccuracy hit from not having good proposals.\nFast R-CNN speeds up the classi\ufb01cation stage of R-CNN\nbut it still relies on selective search which can take around\n2 seconds per image to generate bounding box proposals.\nThus it has high mAP but at 0.5 fps it is still far from real-\ntime.\nThe recent Faster R-CNN replaces selective search with\na neural network to propose bounding boxes, similar to\nSzegedy et al. [8] In our tests, their most accurate model\nachieves 7 fps while a smaller, less accurate one runs at\n18 fps. The VGG-16 version of Faster R-CNN is 10 mAP\nhigher but is also 6 times slower than YOLO.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1374, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6508351a-2c61-4275-b22d-446bec30cedf": {"__data__": {"id_": "6508351a-2c61-4275-b22d-446bec30cedf", "embedding": null, "metadata": {"page_label": "6", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ea657a7-0df6-4d32-9c27-b3384842c752", "node_type": "4", "metadata": {"page_label": "6", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "75371ea3ef4bf6f5e3ccb07fe6551d8228889670f055fa8828d08d9e2aa45a8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c217c2e2-8542-453f-829a-017a3c8a55e0", "node_type": "1", "metadata": {"page_label": "6", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "6638aa7b71bdc795c502fc0824c9a4b844aed3c70906a88035e1eaa29c01d062", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d593f51-ba53-41a9-8c4f-df2be1d4cc4a", "node_type": "1", "metadata": {}, "hash": "c71d2a8ceebff3998ae9046f13cef8519650e6ed070f008da34490723ea762aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Zeiler-\nFergus Faster R-CNN is only 2.5 times slower than YOLO\nbut is also less accurate.\n4.2. VOC 2007 Error Analysis\nTo further examine the differences between YOLO and\nstate-of-the-art detectors, we look at a detailed breakdown\nof results on VOC 2007. We compare YOLO to Fast R-\nCNN since Fast R-CNN is one of the highest performing\ndetectors on PASCAL and it\u2019s detections are publicly avail-\nable.\nWe use the methodology and tools of Hoiem et al. [19]\nFor each category at test time we look at the top N predic-\ntions for that category. Each prediction is either correct or\nit is classi\ufb01ed based on the type of error:\n\u2022Correct: correct class and IOU >.5\n\u2022Localization: correct class, .1 <IOU <.5\n\u2022Similar: class is similar, IOU >.1\nCorrect: 71.6% Correct: 65.5%\nLoc: 8.6%\nSim: 4.3%\nOther: 1.9%\nBackground: 13.6%\nLoc: 19.0%\nSim: 6.75%\nOther: 4.0%\nBackground: 4.75%\nFast R-CNN YOLO\nFigure 4: Error Analysis: Fast R-CNN vs. YOLO These\ncharts show the percentage of localization and background errors\nin the top N detections for various categories (N = # objects in that\ncategory).\n\u2022Other: class is wrong, IOU >.1\n\u2022Background: IOU <.1 for any object\nFigure 4 shows the breakdown of each error type aver-\naged across all 20 classes.\nYOLO struggles to localize objects correctly. Localiza-\ntion errors account for more of YOLO\u2019s errors than all other\nsources combined. Fast R-CNN makes much fewer local-\nization errors but far more background errors. 13.6% of\nit\u2019s top detections are false positives that don\u2019t contain any\nobjects. Fast R-CNN is almost 3x more likely to predict\nbackground detections than YOLO.\n4.3.", "mimetype": "text/plain", "start_char_idx": 1375, "end_char_idx": 2993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d593f51-ba53-41a9-8c4f-df2be1d4cc4a": {"__data__": {"id_": "6d593f51-ba53-41a9-8c4f-df2be1d4cc4a", "embedding": null, "metadata": {"page_label": "6", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ea657a7-0df6-4d32-9c27-b3384842c752", "node_type": "4", "metadata": {"page_label": "6", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "75371ea3ef4bf6f5e3ccb07fe6551d8228889670f055fa8828d08d9e2aa45a8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6508351a-2c61-4275-b22d-446bec30cedf", "node_type": "1", "metadata": {"page_label": "6", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "12ea5377e53fbf3c76ab0600f515cecd3b3d5f7f347d7f7fd712c2ec1ed7bd5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.3. Combining Fast R-CNN and YOLO\nYOLO makes far fewer background mistakes than Fast\nR-CNN. By using YOLO to eliminate background detec-\ntions from Fast R-CNN we get a signi\ufb01cant boost in perfor-\nmance. For every bounding box that R-CNN predicts we\ncheck to see if YOLO predicts a similar box. If it does, we\ngive that prediction a boost based on the probability pre-\ndicted by YOLO and the overlap between the two boxes.\nThe best Fast R-CNN model achieves a mAP of 71.8%\non the VOC 2007 test set. When combined with YOLO, its\nmAP Combined Gain\nFast R-CNN 71.8 - -\nFast R-CNN (2007 data) 66.9 72.4 .6\nFast R-CNN (VGG-M) 59.2 72.4 .6\nFast R-CNN (CaffeNet) 57.1 72.1 .3\nYOLO 63.4 75.0 3.2\nTable 2: Model combination experiments on VOC 2007. We\nexamine the effect of combining various models with the best ver-\nsion of Fast R-CNN. Other versions of Fast R-CNN provide only\na small bene\ufb01t while YOLO provides a signi\ufb01cant performance\nboost.", "mimetype": "text/plain", "start_char_idx": 2989, "end_char_idx": 3926, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddb7948f-45ef-4220-afe6-3c117218fbd9": {"__data__": {"id_": "ddb7948f-45ef-4220-afe6-3c117218fbd9", "embedding": null, "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6aeadcd-0426-4253-9f8f-6e09017ccf8b", "node_type": "4", "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4869c4df4c781f8a013c21d9ba7ace6864a40f61d1059c10cadb69c012519b1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2b57a5e-2b4a-4231-89a2-e922de189bd0", "node_type": "1", "metadata": {}, "hash": "3365f84af1458c3a3c3f2e7f3d0bbc6ab058261b0453120913b9c13d882519d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "VOC 2012 test mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike personplant sheep sofa train tv\nMR CNN MORE DATA [11] 73.9 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0\nHyperNet VGG 71.4 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7\nHyperNet SP 71.3 84.1 78.3 73.3 55.5 53.6 78.6 79.6 87.5 49.5 74.9 52.1 85.6 81.6 83.2 81.6 48.4 73.2 59.3 79.7 65.6\nFast R-CNN + YOLO 70.7 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2\nMR CNN S CNN [11] 70.7 85.0 79.6 71.5 55.3 57.7 76.0 73.9 84.6 50.5 74.3 61.7 85.5 79.9 81.7 76.4 41.0 69.0 61.2 77.7 72.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 727, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2b57a5e-2b4a-4231-89a2-e922de189bd0": {"__data__": {"id_": "e2b57a5e-2b4a-4231-89a2-e922de189bd0", "embedding": null, "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6aeadcd-0426-4253-9f8f-6e09017ccf8b", "node_type": "4", "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4869c4df4c781f8a013c21d9ba7ace6864a40f61d1059c10cadb69c012519b1c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddb7948f-45ef-4220-afe6-3c117218fbd9", "node_type": "1", "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "f2179c59b07dc7b33782188df18f1dd1273fe8f096ad478c48ada20f0bae844d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a68eeb05-c4bc-4fe1-9b55-b0ced343280d", "node_type": "1", "metadata": {}, "hash": "347675198711c010e6f1ac926a1c835176bd21e6aa069658a63303c5cc16c837", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 41.0 69.0 61.2 77.7 72.1\nFaster R-CNN [28] 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\nDEEP ENS COCO 70.1 84.0 79.4 71.6 51.9 51.1 74.1 72.1 88.6 48.3 73.4 57.8 86.1 80.0 80.7 70.4 46.6 69.6 68.8 75.9 71.4\nNoC [29] 68.8 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1\nFast R-CNN [14] 68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2\nUMICH FGS STRUCT 66.4 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2\nNUS NIN C2000 [7] 63.", "mimetype": "text/plain", "start_char_idx": 702, "end_char_idx": 1349, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a68eeb05-c4bc-4fe1-9b55-b0ced343280d": {"__data__": {"id_": "a68eeb05-c4bc-4fe1-9b55-b0ced343280d", "embedding": null, "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6aeadcd-0426-4253-9f8f-6e09017ccf8b", "node_type": "4", "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4869c4df4c781f8a013c21d9ba7ace6864a40f61d1059c10cadb69c012519b1c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2b57a5e-2b4a-4231-89a2-e922de189bd0", "node_type": "1", "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "aae602652206090edd4d4efd6d0aaeffc74a18cc3a98fea163be2b1bfe5bd7c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24d1a104-133c-4206-9866-a07a7e1c8e01", "node_type": "1", "metadata": {}, "hash": "1c480122a4cd66060e7156c37ac15a8b033bf535142c928ac391a128d896c336", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0 66.2\nNUS NIN C2000 [7] 63.8 80.2 73.8 61.9 43.7 43.0 70.3 67.6 80.7 41.9 69.7 51.7 78.2 75.2 76.9 65.1 38.6 68.3 58.0 68.7 63.3\nBabyLearning [7] 63.2 78.0 74.2 61.3 45.7 42.7 68.2 66.8 80.2 40.6 70.0 49.8 79.0 74.5 77.9 64.0 35.3 67.9 55.7 68.7 62.6\nNUS NIN 62.4 77.9 73.1 62.6 39.5 43.3 69.1 66.4 78.9 39.1 68.1 50.0 77.2 71.3 76.1 64.7 38.4 66.9 56.2 66.9 62.7\nR-CNN VGG BB [13] 62.4 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3\nR-CNN VGG [13] 59.2 76.8 70.9 56.6 37.5 36.9 62.9 63.6 81.1 35.7 64.3 43.9 80.4 71.6 74.0 60.0 30.8 63.4 52.0 63.5 58.7\nYOLO 57.9 77.0 67.2 57.7 38.", "mimetype": "text/plain", "start_char_idx": 1321, "end_char_idx": 1957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24d1a104-133c-4206-9866-a07a7e1c8e01": {"__data__": {"id_": "24d1a104-133c-4206-9866-a07a7e1c8e01", "embedding": null, "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6aeadcd-0426-4253-9f8f-6e09017ccf8b", "node_type": "4", "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4869c4df4c781f8a013c21d9ba7ace6864a40f61d1059c10cadb69c012519b1c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a68eeb05-c4bc-4fe1-9b55-b0ced343280d", "node_type": "1", "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "f247a43bcae254c1d1a327b1e4be5abb4b0b86bceedf78ddbb7e063925819ad0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77c0d9b4-2834-4c24-bee6-fb56399af23c", "node_type": "1", "metadata": {}, "hash": "e6471bcc33300d89fcd67389eb3f9c788763e875299d03ae49f5dd299d06e039", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8\nFeature Edit [33] 56.3 74.6 69.1 54.4 39.1 33.1 65.2 62.7 69.7 30.8 56.0 44.6 70.0 64.4 71.1 60.2 33.3 61.3 46.4 61.7 57.8\nR-CNN BB [13] 53.3 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1\nSDS [16] 50.7 69.7 58.4 48.5 28.3 28.8 61.3 57.5 70.8 24.1 50.7 35.9 64.9 59.1 65.8 57.1 26.0 58.8 38.6 58.9 50.7\nR-CNN [13] 49.6 68.1 63.8 46.1 29.4 27.9 56.6 57.0 65.9 26.5 48.7 39.5 66.2 57.3 65.4 53.2 26.2 54.5 38.1 50.6 51.6\nTable 3: PASCAL VOC 2012 Leaderboard. YOLO compared with the full comp4 (outside data allowed) public leaderboard as of\nNovember 6th, 2015.", "mimetype": "text/plain", "start_char_idx": 1937, "end_char_idx": 2650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77c0d9b4-2834-4c24-bee6-fb56399af23c": {"__data__": {"id_": "77c0d9b4-2834-4c24-bee6-fb56399af23c", "embedding": null, "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6aeadcd-0426-4253-9f8f-6e09017ccf8b", "node_type": "4", "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4869c4df4c781f8a013c21d9ba7ace6864a40f61d1059c10cadb69c012519b1c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24d1a104-133c-4206-9866-a07a7e1c8e01", "node_type": "1", "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "0665d5aaea513da74be3efe7bd21406a0e472468d9065d883c02ef7dea9e9726", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a4e76af-c1b4-42d9-a5f7-8224e0843116", "node_type": "1", "metadata": {}, "hash": "d41da0bf501e5aa0c0e4a579c524fe1205841ac1475cfd65ccec89d438b5c4d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mean average precision and per-class average precision are shown for a variety of detection methods. YOLO is the\nonly real-time detector. Fast R-CNN + YOLO is the forth highest scoring method, with a 2.3% boost over Fast R-CNN.\nmAP increases by 3.2% to 75.0%. We also tried combining\nthe top Fast R-CNN model with several other versions of\nFast R-CNN. Those ensembles produced small increases in\nmAP between .3 and .6%, see Table 2 for details.\nThe boost from YOLO is not simply a byproduct of\nmodel ensembling since there is little bene\ufb01t from combin-\ning different versions of Fast R-CNN. Rather, it is precisely\nbecause YOLO makes different kinds of mistakes at test\ntime that it is so effective at boosting Fast R-CNN\u2019s per-\nformance.\nUnfortunately, this combination doesn\u2019t bene\ufb01t from the\nspeed of YOLO since we run each model seperately and\nthen combine the results. However, since YOLO is so fast\nit doesn\u2019t add any signi\ufb01cant computational time compared\nto Fast R-CNN.\n4.4. VOC 2012 Results\nOn the VOC 2012 test set, YOLO scores 57.9% mAP.\nThis is lower than the current state of the art, closer to\nthe original R-CNN using VGG-16, see Table 3. Our sys-\ntem struggles with small objects compared to its closest\ncompetitors. On categories like bottle, sheep, and\ntv/monitor YOLO scores 8-10% lower than R-CNN or\nFeature Edit. However, on other categories like cat and\ntrain YOLO achieves higher performance.\nOur combined Fast R-CNN + YOLO model is one of the\nhighest performing detection methods. Fast R-CNN gets\na 2.3% improvement from the combination with YOLO,\nboosting it 5 spots up on the public leaderboard.\n4.5. Generalizability: Person Detection in Artwork\nAcademic datasets for object detection draw the training\nand testing data from the same distribution.", "mimetype": "text/plain", "start_char_idx": 2651, "end_char_idx": 4425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a4e76af-c1b4-42d9-a5f7-8224e0843116": {"__data__": {"id_": "8a4e76af-c1b4-42d9-a5f7-8224e0843116", "embedding": null, "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6aeadcd-0426-4253-9f8f-6e09017ccf8b", "node_type": "4", "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4869c4df4c781f8a013c21d9ba7ace6864a40f61d1059c10cadb69c012519b1c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77c0d9b4-2834-4c24-bee6-fb56399af23c", "node_type": "1", "metadata": {"page_label": "7", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "731fa5cd0fea8b2f487e87626145733a3a02eb13393d3109b0a65df36d77e63f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In real-world\napplications it is hard to predict all possible use cases and\nthe test data can diverge from what the system has seen be-\nfore [3]. We compare YOLO to other detection systems on\nthe Picasso Dataset [12] and the People-Art Dataset [3], two\ndatasets for testing person detection on artwork.\nFigure 5 shows comparative performance between\nYOLO and other detection methods. For reference, we give\nVOC 2007 detection AP on person where all models are\ntrained only on VOC 2007 data. On Picasso models are\ntrained on VOC 2012 while on People-Art they are trained\non VOC 2010.\nR-CNN has high AP on VOC 2007. However, R-CNN\ndrops off considerably when applied to artwork. R-CNN\nuses Selective Search for bounding box proposals which is\ntuned for natural images. The classi\ufb01er step in R-CNN only\nsees small regions and needs good proposals.\nDPM maintains its AP well when applied to artwork.\nPrior work theorizes that DPM performs well because it has\nstrong spatial models of the shape and layout of objects.\nThough DPM doesn\u2019t degrade as much as R-CNN, it starts\nfrom a lower AP.\nYOLO has good performance on VOC 2007 and its AP\ndegrades less than other methods when applied to artwork.\nLike DPM, YOLO models the size and shape of objects,\nas well as relationships between objects and where objects\ncommonly appear. Artwork and natural images are very\ndifferent on a pixel level but they are similar in terms of\nthe size and shape of objects, thus YOLO can still predict\ngood bounding boxes and detections.\n5. Real-Time Detection In The Wild\nYOLO is a fast, accurate object detector, making it ideal\nfor computer vision applications. We connect YOLO to a\nwebcam and verify that it maintains real-time performance,", "mimetype": "text/plain", "start_char_idx": 4426, "end_char_idx": 6144, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c718caf-4dd3-42f1-a46d-574012f9c5d7": {"__data__": {"id_": "1c718caf-4dd3-42f1-a46d-574012f9c5d7", "embedding": null, "metadata": {"page_label": "8", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de4fd050-914c-477a-87fe-16bfebd34550", "node_type": "4", "metadata": {"page_label": "8", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "8511f9f0bb2df3f27184ef94ecfcea5e26ab40c2287baa97be212ca32ec1b483", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Poselets\nRCNN\nD&T\nHumans\nDPM\nYOLO\n(a) Picasso Dataset precision-recall curves.\nVOC 2007 Picasso People-Art\nAP AP Best F1 AP\nYOLO 59.2 53.3 0.590 45\nR-CNN 54.2 10.4 0.226 26\nDPM 43.2 37.8 0.458 32\nPoselets [2] 36.5 17.8 0.271\nD&T [4] - 1.9 0.051\n(b) Quantitative results on the VOC 2007, Picasso, and People-Art Datasets.\nThe Picasso Dataset evaluates on both AP and best F1 score.\nFigure 5: Generalization results on Picasso and People-Art datasets.\nFigure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it\ndoes think one person is an airplane.\nincluding the time to fetch images from the camera and dis-\nplay the detections.\nThe resulting system is interactive and engaging. While\nYOLO processes images individually, when attached to a\nwebcam it functions like a tracking system, detecting ob-\njects as they move around and change in appearance. A\ndemo of the system and the source code can be found on\nour project website: http://pjreddie.com/yolo/.\n6. Conclusion\nWe introduce YOLO, a uni\ufb01ed model for object detec-\ntion. Our model is simple to construct and can be trained\ndirectly on full images. Unlike classi\ufb01er-based approaches,\nYOLO is trained on a loss function that directly corresponds\nto detection performance and the entire model is trained\njointly.\nFast YOLO is the fastest general-purpose object detec-\ntor in the literature and YOLO pushes the state-of-the-art in\nreal-time object detection. YOLO also generalizes well to\nnew domains making it ideal for applications that rely on\nfast, robust object detection.\nAcknowledgements: This work is partially supported by\nONR N00014-13-1-0720, NSF IIS-1338054, and The Allen\nDistinguished Investigator Award.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1744, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1cbd5f2-d8b6-4156-9e7a-7f833ea8212d": {"__data__": {"id_": "f1cbd5f2-d8b6-4156-9e7a-7f833ea8212d", "embedding": null, "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7926f71f-4c4b-4e34-9079-672324d252a9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4075ba7cad6b762893da36ff35a5a08def3ebabeb91ab4d47a3045b3b08933db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f679a57-91f0-4365-a125-142d0b65f90b", "node_type": "1", "metadata": {}, "hash": "e1dfa2b4d92b496e88a19f80a2404570b70157a21d649b6c49bf477b0547e41c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References\n[1] M. B. Blaschko and C. H. Lampert. Learning to localize ob-\njects with structured output regression. In Computer Vision\u2013\nECCV 2008, pages 2\u201315. Springer, 2008. 4\n[2] L. Bourdev and J. Malik. Poselets: Body part detectors\ntrained using 3d human pose annotations. In International\nConference on Computer Vision (ICCV), 2009. 8\n[3] H. Cai, Q. Wu, T. Corradi, and P. Hall. The cross-\ndepiction problem: Computer vision algorithms for recog-\nnising objects in artwork and in photographs. arXiv preprint\narXiv:1505.00110, 2015. 7\n[4] N. Dalal and B. Triggs. Histograms of oriented gradients for\nhuman detection. In Computer Vision and Pattern Recogni-\ntion, 2005. CVPR 2005. IEEE Computer Society Conference\non, volume 1, pages 886\u2013893. IEEE, 2005. 4, 8\n[5] T. Dean, M. Ruzon, M. Segal, J. Shlens, S. Vijaya-\nnarasimhan, J. Yagnik, et al. Fast, accurate detection of\n100,000 object classes on a single machine. In Computer\nVision and Pattern Recognition (CVPR), 2013 IEEE Confer-\nence on, pages 1814\u20131821. IEEE, 2013. 5\n[6] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-\nvation feature for generic visual recognition. arXiv preprint\narXiv:1310.1531, 2013. 4\n[7] J. Dong, Q. Chen, S. Yan, and A. Yuille. Towards uni\ufb01ed\nobject detection and semantic segmentation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1337, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f679a57-91f0-4365-a125-142d0b65f90b": {"__data__": {"id_": "3f679a57-91f0-4365-a125-142d0b65f90b", "embedding": null, "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7926f71f-4c4b-4e34-9079-672324d252a9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4075ba7cad6b762893da36ff35a5a08def3ebabeb91ab4d47a3045b3b08933db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1cbd5f2-d8b6-4156-9e7a-7f833ea8212d", "node_type": "1", "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "0e17ce0587dc520ecde1b17a24aa1b628f65c4c35da00e3b1cdc2d5c94579819", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e823a811-c96b-405e-b264-e46fda9204f8", "node_type": "1", "metadata": {}, "hash": "733b6250914a41e2de13a7e48f24011c899b30a0fba39da1d56a4027c2a23660", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Towards uni\ufb01ed\nobject detection and semantic segmentation. In Computer\nVision\u2013ECCV 2014, pages 299\u2013314. Springer, 2014. 7\n[8] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\nobject detection using deep neural networks. In Computer\nVision and Pattern Recognition (CVPR), 2014 IEEE Confer-\nence on, pages 2155\u20132162. IEEE, 2014. 5, 6\n[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\nWilliams, J. Winn, and A. Zisserman. The pascal visual ob-\nject classes challenge: A retrospective.International Journal\nof Computer Vision, 111(1):98\u2013136, Jan. 2015. 2\n[10] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\nmanan. Object detection with discriminatively trained part\nbased models. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 32(9):1627\u20131645, 2010. 1, 4\n[11] S. Gidaris and N. Komodakis. Object detection via a multi-\nregion & semantic segmentation-aware CNN model. CoRR,\nabs/1505.01749, 2015. 7\n[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting peo-\nple in cubist art. InComputer Vision-ECCV 2014 Workshops,\npages 101\u2013116. Springer, 2014. 7\n[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In Computer Vision and Pattern Recognition\n(CVPR), 2014 IEEE Conference on , pages 580\u2013587. IEEE,\n2014.", "mimetype": "text/plain", "start_char_idx": 1279, "end_char_idx": 2621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e823a811-c96b-405e-b264-e46fda9204f8": {"__data__": {"id_": "e823a811-c96b-405e-b264-e46fda9204f8", "embedding": null, "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7926f71f-4c4b-4e34-9079-672324d252a9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4075ba7cad6b762893da36ff35a5a08def3ebabeb91ab4d47a3045b3b08933db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f679a57-91f0-4365-a125-142d0b65f90b", "node_type": "1", "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "c2e98e6fa111f639b790a11b4f58b64924aef4793dc4a5729be6de7c26692e4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "989ff429-2ab2-485c-805c-9ca537c7cc63", "node_type": "1", "metadata": {}, "hash": "bd21f39f9cb3f63fb5ede312e256833874e7d83024a68717caa46b3f3d830e15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE,\n2014. 1, 4, 7\n[14] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015.\n2, 5, 6, 7\n[15] S. Gould, T. Gao, and D. Koller. Region-based segmenta-\ntion and object detection. In Advances in neural information\nprocessing systems, pages 655\u2013663, 2009. 4\n[16] B. Hariharan, P. Arbel \u00b4aez, R. Girshick, and J. Malik. Simul-\ntaneous detection and segmentation. In Computer Vision\u2013\nECCV 2014, pages 297\u2013312. Springer, 2014. 7\n[17] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\nin deep convolutional networks for visual recognition. arXiv\npreprint arXiv:1406.4729, 2014. 5\n[18] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. Improving neural networks by pre-\nventing co-adaptation of feature detectors. arXiv preprint\narXiv:1207.0580, 2012. 4\n[19] D. Hoiem, Y . Chodpathumwan, and Q. Dai. Diagnosing error\nin object detectors. In Computer Vision\u2013ECCV 2012, pages\n340\u2013353. Springer, 2012. 6\n[20] K. Lenc and A. Vedaldi. R-cnn minus r. arXiv preprint\narXiv:1506.06981, 2015. 5, 6\n[21] R. Lienhart and J. Maydt. An extended set of haar-like fea-\ntures for rapid object detection. In Image Processing. 2002.\nProceedings. 2002 International Conference on , volume 1,\npages I\u2013900.", "mimetype": "text/plain", "start_char_idx": 2610, "end_char_idx": 3833, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "989ff429-2ab2-485c-805c-9ca537c7cc63": {"__data__": {"id_": "989ff429-2ab2-485c-805c-9ca537c7cc63", "embedding": null, "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7926f71f-4c4b-4e34-9079-672324d252a9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4075ba7cad6b762893da36ff35a5a08def3ebabeb91ab4d47a3045b3b08933db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e823a811-c96b-405e-b264-e46fda9204f8", "node_type": "1", "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "9bdb4c386ee7dffa48f38adc6a2d91fa640823db6f72a1f60012cbba20deade4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "400832fc-628f-4c7c-ab89-6540ff9aa9ff", "node_type": "1", "metadata": {}, "hash": "7d3746b89174404870dc4783ea24f174ecce7cfd6177e6441bd2793a3f42a194", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Proceedings. 2002 International Conference on , volume 1,\npages I\u2013900. IEEE, 2002. 4\n[22] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR,\nabs/1312.4400, 2013. 2\n[23] D. G. Lowe. Object recognition from local scale-invariant\nfeatures. In Computer vision, 1999. The proceedings of the\nseventh IEEE international conference on , volume 2, pages\n1150\u20131157. Ieee, 1999. 4\n[24] D. Mishkin. Models accuracy on imagenet 2012\nval. https://github.com/BVLC/caffe/wiki/\nModels-accuracy-on-ImageNet-2012-val . Ac-\ncessed: 2015-10-2. 3\n[25] C. P. Papageorgiou, M. Oren, and T. Poggio. A general\nframework for object detection. In Computer vision, 1998.\nsixth international conference on , pages 555\u2013562. IEEE,\n1998. 4\n[26] J. Redmon. Darknet: Open source neural networks in c.\nhttp://pjreddie.com/darknet/, 2013\u20132016. 3\n[27] J. Redmon and A. Angelova. Real-time grasp detection using\nconvolutional neural networks. CoRR, abs/1412.3128, 2014.\n5\n[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-\nwards real-time object detection with region proposal net-\nworks. arXiv preprint arXiv:1506.01497, 2015. 5, 6, 7\n[29] S. Ren, K. He, R. B. Girshick, X. Zhang, and J. Sun. Object\ndetection networks on convolutional feature maps. CoRR,\nabs/1504.06066, 2015.", "mimetype": "text/plain", "start_char_idx": 3763, "end_char_idx": 5018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "400832fc-628f-4c7c-ab89-6540ff9aa9ff": {"__data__": {"id_": "400832fc-628f-4c7c-ab89-6540ff9aa9ff", "embedding": null, "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7926f71f-4c4b-4e34-9079-672324d252a9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "4075ba7cad6b762893da36ff35a5a08def3ebabeb91ab4d47a3045b3b08933db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "989ff429-2ab2-485c-805c-9ca537c7cc63", "node_type": "1", "metadata": {"page_label": "9", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "0de9a8f8219f78ae68ebed7916c1783fc9b061b9474959b2ea5e0a319f33774c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CoRR,\nabs/1504.06066, 2015. 3, 7\n[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\nRecognition Challenge. International Journal of Computer\nVision (IJCV), 2015. 3\n[31] M. A. Sadeghi and D. Forsyth. 30hz object detection with\ndpm v5. In Computer Vision\u2013ECCV 2014 , pages 65\u201379.\nSpringer, 2014. 5, 6\n[32] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y . LeCun. Overfeat: Integrated recognition, localiza-\ntion and detection using convolutional networks. CoRR,\nabs/1312.6229, 2013. 4, 5", "mimetype": "text/plain", "start_char_idx": 4991, "end_char_idx": 5611, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85c2b027-7de7-40bf-a62a-ad643a2f7b0f": {"__data__": {"id_": "85c2b027-7de7-40bf-a62a-ad643a2f7b0f", "embedding": null, "metadata": {"page_label": "10", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0aecbee5-a75d-4421-858a-c7ee37042a3b", "node_type": "4", "metadata": {"page_label": "10", "file_name": "yolo.pdf", "file_path": "/Users/betopia/LlamaIndex/data/yolo.pdf", "file_type": "application/pdf", "file_size": 5296750, "creation_date": "2025-11-03", "last_modified_date": "2025-11-03"}, "hash": "790fc2cb8ce0c57dcaf8af11f21cd0cb0650231af00bb5cef42bfdb6f15f8bbf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[33] Z. Shen and X. Xue. Do more dropouts in pool5 feature maps\nfor better object detection. arXiv preprint arXiv:1409.6911,\n2014. 7\n[34] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed,\nD. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. CoRR, abs/1409.4842,\n2014. 2\n[35] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.\nSmeulders. Selective search for object recognition. Inter-\nnational journal of computer vision , 104(2):154\u2013171, 2013.\n4\n[36] P. Viola and M. Jones. Robust real-time object detection.\nInternational Journal of Computer Vision, 4:34\u201347, 2001. 4\n[37] P. Viola and M. J. Jones. Robust real-time face detection.\nInternational journal of computer vision , 57(2):137\u2013154,\n2004. 5\n[38] J. Yan, Z. Lei, L. Wen, and S. Z. Li. The fastest deformable\npart model for object detection. In Computer Vision and Pat-\ntern Recognition (CVPR), 2014 IEEE Conference on , pages\n2497\u20132504. IEEE, 2014. 5, 6\n[39] C. L. Zitnick and P. Doll\u00b4ar. Edge boxes: Locating object pro-\nposals from edges. In Computer Vision\u2013ECCV 2014, pages\n391\u2013405. Springer, 2014. 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1103, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}